#!/bin/bash
# AI Documentation Generator - Complete Working Implementation
# This integrates with your existing tools and provides the AI layer

set -euo pipefail

# Configuration
AI_SERVICE="${AI_SERVICE:-mock}"  # Options: ollama, openai, gemini, mock
WORK_DIR="/tmp/ai_doc_work_$$"
LAB_ROOT="${LAB_ROOT:-/home/es/lab}"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() { echo -e "${BLUE}[AI-DOC]${NC} $1"; }
success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
error() { echo -e "${RED}[ERROR]${NC} $1" >&2; }

# Initialize work directory
init() {
    mkdir -p "$WORK_DIR"
    trap "rm -rf '$WORK_DIR'" EXIT
}

# Collect metadata using your existing tools
collect_metadata() {
    local target_dir="$1"
    local output_file="$2"
    
    log "Collecting comprehensive metadata for: $target_dir"
    
    # Initialize JSON structure
    cat > "$output_file" << 'EOF'
{
  "directory": "",
  "timestamp": "",
  "functions": [],
  "variables": [],
  "documentation": [],
  "project_stats": {},
  "files": [],
  "code_quality": {},
  "integration": {},
  "usage_patterns": {},
  "security_environment": {},
  "evolution": {}
}
EOF
    
    # Add basic info
    jq --arg dir "$target_dir" --arg ts "$(date -Iseconds)" \
       '.directory = $dir | .timestamp = $ts' \
       "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    
    # PHASE 1: Function Intelligence (from func module)
    if command -v ana_laf >/dev/null 2>&1 && [[ -d "$target_dir" ]]; then
        log "🔧 Analyzing functions with ana_laf (func module integration)..."
        if ana_laf "$target_dir" -j > "$WORK_DIR/functions.json" 2>/dev/null; then
            jq --slurpfile funcs "$WORK_DIR/functions.json" '.functions = $funcs[0] // []' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # PHASE 2: Configuration Intelligence (from var module)
    if command -v ana_acu >/dev/null 2>&1 && [[ -d "$target_dir" ]]; then
        log "⚙️ Analyzing variables with ana_acu (var module integration)..."
        # Analyze the target directory and related config locations
        if ana_acu -j "" "$target_dir" "$LAB_ROOT/cfg/env" "$LAB_ROOT/lib/ops" > "$WORK_DIR/variables.json" 2>/dev/null; then
            jq --slurpfile vars "$WORK_DIR/variables.json" '.variables = $vars[0] // []' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # PHASE 3: Documentation Context (from hub module)  
    if command -v ana_lad >/dev/null 2>&1; then
        log "📚 Analyzing documentation context with ana_lad (hub module integration)..."
        # Analyze both the target directory and the overall doc structure
        if ana_lad -j "$target_dir" > "$WORK_DIR/local_docs.json" 2>/dev/null; then
            jq --slurpfile docs "$WORK_DIR/local_docs.json" '.documentation = $docs[0] // []' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
        
        # Get project-wide documentation context
        if ana_lad -j "$LAB_ROOT/doc" > "$WORK_DIR/project_docs.json" 2>/dev/null; then
            jq --slurpfile pdocs "$WORK_DIR/project_docs.json" '.project_documentation = $pdocs[0] // []' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # PHASE 4: Project Scale Intelligence (from stats module)
    log "📊 Collecting project scale metrics (stats module integration)..."
    local project_stats="{}"
    if [[ -d "$LAB_ROOT" ]]; then
        local total_files=$(find "$LAB_ROOT" -type f 2>/dev/null | wc -l)
        local lib_functions=$(find "$LAB_ROOT/lib" -type f -exec grep -l "^[a-zA-Z_][a-zA-Z0-9_-]*(" {} \; 2>/dev/null | wc -l)
        local config_files=$(find "$LAB_ROOT/cfg" -type f 2>/dev/null | wc -l)
        local doc_files=$(find "$LAB_ROOT/doc" -name "*.md" 2>/dev/null | wc -l)
        
        project_stats="{\"total_files\": $total_files, \"lib_functions\": $lib_functions, \"config_files\": $config_files, \"doc_files\": $doc_files}"
    fi
    
    echo "$project_stats" | jq . > "$WORK_DIR/stats.json" 2>/dev/null
    if [[ -f "$WORK_DIR/stats.json" ]]; then
        jq --slurpfile stats "$WORK_DIR/stats.json" '.project_stats = $stats[0] // {}' \
           "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    fi
    
    # Basic file listing
    if [[ -d "$target_dir" ]]; then
        find "$target_dir" -maxdepth 1 -type f -printf '%f\n' 2>/dev/null | \
        jq -R . | jq -s '.' > "$WORK_DIR/files.json"
        jq --slurpfile files "$WORK_DIR/files.json" '.files = $files[0]' \
           "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    fi
    
    # PHASE 5: Code Quality Intelligence 
    log "🎯 Analyzing code quality and complexity..."
    local code_quality="{}"
    if [[ -d "$target_dir" ]]; then
        local avg_func_length=0
        local total_funcs=$(grep -rc "^[a-zA-Z_][a-zA-Z0-9_]*() {" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}')
        local nested_conditionals=$(grep -rc "if.*if\|case.*case" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}')
        local error_handling=$(grep -rc "|| exit\||| return\|set -e\|trap" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}')
        local comments=$(grep -rc "^[[:space:]]*#" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}')
        local total_lines=$(find "$target_dir" -name "*.sh" -o -name "*" -type f -exec wc -l {} + 2>/dev/null | tail -1 | awk '{print $1}' || echo "1")
        local test_files=$(find "$target_dir" -name "*test*" -o -name "*spec*" -type f 2>/dev/null | wc -l)
        
        local doc_ratio=0
        if [[ $total_lines -gt 0 ]]; then
            doc_ratio=$(awk "BEGIN {printf \"%.2f\", $comments * 100 / $total_lines}" 2>/dev/null || echo "0")
        fi
        
        code_quality=$(cat << EOF
{
  "complexity_metrics": {
    "total_functions": $total_funcs,
    "nested_conditionals": $nested_conditionals,
    "error_handling_patterns": $error_handling,
    "documentation_ratio": $doc_ratio,
    "test_files_present": $test_files
  },
  "maintainability": {
    "todo_comments": $(grep -rc "TODO\|FIXME\|XXX\|HACK" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "deprecated_indicators": $(grep -rc "deprecated\|obsolete\|legacy" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "version_indicators": $(grep -rc "version\|v[0-9]\|release" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}')
  }
}
EOF
        )
    fi
    echo "$code_quality" | jq . > "$WORK_DIR/code_quality.json" 2>/dev/null
    if [[ -f "$WORK_DIR/code_quality.json" ]]; then
        jq --slurpfile cq "$WORK_DIR/code_quality.json" '.code_quality = $cq[0] // {}' \
           "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    fi
    
    # PHASE 6: Integration and Dependencies Intelligence
    log "🔗 Analyzing integration patterns and dependencies..."
    local integration_data="{}"
    if [[ -d "$target_dir" ]]; then
        # Simplified command extraction - avoid complex pipelines
        local external_cmds="[]"
        
        # Safe command counting with fallbacks
        local file_sources=$(find "$target_dir" -name "*.sh" -exec grep -l "source\|\\\." {} \; 2>/dev/null | wc -l || echo "0")
        local network_usage=$(find "$target_dir" -name "*.sh" -exec grep -l "curl\|wget\|ssh\|scp\|rsync" {} \; 2>/dev/null | wc -l || echo "0")
        local database_usage=$(find "$target_dir" -name "*.sh" -exec grep -l "mysql\|postgres\|sqlite\|redis" {} \; 2>/dev/null | wc -l || echo "0")
        local container_usage=$(find "$target_dir" -name "*.sh" -exec grep -l "docker\|podman\|kubectl\|systemctl" {} \; 2>/dev/null | wc -l || echo "0")
        local service_integration=$(find "$target_dir" -name "*.sh" -exec grep -l "systemctl\|service\|daemon" {} \; 2>/dev/null | wc -l || echo "0")
        local cron_scheduling=$(find "$target_dir" -name "*cron*" -o -name "*@daily*" -o -name "*@hourly*" 2>/dev/null | wc -l || echo "0")
        local config_management=$(find "$target_dir" -name "*.conf" -o -name "*.cfg" -o -name "*.env" 2>/dev/null | wc -l || echo "0")
        
        integration_data=$(cat << EOF
{
  "dependencies": {
    "external_commands": $external_cmds,
    "file_sources": $file_sources,
    "network_usage": $network_usage,
    "database_usage": $database_usage,
    "container_usage": $container_usage
  },
  "integration_patterns": {
    "service_integration": $service_integration,
    "cron_scheduling": $cron_scheduling,
    "config_management": $config_management
  }
}
EOF
        )
    fi
    echo "$integration_data" | jq . > "$WORK_DIR/integration.json" 2>/dev/null
    if [[ -f "$WORK_DIR/integration.json" ]]; then
        jq --slurpfile int "$WORK_DIR/integration.json" '.integration = $int[0] // {}' \
           "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    fi
    
    # PHASE 7: Usage Patterns and Workflow Intelligence
    log "⚡ Analyzing usage patterns and workflow..."
    local usage_data="{}"
    if [[ -d "$target_dir" ]]; then
        local executable_files=$(find "$target_dir" -executable -type f 2>/dev/null | wc -l)
        local config_files=$(find "$target_dir" -name "*.conf" -o -name "*.cfg" -o -name "*.env" 2>/dev/null | wc -l)
        
        usage_data=$(cat << EOF
{
  "execution_patterns": {
    "has_main_function": $(grep -q "^main()" "$target_dir"/* 2>/dev/null && echo "true" || echo "false"),
    "is_library": $(grep -q "^[a-zA-Z_][a-zA-Z0-9_]*() {" "$target_dir"/* 2>/dev/null && echo "true" || echo "false"),
    "executable_files": $executable_files,
    "configuration_files": $config_files
  },
  "interaction_types": {
    "user_interactive": $(grep -rc "read\|select\|dialog" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "batch_processing": $(grep -rc "for.*in\|while.*do\|parallel" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "cli_arguments": $(grep -rc "getopts\|\\\$1\|\\\$@\|case.*\\\$1" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "help_documentation": $(grep -rc "usage()\|help()\|--help\|-h" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}')
  }
}
EOF
        )
    fi
    echo "$usage_data" | jq . > "$WORK_DIR/usage.json" 2>/dev/null
    if [[ -f "$WORK_DIR/usage.json" ]]; then
        jq --slurpfile usage "$WORK_DIR/usage.json" '.usage_patterns = $usage[0] // {}' \
           "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    fi
    
    # PHASE 8: Security and Environment Intelligence  
    log "🛡️ Analyzing security patterns and environment context..."
    local security_env_data="{}"
    if [[ -d "$target_dir" ]]; then
        security_env_data=$(cat << EOF
{
  "security_indicators": {
    "credential_handling": $(grep -rc "password\|token\|key\|secret" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "privilege_operations": $(grep -rc "sudo\|su\|chmod\|chown" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "input_validation": $(grep -rc "\\\[\\\[.*=~\\\]\\\]\|grep -q\|validate" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "error_handling": $(grep -rc "echo.*Error:\|warn\|error.*function" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}')
  },
  "environment_context": {
    "container_ready": $(find "$target_dir" -name "*Dockerfile*" -o -name "*Containerfile*" 2>/dev/null | wc -l),
    "systemd_integration": $(find "$target_dir" -name "*.service" -o -name "*.timer" 2>/dev/null | wc -l),
    "gpu_dependent": $(grep -rc "nvidia\|cuda\|gpu" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}'),
    "storage_dependent": $(grep -rc "mount\|df\|lvm\|zfs\|btrfs" "$target_dir" 2>/dev/null | awk -F: '{sum += $2} END {print sum+0}')
  }
}
EOF
        )
    fi
    echo "$security_env_data" | jq . > "$WORK_DIR/security_env.json" 2>/dev/null
    if [[ -f "$WORK_DIR/security_env.json" ]]; then
        jq --slurpfile se "$WORK_DIR/security_env.json" '.security_environment = $se[0] // {}' \
           "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    fi
    
    # PHASE 9: Git History and Evolution Intelligence (if available)
    if [[ -d "$LAB_ROOT/.git" ]]; then
        log "📈 Analyzing development history and evolution..."
        local evolution_data="{}"
        
        # Simplify git analysis to avoid complex command substitution issues
        local rel_path="${target_dir#$LAB_ROOT/}"
        local commits_last_month=0
        local total_commits=0
        local recent_stability=0
        
        # Safe git commands with proper error handling
        if cd "$LAB_ROOT" 2>/dev/null; then
            commits_last_month=$(git log --since="1 month ago" --oneline -- "$rel_path" 2>/dev/null | wc -l || echo "0")
            total_commits=$(git log --oneline -- "$rel_path" 2>/dev/null | wc -l || echo "0")
            recent_stability=$(git log --since="1 week ago" --oneline -- "$rel_path" 2>/dev/null | wc -l || echo "0")
        fi
        
        evolution_data=$(cat << EOF
{
  "development_activity": {
    "commits_last_month": $commits_last_month,
    "total_commits": $total_commits,
    "last_modified": "Available in git history",
    "active_contributors": 1
  },
  "stability_indicators": {
    "recent_stability": $recent_stability,
    "file_age": "Tracked in git"
  }
}
EOF
        )
        
        echo "$evolution_data" | jq . > "$WORK_DIR/evolution.json" 2>/dev/null
        if [[ -f "$WORK_DIR/evolution.json" ]]; then
            jq --slurpfile evo "$WORK_DIR/evolution.json" '.evolution = $evo[0] // {}' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # PHASE 10: Performance Intelligence Module
    log "⚡ Analyzing performance patterns and optimization opportunities..."
    if [[ -x "$LAB_ROOT/utl/doc/perf" ]]; then
        "$LAB_ROOT/utl/doc/perf" "$target_dir" > "$WORK_DIR/perf.json" 2>/dev/null
        if [[ -f "$WORK_DIR/perf.json" ]]; then
            jq --slurpfile perf "$WORK_DIR/perf.json" '.performance_intelligence = $perf[0] // {}' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # PHASE 11: Dependency Graph Intelligence Module  
    log "🔗 Analyzing dependency relationships and architecture..."
    if [[ -x "$LAB_ROOT/utl/doc/deps" ]]; then
        "$LAB_ROOT/utl/doc/deps" "$target_dir" > "$WORK_DIR/deps.json" 2>/dev/null
        if [[ -f "$WORK_DIR/deps.json" ]]; then
            jq --slurpfile deps "$WORK_DIR/deps.json" '.dependency_intelligence = $deps[0] // {}' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # PHASE 12: Testing Intelligence Module
    log "🧪 Analyzing testing patterns and quality assurance..."
    if [[ -x "$LAB_ROOT/utl/doc/test" ]]; then
        "$LAB_ROOT/utl/doc/test" "$target_dir" > "$WORK_DIR/test.json" 2>/dev/null
        if [[ -f "$WORK_DIR/test.json" ]]; then
            jq --slurpfile test "$WORK_DIR/test.json" '.testing_intelligence = $test[0] // {}' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # PHASE 13: UX Intelligence Module
    log "🎨 Analyzing user experience and interface design..."
    if [[ -x "$LAB_ROOT/utl/doc/ux" ]]; then
        "$LAB_ROOT/utl/doc/ux" "$target_dir" > "$WORK_DIR/ux.json" 2>/dev/null
        if [[ -f "$WORK_DIR/ux.json" ]]; then
            jq --slurpfile ux "$WORK_DIR/ux.json" '.ux_intelligence = $ux[0] // {}' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    success "✅ Comprehensive metadata collected using all 13 intelligence modules"
}

# Build AI prompt
build_prompt() {
    local metadata_file="$1"
    local prompt_file="$2"
    
    log "Building AI prompt..."
    
    # Get example README for style
    local style_example=""
    if [[ -f "$LAB_ROOT/README.md" ]]; then
        style_example=$(head -20 "$LAB_ROOT/README.md" | sed 's/"/\\"/g')
    fi
    
    cat > "$prompt_file" << EOF
You are a technical documentation expert. Generate a comprehensive README.md file based on the following directory analysis:

DIRECTORY METADATA:
$(cat "$metadata_file")

EXISTING STYLE EXAMPLE:
$style_example

REQUIREMENTS:
1. Use user-focused emoji headers that explain value and purpose:
   - 🎯 "What This Does for You" (instead of just "Overview")
   - 🚀 "Get Started in X Minutes" (actionable quick start)
   - 📁 "What's Inside - Key Files You'll Use" (practical file guide)
   - 🤝 "How This Connects to Your Workflow" (integration value)
   - 💡 "Pro Tips & Best Practices" (user experience insights)
   - 🆘 "Need Help? Find More Info" (support resources)
2. Lead with user benefits and practical value, not just technical details
3. Include realistic time estimates ("Get Started in 2 Minutes")
4. Use actionable language ("You'll use this", "This helps you", "Saves time")
5. Document all functions and components with user context from the metadata
6. Add navigation links with descriptive text, not just technical names
7. Include pro tips and best practices from a user perspective
8. Make troubleshooting and help resources prominent
9. Focus on workflow integration and practical use cases
10. **LEVERAGE ENHANCED METADATA**: Use the comprehensive metadata including:
    - Code quality metrics for complexity assessment
    - Integration patterns for workflow context
    - Usage patterns for user guidance
    - Security indicators for best practices
    - Evolution data for maturity assessment
    - Environment context for deployment guidance
    - Performance analysis for optimization opportunities
    - Dependency mapping for architecture insights
    - Testing coverage for quality assessment
    - UX patterns for usability guidance

INTELLIGENCE ANALYSIS GUIDELINES:
- If code_quality.complexity_metrics shows high function count: emphasize modular design
- If integration.dependencies shows network usage: highlight network requirements
- If usage_patterns.execution_patterns shows CLI args: document command-line usage
- If security_environment shows credential handling: emphasize security best practices
- If evolution.development_activity shows recent commits: mention active development
- If environment_context shows container/systemd: document deployment patterns
- If performance_intelligence shows bottlenecks: suggest optimization strategies
- If dependency_intelligence shows tight coupling: recommend architectural improvements
- If testing_intelligence shows low coverage: emphasize testing needs
- If ux_intelligence shows poor CLI design: suggest usability improvements

OUTPUT: Provide ONLY the README.md content, no additional text.
EOF
}

# Call AI service
call_ai() {
    local prompt_file="$1"
    local output_file="$2"
    
    log "Calling AI service: $AI_SERVICE"
    
    case "$AI_SERVICE" in
        "ollama")
            if command -v curl >/dev/null 2>&1; then
                curl -X POST http://localhost:11434/api/generate \
                    -H "Content-Type: application/json" \
                    -d "{\"model\":\"codellama\",\"prompt\":\"$(cat "$prompt_file" | jq -Rs .)\",\"stream\":false}" \
                    2>/dev/null | jq -r '.response // "Error: No response"' > "$output_file"
            else
                error "curl not available for Ollama"
                return 1
            fi
            ;;
        "openai")
            if [[ -z "${OPENAI_API_KEY:-}" ]]; then
                error "OPENAI_API_KEY not set"
                return 1
            fi
            curl -X POST https://api.openai.com/v1/chat/completions \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d "{\"model\":\"gpt-4\",\"messages\":[{\"role\":\"user\",\"content\":$(cat "$prompt_file" | jq -Rs .)}]}" \
                2>/dev/null | jq -r '.choices[0].message.content // "Error: No response"' > "$output_file"
            ;;
        "gemini")
            if [[ -z "${GEMINI_API_KEY:-}" ]]; then
                error "GEMINI_API_KEY not set"
                return 1
            fi
            # Escape the prompt content for JSON
            local escaped_prompt=$(cat "$prompt_file" | jq -Rs .)
            curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=$GEMINI_API_KEY" \
                -H "Content-Type: application/json" \
                -d "{\"contents\":[{\"parts\":[{\"text\":$escaped_prompt}]}]}" \
                2>/dev/null | jq -r '.candidates[0].content.parts[0].text // "Error: No response"' > "$output_file"
            ;;
        "mock")
            log "Using mock AI (for testing)"
            local dir_name=$(basename "$1" | sed 's/\.txt$//')
            cat > "$output_file" << EOF
# 🔧 $(basename "$(dirname "$output_file")" | sed 's|/README.md||') - What You Need to Know

## 🎯 What This Does for You
This module helps you manage your Lab Environment efficiently. Whether you're a developer, system administrator, or DevOps engineer, this tool streamlines your workflow and reduces manual configuration tasks.

**Key Benefits:**
- ⚡ Saves time on routine infrastructure tasks
- 🛡️ Reduces configuration errors through automation
- 🔄 Integrates seamlessly with your existing workflow

## 🚀 Get Started in 2 Minutes
\`\`\`bash
# Step 1: Initialize your lab environment
source bin/ini

# Step 2: Navigate to the module
cd $(dirname "$output_file")

# Step 3: You're ready to go!
\`\`\`

## 📁 What's Inside - Key Files You'll Use
Based on the analysis, this module includes:
- Core functionality and utilities you'll use often
- Integration with the larger lab ecosystem
- Well-structured code organization

## 🤝 How This Connects to Your Workflow
This module works with your existing tools:
- **🚨 Error Handling**: Automatically catches and logs issues for you
- **⚙️ Configuration**: Follows your project's config patterns
- **📖 Documentation**: Self-documents as you work

## 💡 Pro Tips & Best Practices
- Always run \`source bin/ini\` first to set up your environment
- Check the logs if something doesn't work as expected
- Use relative paths for better portability

## 🆘 Need Help? Find More Info
- [📚 Main Documentation](../../README.md) - Complete system guide
- [🎓 Standards & Conventions](../standards.md) - Follow the project patterns
- [🐛 Troubleshooting](../troubleshooting.md) - Common issues and solutions

---

**Navigation**: Return to [Parent Directory](../README.md) | [Main Lab Documentation](../../README.md)

*Generated by AI Documentation System - $(date)*
EOF
            ;;
        *)
            error "Unknown AI service: $AI_SERVICE"
            return 1
            ;;
    esac
}

# Main function to generate documentation
generate_doc() {
    local target_dir="$1"
    local force="${2:-false}"
    
    if [[ ! -d "$target_dir" ]]; then
        error "Directory not found: $target_dir"
        return 1
    fi
    
    local readme_path="$target_dir/README.md"
    
    if [[ -f "$readme_path" && "$force" != "true" ]]; then
        log "README exists: $readme_path (use --force to overwrite)"
        return 0
    fi
    
    log "🤖 Generating documentation for: $target_dir"
    
    # Step 1: Collect metadata
    collect_metadata "$target_dir" "$WORK_DIR/metadata.json"
    
    # Step 2: Build prompt
    build_prompt "$WORK_DIR/metadata.json" "$WORK_DIR/prompt.txt"
    
    # Step 3: Call AI
    if ! call_ai "$WORK_DIR/prompt.txt" "$WORK_DIR/output.md"; then
        error "AI call failed"
        return 1
    fi
    
    # Step 4: Validate and save
    if [[ -s "$WORK_DIR/output.md" ]]; then
        cp "$WORK_DIR/output.md" "$readme_path"
        success "Generated: $readme_path"
    else
        error "AI generated empty output"
        return 1
    fi
}

# Hierarchical generation (bottom-up)
generate_hierarchical() {
    local root_dir="$1"
    local max_depth="${2:-3}"
    local force="${3:-false}"
    
    log "🏗️  Hierarchical generation from: $root_dir (depth: $max_depth)"
    
    # Find directories, sorted by depth (deepest first)
    local dirs=()
    while IFS= read -r -d '' dir; do
        dirs+=("$dir")
    done < <(find "$root_dir" -maxdepth "$max_depth" -type d -print0 | sort -rz)
    
    log "Found ${#dirs[@]} directories"
    
    local success_count=0
    for dir in "${dirs[@]}"; do
        if generate_doc "$dir" "$force"; then
            ((success_count++))
        fi
    done
    
    success "Completed: $success_count/${#dirs[@]} directories"
}

# Usage
usage() {
    cat << EOF
AI Documentation Generator - Working Implementation

USAGE:
    $0 <directory>                    # Generate for single directory
    $0 --hierarchical <root_dir>      # Generate hierarchically
    $0 --help                         # Show this help

OPTIONS:
    --force                           # Overwrite existing READMEs
    --hierarchical                    # Bottom-up generation
    --max-depth N                     # Max depth for hierarchical (default: 3)
    --ai-service SERVICE              # AI service: ollama, openai, gemini, mock

EXAMPLES:
    # Test with mock AI
    $0 /home/es/lab/lib/core/err
    
    # Generate hierarchically
    $0 --hierarchical /home/es/lab/lib
    
    # Use with OpenAI
    AI_SERVICE=openai OPENAI_API_KEY=sk-... $0 /home/es/lab/lib
    
    # Use with Gemini
    AI_SERVICE=gemini GEMINI_API_KEY=your-key $0 /home/es/lab/lib

ENVIRONMENT:
    AI_SERVICE      # AI service to use (default: mock)
    OPENAI_API_KEY  # Required for OpenAI service
    GEMINI_API_KEY  # Required for Gemini service
    LAB_ROOT        # Lab root directory
EOF
}

# Main script
main() {
    local hierarchical=false
    local force=false
    local max_depth=3
    local target_dir=""
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --hierarchical) hierarchical=true; shift ;;
            --force) force=true; shift ;;
            --max-depth) max_depth="$2"; shift 2 ;;
            --ai-service) AI_SERVICE="$2"; shift 2 ;;
            --help) usage; exit 0 ;;
            -*) error "Unknown option: $1"; usage; exit 1 ;;
            *) target_dir="$1"; shift ;;
        esac
    done
    
    if [[ -z "$target_dir" ]]; then
        # When called from orchestrator without parameters, use lab root
        if [[ -n "${LAB_DIR:-}" ]]; then
            target_dir="$LAB_DIR"
            log "Using LAB_DIR as target: $target_dir"
        else
            error "No directory specified"
            usage
            exit 1
        fi
    fi
    
    # Initialize
    init
    
    log "Starting AI Documentation Generator"
    log "Target: $target_dir"
    log "AI Service: $AI_SERVICE"
    
    # Generate
    if [[ "$hierarchical" == "true" ]]; then
        generate_hierarchical "$target_dir" "$max_depth" "$force"
    else
        generate_doc "$target_dir" "$force"
    fi
    
    success "🎉 AI Documentation Generator complete!"
}

# Run if called directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
