#!/bin/bash
# AI Documentation Generator - Complete Working Implementation
# This integrates with your existing tools and provides the AI layer

set -euo pipefail

# Configuration
AI_SERVICE="${AI_SERVICE:-mock}"  # Options: ollama, openai, mock
WORK_DIR="/tmp/ai_doc_work_$$"
LAB_ROOT="${LAB_ROOT:-/home/es/lab}"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() { echo -e "${BLUE}[AI-DOC]${NC} $1"; }
success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
error() { echo -e "${RED}[ERROR]${NC} $1" >&2; }

# Initialize work directory
init() {
    mkdir -p "$WORK_DIR"
    trap "rm -rf '$WORK_DIR'" EXIT
}

# Collect metadata using your existing tools
collect_metadata() {
    local target_dir="$1"
    local output_file="$2"
    
    log "Collecting metadata for: $target_dir"
    
    # Initialize JSON structure
    cat > "$output_file" << 'EOF'
{
  "directory": "",
  "timestamp": "",
  "functions": [],
  "variables": [],
  "documentation": [],
  "files": []
}
EOF
    
    # Add basic info
    jq --arg dir "$target_dir" --arg ts "$(date -Iseconds)" \
       '.directory = $dir | .timestamp = $ts' \
       "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    
    # Use aux_laf for functions (if available)
    if command -v aux_laf >/dev/null 2>&1 && [[ -d "$target_dir" ]]; then
        log "Analyzing functions with aux_laf..."
        if aux_laf "$target_dir" -j > "$WORK_DIR/functions.json" 2>/dev/null; then
            jq --slurpfile funcs "$WORK_DIR/functions.json" '.functions = $funcs[0] // []' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # Use aux_acu for variables (if available)
    if command -v aux_acu >/dev/null 2>&1 && [[ -d "$target_dir" ]]; then
        log "Analyzing variables with aux_acu..."
        if aux_acu -j "$target_dir" > "$WORK_DIR/variables.json" 2>/dev/null; then
            jq --slurpfile vars "$WORK_DIR/variables.json" '.variables = $vars[0] // []' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # Use aux_lad for documentation (if available)
    if command -v aux_lad >/dev/null 2>&1 && [[ -d "$target_dir" ]]; then
        log "Analyzing documentation with aux_lad..."
        if aux_lad -j "$target_dir" > "$WORK_DIR/docs.json" 2>/dev/null; then
            jq --slurpfile docs "$WORK_DIR/docs.json" '.documentation = $docs[0] // []' \
               "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
        fi
    fi
    
    # Basic file listing
    if [[ -d "$target_dir" ]]; then
        find "$target_dir" -maxdepth 1 -type f -printf '%f\n' 2>/dev/null | \
        jq -R . | jq -s '.' > "$WORK_DIR/files.json"
        jq --slurpfile files "$WORK_DIR/files.json" '.files = $files[0]' \
           "$output_file" > "$output_file.tmp" && mv "$output_file.tmp" "$output_file"
    fi
    
    success "Metadata collected"
}

# Build AI prompt
build_prompt() {
    local metadata_file="$1"
    local prompt_file="$2"
    
    log "Building AI prompt..."
    
    # Get example README for style
    local style_example=""
    if [[ -f "$LAB_ROOT/README.md" ]]; then
        style_example=$(head -20 "$LAB_ROOT/README.md" | sed 's/"/\\"/g')
    fi
    
    cat > "$prompt_file" << EOF
You are a technical documentation expert. Generate a comprehensive README.md file based on the following directory analysis:

DIRECTORY METADATA:
$(cat "$metadata_file")

EXISTING STYLE EXAMPLE:
$style_example

REQUIREMENTS:
1. Use appropriate emojis (🔧, 📋, 🎯, 🚀, etc.)
2. Create clear markdown structure
3. Document all functions and components found
4. Explain the directory's purpose
5. Include usage examples
6. Be concise but comprehensive
7. Follow the existing project style

OUTPUT: Provide ONLY the README.md content, no additional text.
EOF
}

# Call AI service
call_ai() {
    local prompt_file="$1"
    local output_file="$2"
    
    log "Calling AI service: $AI_SERVICE"
    
    case "$AI_SERVICE" in
        "ollama")
            if command -v curl >/dev/null 2>&1; then
                curl -X POST http://localhost:11434/api/generate \
                    -H "Content-Type: application/json" \
                    -d "{\"model\":\"codellama\",\"prompt\":\"$(cat "$prompt_file" | jq -Rs .)\",\"stream\":false}" \
                    2>/dev/null | jq -r '.response // "Error: No response"' > "$output_file"
            else
                error "curl not available for Ollama"
                return 1
            fi
            ;;
        "openai")
            if [[ -z "${OPENAI_API_KEY:-}" ]]; then
                error "OPENAI_API_KEY not set"
                return 1
            fi
            curl -X POST https://api.openai.com/v1/chat/completions \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d "{\"model\":\"gpt-4\",\"messages\":[{\"role\":\"user\",\"content\":$(cat "$prompt_file" | jq -Rs .)}]}" \
                2>/dev/null | jq -r '.choices[0].message.content // "Error: No response"' > "$output_file"
            ;;
        "mock")
            log "Using mock AI (for testing)"
            local dir_name=$(basename "$1" | sed 's/\.txt$//')
            cat > "$output_file" << EOF
# 🔧 $(basename "$(dirname "$output_file")" | sed 's|/README.md||') Module

## 📋 Overview
This directory contains important functionality for the lab infrastructure.

## 🚀 Components
Based on the analysis, this module includes:
- Core functionality and utilities
- Integration with the larger lab ecosystem
- Well-structured code organization

## 📚 Usage
This module is designed to be used as part of the lab infrastructure.

## 🔗 Related
- [Lab Root](../../README.md)
- [Documentation Hub](../README.md)

---
*Generated by AI Documentation System - $(date)*
EOF
            ;;
        *)
            error "Unknown AI service: $AI_SERVICE"
            return 1
            ;;
    esac
}

# Main function to generate documentation
generate_doc() {
    local target_dir="$1"
    local force="${2:-false}"
    
    if [[ ! -d "$target_dir" ]]; then
        error "Directory not found: $target_dir"
        return 1
    fi
    
    local readme_path="$target_dir/README.md"
    
    if [[ -f "$readme_path" && "$force" != "true" ]]; then
        log "README exists: $readme_path (use --force to overwrite)"
        return 0
    fi
    
    log "🤖 Generating documentation for: $target_dir"
    
    # Step 1: Collect metadata
    collect_metadata "$target_dir" "$WORK_DIR/metadata.json"
    
    # Step 2: Build prompt
    build_prompt "$WORK_DIR/metadata.json" "$WORK_DIR/prompt.txt"
    
    # Step 3: Call AI
    if ! call_ai "$WORK_DIR/prompt.txt" "$WORK_DIR/output.md"; then
        error "AI call failed"
        return 1
    fi
    
    # Step 4: Validate and save
    if [[ -s "$WORK_DIR/output.md" ]]; then
        cp "$WORK_DIR/output.md" "$readme_path"
        success "Generated: $readme_path"
    else
        error "AI generated empty output"
        return 1
    fi
}

# Hierarchical generation (bottom-up)
generate_hierarchical() {
    local root_dir="$1"
    local max_depth="${2:-3}"
    local force="${3:-false}"
    
    log "🏗️  Hierarchical generation from: $root_dir (depth: $max_depth)"
    
    # Find directories, sorted by depth (deepest first)
    local dirs=()
    while IFS= read -r -d '' dir; do
        dirs+=("$dir")
    done < <(find "$root_dir" -maxdepth "$max_depth" -type d -print0 | sort -rz)
    
    log "Found ${#dirs[@]} directories"
    
    local success_count=0
    for dir in "${dirs[@]}"; do
        if generate_doc "$dir" "$force"; then
            ((success_count++))
        fi
    done
    
    success "Completed: $success_count/${#dirs[@]} directories"
}

# Usage
usage() {
    cat << EOF
AI Documentation Generator - Working Implementation

USAGE:
    $0 <directory>                    # Generate for single directory
    $0 --hierarchical <root_dir>      # Generate hierarchically
    $0 --help                         # Show this help

OPTIONS:
    --force                           # Overwrite existing READMEs
    --hierarchical                    # Bottom-up generation
    --max-depth N                     # Max depth for hierarchical (default: 3)
    --ai-service SERVICE              # AI service: ollama, openai, mock

EXAMPLES:
    # Test with mock AI
    $0 /home/es/lab/lib/core/err
    
    # Generate hierarchically
    $0 --hierarchical /home/es/lab/lib
    
    # Use with OpenAI
    AI_SERVICE=openai OPENAI_API_KEY=sk-... $0 /home/es/lab/lib

ENVIRONMENT:
    AI_SERVICE      # AI service to use (default: mock)
    OPENAI_API_KEY  # Required for OpenAI service
    LAB_ROOT        # Lab root directory
EOF
}

# Main script
main() {
    local hierarchical=false
    local force=false
    local max_depth=3
    local target_dir=""
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            --hierarchical) hierarchical=true; shift ;;
            --force) force=true; shift ;;
            --max-depth) max_depth="$2"; shift 2 ;;
            --ai-service) AI_SERVICE="$2"; shift 2 ;;
            --help) usage; exit 0 ;;
            -*) error "Unknown option: $1"; usage; exit 1 ;;
            *) target_dir="$1"; shift ;;
        esac
    done
    
    if [[ -z "$target_dir" ]]; then
        error "No directory specified"
        usage
        exit 1
    fi
    
    # Initialize
    init
    
    log "Starting AI Documentation Generator"
    log "Target: $target_dir"
    log "AI Service: $AI_SERVICE"
    
    # Generate
    if [[ "$hierarchical" == "true" ]]; then
        generate_hierarchical "$target_dir" "$max_depth" "$force"
    else
        generate_doc "$target_dir" "$force"
    fi
    
    success "🎉 AI Documentation Generator complete!"
}

# Run if called directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
