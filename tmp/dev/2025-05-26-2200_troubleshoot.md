<!--
#######################################################################
# Development Troubleshooting - RTX 50 Series GPU Passthrough Issues
#######################################################################
# File: /home/es/lab/tmp/dev/2025-05-26-2200_troubleshoot.md
# Description: Investigation and troubleshooting documentation for
#              RTX 5060 Ti GPU passthrough issues on Proxmox node x1
#              with comprehensive diagnostic and resolution procedures.
#
# Session Context:
#   Focused troubleshooting session for RTX 50 series GPU passthrough
#   configuration issues where technical setup succeeds but display
#   output fails on specific Proxmox infrastructure nodes.
#
# Technical Scope:
#   - RTX 5060 Ti GPU passthrough analysis
#   - Proxmox node x1 specific configuration
#   - Display output troubleshooting procedures
#   - VM configuration validation and optimization
#
# Target Audience:
#   Hardware specialists, Proxmox administrators, and GPU passthrough
#   experts working on high-end graphics card virtualization and
#   display output configuration in enterprise environments.
#######################################################################
-->

# RTX 50 Series GPU Passthrough Issues

## Overview

This document details the investigation and troubleshooting steps for RTX 5060 Ti GPU passthrough issues on Proxmox node `x1`. The GPU passthrough setup works correctly on other nodes but fails to display output on this specific node despite successful technical configuration.

## Problem Description

**Symptoms:**
- GPU detaches from host successfully (`gpu-ptd` works without errors)
- VM starts and runs without errors
- GPU is properly bound to vfio-pci driver
- **No display output** from the GPU in the VM
- Same configuration works on other Proxmox nodes

**Hardware:**
- GPU: NVIDIA RTX 5060 Ti (Device ID: `10de:2d04`)
- Node: Proxmox x1 (6.8.12-10-pve kernel)
- VM: Windows 11 (VM ID 111)

## Initial Configuration

**VM Configuration:**
```
hostpci0: 0000:3b:00.0,pcie=1,x-vga=1
hostpci1: 0000:3b:00.1,pcie=1
bios: ovmf
machine: pc-q35-9.2+pve1
```

**Kernel Parameters (Initial):**
```
GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt"
```

## Troubleshooting Steps Attempted

### 1. PCIe Power Management Fix

Based on similar RTX 50 series issues reported in forums, we tried disabling PCIe power management:

**Added kernel parameter:**
```bash
GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt pcie_port_pm=off"
```

**Result:** No improvement in display output.

### 2. Additional PCIe Compatibility Parameters

Added more RTX 50 series compatibility parameters:

**Updated kernel parameters:**
```bash
GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt pcie_port_pm=off pci=nommconf pcie_aspm=off"
```

**Result:** No improvement in display output.

### 3. ROM BAR Configuration

**Attempted:**
```bash
qm set 111 -hostpci0 0000:3b:00.0,pcie=1,x-vga=1,rombar=1
```

**Result:** Caused system instability, VM hangs. Configuration was reverted.

### 4. NVIDIA DRM Modeset Parameter

Based on forum reports specifically for RTX 50 series, implemented the `nvidia_drm modeset=1` parameter:

**Created `/etc/modprobe.d/nvidia.conf`:**
```
options nvidia_drm modeset=1
```

**Initial Issue:** Typo in configuration file (`modset=1` instead of `modeset=1`)

**After Fix:**
```bash
# Verification shows parameter is applied
cat /sys/module/nvidia_drm/parameters/modeset
# Output: Y
```

**Current Status:** Parameter correctly applied on host, but **FAILED** - no display output achieved with Windows VM.

**Important Note:** The forum solution specifically mentioned Linux guests, but our testing was done with Windows 11 VM. The `nvidia_drm modeset=1` parameter only affects the host system, not the Windows guest drivers.

### 5. Windows vs Linux Guest OS Issue Discovery

**Key Realization:** The forum solution that appeared promising was specifically designed for Linux guest VMs, not Windows VMs.

**Forum Solution Context:**
- Solution mentioned installing nvidia driver "inside the VM" 
- Required `nvidia_drm modeset=1` on "both proxmox and your linux guest"
- Specifically stated: "I'm using linux guest btw"

**Why This Failed for Windows:**
```bash
# This parameter only affects Linux DRM subsystem
options nvidia_drm modeset=1

# Windows VMs use different graphics initialization:
# - Windows Display Driver Model (WDDM)
# - Direct3D/DirectX graphics stack  
# - No Linux DRM kernel mode setting
```

**Testing Results:**
- ✅ Host `nvidia_drm modeset=1` applied successfully
- ❌ No effect on Windows 11 VM display output
- ❌ Windows VM still shows black screen despite proper GPU passthrough
- ❌ Forum solution confirmed irrelevant for Windows guests

**Conclusion:** Need Windows-specific RTX 50 series passthrough solutions, not Linux-based workarounds.

## Technical Analysis

### Successful Operations
- ✅ GPU detachment works perfectly (`gpu-ptd`)
- ✅ VFIO binding successful
- ✅ VM starts without errors
- ✅ No PCIe power management errors in dmesg
- ✅ GPU shows "active" power state
- ✅ `nvidia_drm modeset=1` applied on host (confirmed)

### Key Observations

**VFIO Messages in dmesg:**
```
vfio-pci 0000:3b:00.0: vgaarb: deactivate vga console
vfio-pci 0000:3b:00.0: vgaarb: VGA decodes changed: olddecodes=none,decodes=io+mem:owns=none
vfio-pci 0000:3b:00.0: Enabling HDA controller
```

**Power State Check:**
```bash
cat /sys/bus/pci/devices/0000:3b:00.0/power/runtime_status
# Output: active
```

## Current Hypothesis

The issue appears to be related to RTX 50 series initialization requirements rather than traditional PCIe power management problems. 

**Critical Discovery:** The forum solution that suggested `nvidia_drm modeset=1` was specifically for **Linux guests**, not Windows VMs. Our testing was done with a Windows 11 VM, which explains why this approach failed.

The forum post mentioned a "chicken and egg" problem for Linux VMs:
1. RTX 50 series cards need NVIDIA drivers installed in the Linux VM to provide display output
2. Display output is needed to install NVIDIA drivers  
3. The `nvidia_drm modeset=1` parameter on both host and Linux guest helps break this cycle

**For Windows VMs:** The `nvidia_drm modeset=1` parameter only affects the Linux host system and has no impact on Windows guest drivers. Windows uses different display initialization mechanisms that don't rely on DRM kernel mode setting.

## Next Steps

**Priority Update:** Since the forum solution was Linux-specific and we're using Windows 11, we need Windows-specific approaches.

### Option 1: Windows Virtual GPU Installation Method
1. Temporarily remove GPU passthrough from Windows VM
2. Add virtual display (QXL/VGA) to Windows VM
3. Boot Windows with virtual display
4. Install NVIDIA drivers via Windows Update or NVIDIA website
5. Shut down VM, remove virtual display, re-add GPU passthrough
6. Test display output with NVIDIA drivers pre-installed in Windows

### Option 2: UEFI GOP/CSM Investigation
Research Windows-specific RTX 50 series initialization issues:
- Check if UEFI GOP (Graphics Output Protocol) support is needed
- Investigate CSM (Compatibility Support Module) settings
- Test different OVMF/SeaBIOS configurations

### Option 3: Compare Working Node (Windows-focused)
Investigate differences between working node and current node:
- Windows VM configurations on working node
- OVMF versions and settings
- Any Windows-specific workarounds applied

### Option 4: Alternative Windows Solutions
Research RTX 50 series + Windows VM specific fixes:
- Windows driver installation methods for headless systems
- Alternative Windows display initialization approaches
- RTX 50 series Windows VM compatibility reports

## Configuration Files Status

**Current Kernel Parameters:**
```bash
cat /proc/cmdline
# BOOT_IMAGE=/boot/vmlinuz-6.8.12-10-pve root=/dev/mapper/pve-root ro quiet iommu=pt pcie_port_pm=off
```

**NVIDIA DRM Configuration:**
```bash
cat /etc/modprobe.d/nvidia.conf
# options nvidia_drm modeset=1

cat /sys/module/nvidia_drm/parameters/modeset
# Y
```

**GPU Status:**
```bash
gpu-pts
# Shows GPU properly detached and bound to vfio-pci
```

## References

- Forum post describing RTX 50 series issues **for Linux guests** (not applicable to Windows VMs)
- Solution involving `nvidia_drm modeset=1` parameter (Linux-specific, host-only effect)
- Warning about rombar incompatibility with RTX 50 series
- Need for Windows-specific RTX 50 series passthrough solutions

## Lessons Learned

- **OS-Specific Solutions:** Linux VM solutions don't translate to Windows VMs
- **Host vs Guest Parameters:** `nvidia_drm modeset=1` only affects Linux host, not Windows guest
- **Forum Context Important:** Must verify if solutions apply to specific guest OS
- **RTX 50 Series Complexity:** New generation requires OS-specific workarounds

## Notes

- RTX 50 series cards are relatively new and may require OS-specific workarounds
- Traditional PCIe power management fixes don't resolve RTX 50 + Windows VM issues
- The `nvidia_drm modeset=1` parameter is for Linux systems only, not Windows VMs
- ROM BAR should be avoided with RTX 50 series cards
- **Windows VMs need different initialization approaches than Linux VMs**

---

**Last Updated:** May 27, 2025  
**Status:** Linux-specific solutions tested and failed. Need Windows-specific RTX 50 series approaches.