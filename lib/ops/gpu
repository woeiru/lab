#!/bin/bash

# ============================================================================
# gpu - Function Summary
#
#   gpu-fun : Shows a summary of selected functions in the script.
#   gpu-var : Displays an overview of specific variables defined in the configuration file.
#   gpu-nds : Downloads and installs NVIDIA drivers, blacklisting Nouveau for host use.
#   gpu-pt1 : Configures initial GRUB and EFI settings for GPU passthrough, installs packages, and reboots.
#   gpu-pt2 : Adds kernel modules for GPU passthrough, updates initramfs, and reboots.
#   gpu-pt3 : Finalizes or reverts GPU passthrough setup (VFIO-PCI IDs, blacklisting drivers for passthrough).
#   gpu-ptd : Detaches the GPU from the host system for VM passthrough (dynamic).
#   gpu-pta : Attaches the GPU back to the host system (dynamic).
#   gpu-pts : Checks the current status of the GPU passthrough setup.
#
# ============================================================================
#
# ============================================================================
# GPU Driver and Passthrough Function Interactions - Technical Overview
#
# This section outlines crucial interactions between different GPU management
# functions, particularly concerning driver blacklisting, preferences, and
# dynamic vs. persistent configurations.
#
# Persistent Configuration (Primarily via `gpu-pt1`, `gpu-pt2`, `gpu-pt3`):
#   - These functions establish the boot-time default state for GPU passthrough.
#   - `gpu-pt3 enable`: Configures the system to assign specified GPUs to `vfio-pci`
#     at boot, creating persistent blacklists for host drivers for those GPUs.
#     This is the most robust way to dedicate GPUs for passthrough.
#   - `gpu-pt3 disable`: Reverts the persistent passthrough configuration, allowing
#     host drivers to claim GPUs at boot (defaulting to host usage).
#
# Dynamic Switching (`gpu-ptd`, `gpu-pta`):
#   - These functions allow runtime switching of GPU control between the host and
#     `vfio-pci` (for VMs) *without requiring a reboot*.
#   - They are designed to be as self-sufficient as possible, meaning they can
#     often work even if the persistent state set by `gpu-pt3` is for host usage
#     (i.e., after `gpu-pt3 disable` or if `gpu-pt3 enable` was never run).
#   - `gpu-ptd` (Detach): Attempts to dynamically prepare the system for passthrough
#     by loading necessary VFIO modules, unbinding host drivers, and binding `vfio-pci`.
#   - `gpu-pta` (Attach): Attempts to dynamically return GPU control to the host by
#     unbinding `vfio-pci`, loading necessary host drivers, and binding them.
#
# Blacklist Files:
#   - `/etc/modprobe.d/blacklist-nouveau.conf`:
#       - Created/Managed by: `gpu-nds` (NVIDIA Driver Setup).
#       - Purpose: Persistently blacklists `nouveau` if NVIDIA proprietary drivers
#         are installed for host use.
#
#   - `/etc/modprobe.d/zz-vfio-gpu-blacklist.conf`:
#       - Created/Managed by: `gpu-pt3 enable`.
#       - Removed by: `gpu-pt3 disable`.
#       - Purpose: Persistently blacklists host drivers (`nouveau`, `nvidia`,
#         `amdgpu`, `radeon`) for GPUs intended for passthrough at boot.
#         `gpu-ptd` does *not* create this file but attempts to achieve a similar
#         effect for the current session by unloading host drivers.
#
# Key Interactions & Considerations:
#
# 1. `gpu-nds` (NVIDIA Driver Setup):
#    - Installs NVIDIA proprietary drivers and can blacklist `nouveau` for host use.
#    This is independent of passthrough but affects which driver (`nvidia` or `nouveau`)
#    `gpu-pta` might try to load for an NVIDIA GPU if it's the host preference.
#
# 2. `gpu-pt1`, `gpu-pt2` (Initial Passthrough Setup):
#    - These set up essential prerequisites like IOMMU kernel parameters and ensure
#      VFIO kernel modules are listed in `/etc/modules`.
#    - While `gpu-ptd` attempts to load VFIO modules dynamically, having these base
#      prerequisites met (especially IOMMU in kernel cmdline) is highly recommended
#      for `gpu-ptd` to succeed reliably.
#
# 3. `gpu-pt3 enable` (Persistent Passthrough):
#    - Provides the most stable environment for passthrough by ensuring GPUs are
#      bound to `vfio-pci` from boot and host drivers are blacklisted.
#    - If you primarily use GPUs for passthrough, this is the recommended default state.
#
# 4. `gpu-pt3 disable` (Persistent Host GPU Usage):
#    - Sets the system to use GPUs with host drivers by default after boot.
#    - This is the state from which `gpu-ptd` can be used to dynamically switch
#      a GPU to `vfio-pci` for a VM session.
#
# 5. `gpu-ptd` (Dynamic Detach for VM):
#    - Will attempt to load `vfio`, `vfio_iommu_type1`, and `vfio_pci` if not present.
#    - Will attempt to unbind host drivers (e.g., `nvidia`, `nouveau`, `amdgpu`).
#    - Uses `driver_override` to bind `vfio-pci` to the target GPU(s).
#    - Its changes are not persistent across reboots.
#    - If IOMMU is not enabled in the kernel, `gpu-ptd` will warn but cannot fix it.
#
# 6. `gpu-pta` (Dynamic Attach to Host):
#    - Unbinds `vfio-pci`.
#    - Clears `driver_override`.
#    - Attempts to load the preferred/appropriate host driver (e.g., `nvidia`, `nouveau`, `amdgpu`)
#      if not already loaded, and binds it to the GPU.
#    - Considers `*_nvidia_driver_preference` from site config for NVIDIA GPUs.
#      Warns if `nouveau` is preferred but blacklisted by `gpu-nds`.
#
# General Workflow Examples:
#
#   Initial One-Time Setup (Common for both scenarios below):
#     1. Run `gpu-pt1`: Configures GRUB/EFI for IOMMU, installs related packages.
#        Reboot as prompted.
#     2. Run `gpu-pt2`: Adds necessary kernel modules (vfio, vfio_iommu_type1, vfio_pci)
#        to /etc/modules for them to be loaded at boot, updates initramfs.
#        Reboot as prompted.
#     3. (Optional) Run `gpu-nds`: If using an NVIDIA GPU and proprietary drivers are
#        desired for host usage (when GPU is attached to host). This blacklists nouveau
#        and installs the NVIDIA driver. Reboot if prompted.
#
#   Scenario A: GPU Primarily for Passthrough (Persistent Setup)
#     (After completing the Initial One-Time Setup)
#     a. Run `gpu-pt3 enable`: Assigns specified GPU(s) to `vfio-pci` at boot.
#        This involves creating /etc/modprobe.d/vfio.conf with the GPU IDs and
#        blacklisting their host drivers (e.g., nvidia, amgpu, nouveau) in
#        /etc/modprobe.d/zz-vfio-gpu-blacklist.conf. Reboot.
#        The GPU is now persistently configured for VM passthrough.
#     b. (Optional) If the GPU needs to be temporarily used by the host:
#        Run `gpu-pta` to attach it to host drivers.
#        Run `gpu-ptd` to detach it again for VM use. (No reboots needed for these).
#
#   Scenario B: GPU Primarily for Host, Dynamic Passthrough as Needed
#     (After completing the Initial One-Time Setup)
#     a. Ensure the system boots with the GPU attached to host drivers.
#        This is the default state if `gpu-pt3 enable` has not been run for the GPU,
#        or if `gpu-pt3 disable` was run to revert a persistent passthrough setup.
#        (If `gpu-nds` was run, the NVIDIA GPU will use proprietary drivers; otherwise,
#        it might use nouveau or amdgpu for AMD).
#     b. When passthrough is needed for a VM: Run `gpu-ptd` for the target GPU.
#        This dynamically attempts to:
#          - Ensure VFIO modules are loaded (they should be if `gpu-pt2` was run).
#          - Unbind the GPU from its host driver.
#          - Bind the GPU to `vfio-pci`.
#     c. After VM use: Run `gpu-pta` for the target GPU. This dynamically:
#          - Unbinds the GPU from `vfio-pci`.
#          - Rebinds the GPU to its appropriate host driver.
#     d. Note: If dynamic switching (`gpu-ptd`) faces issues (e.g., host drivers
#        re-asserting control, or instability), using `gpu-pt3 enable` (Scenario A)
#        for a persistent passthrough setup is generally more robust.
#
# ============================================================================

# Define directory and file variables
DIR_FUN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
FILE_FUN=$(basename "$BASH_SOURCE")
BASE_FUN="${FILE_FUN%.*}"
FILEPATH_FUN="${DIR_FUN}/${FILE_FUN}"
CONFIG_FUN="${SITE_CONFIG_FILE}"

# Dynamically create variables based on the base name
eval "FILEPATH_${BASE_FUN}=\$FILEPATH_FUN"
eval "FILE_${BASE_FUN}=\$FILE_FUN"
eval "BASE_${BASE_FUN}=\$BASE_FUN"
eval "CONFIG_${BASE_FUN}=\$CONFIG_FUN"

# Shows a summary of selected functions in the script, displaying their usage, shortname, and description
# overview functions
#
gpu-fun() {

    # Technical Description:
    #   Lists selected functions from this script.
    #   Displays usage, shortname, and description for each.
    #   Likely parses comments or uses a predefined list.
    # Dependencies:
    #   - Relies on conventions for function comments if parsing.

    # Pass all arguments directly to aux-laf
    aux-laf "$FILEPATH_gpu" "$@"
}

# Displays an overview of specific variables defined in the configuration file, showing their names, values, and usage across different files
# overview variables
#
gpu-var() {

    # Technical Description:
    #   Shows details of configuration variables.
    #   Information includes name, value, and usage context.
    #   Likely reads from SITE_CONFIG_FILE and parses it.
    # Dependencies:
    #   - SITE_CONFIG_FILE` env`ironment variable.
    #   - Text processing tools (e.g. `grep`, `awk`).

    aux-acu -o "$CONFIG_gpu" "$DIR_FUN/.."
}

# Downloads and installs NVIDIA drivers, blacklisting Nouveau
# nvidia driver setup
# <driver_version> (optional)
gpu-nds() {
  local drv_ver="${1:-550.142}"  # default driver version; override as needed
  local url="https://us.download.nvidia.com/XFree86/Linux-x86_64/${drv_ver}/NVIDIA-Linux-x86_64-${drv_ver}.run"
  local installer="NVIDIA-Linux-x86_64-${drv_ver}.run"

  # Technical Description:
  #   Downloads a specific NVIDIA driver version (default or specified).
  #   Blacklists the Nouveau driver by creating a modprobe.d configuration file.
  #   Updates the initramfs.
  #   Installs prerequisite packages (dkms, build-essential, pve-headers).
  #   Makes the downloaded driver installer executable.
  #   Runs the NVIDIA installer silently with DKMS support.
  #   Prompts the user to reboot.
  # Dependencies:
  #   - `tee`, `update-initramfs`, `apt`, `wget`, `chmod`, `reboot`.
  #   - Root privileges for most operations.
  #   - Internet connectivity for downloading the driver.
  #   - `uname -r` to get the current kernel version for PVE headers.

  echo "1) Blacklisting Nouveau..."
  cat <<EOF | tee /etc/modprobe.d/blacklist-nouveau.conf
blacklist nouveau
options nouveau modeset=0
EOF
  update-initramfs -u   # rebuild initramfs

  echo "2) Installing prerequisites..."
  apt update
  apt install -y dkms build-essential pve-headers-$(uname -r)   # DKMS & kernel headers

  echo "3) Downloading NVIDIA driver ${drv_ver}..."
  wget -q "${url}" -O "${installer}" || { echo "Download failed"; return 1; }

  chmod +x "${installer}"

  echo "4) Installing driver with DKMS support..."
  ./"${installer}" --dkms --silent  # silent install, auto DKMS

  echo "5) Finalizingâ€”reboot recommended."
  read -p "Reboot now? [y/N] " yn
  if [[ "$yn" =~ ^[Yy]$ ]]; then
    reboot
  else
    echo "You can reboot later with: reboot"
  fi

  echo "6) After reboot, verify with: nvidia-smi"   # verify driver loaded
}


# Configures initial GRUB and EFI settings for GPU passthrough, installs necessary packages, and reboots the system
# gpu passthrough step 1
#
gpu-pt1() {
    local function_name="${FUNCNAME[0]}"
    echo "Executing section 1:"

    # Display EFI boot information
    efibootmgr -v

    # Edit GRUB configuration
    sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="quiet"/GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt"/' /etc/default/grub
    update-grub
    update-grub2 # Corrected typo from update-grug2

    # Technical Description:
    #   Installs 'grub-efi-amd64' package for EFI systems.
    #   Notifies completion using 'printf', using 'function_name'.
    #   Forces a system reboot. This is step 1 of GPU passthrough setup.
    # Dependencies:
    #   - `apt` command for package installation.
    #   - `printf` function (for notifications).
    #   - `reboot` command.
    #   - Root privileges for apt and reboot.

    # Install grub-efi-amd64
    apt install grub-efi-amd64 -y

    # Notify status
    printf "%s: Completed section 1, system will reboot now.\\n" "$function_name"

    # Perform system reboot without prompting
    reboot
}

# Adds necessary kernel modules for GPU passthrough to /etc/modules, updates initramfs, and reboots the system
# gpu passthrough step 2
#
gpu-pt2() {
    local function_name="${FUNCNAME[0]}"
    echo "Executing section 2:"

    # Technical Description:
    #   Appends 'vfio', 'vfio_iommu_type1', 'vfio_pci' to /etc/modules.
    #   Updates initramfs for all kernels. Notifies completion via 'printf'.
    #   Forces a system reboot. This is step 2 of GPU passthrough setup.
    # Dependencies:
    #   - `echo` (to modify /etc/modules), `update-initramfs`.
    #   - `printf` function.
    #   - `reboot` command.
    #   - Root privileges for file edits and reboot.
    #   - Assumes /etc/modules is the correct file for loading modules.

    # Add modules to /etc/modules
    echo "vfio" >> /etc/modules
    echo "vfio_iommu_type1" >> /etc/modules
    echo "vfio_pci" >> /etc/modules

    # Update initramfs
    update-initramfs -u -k all

    # Notify status
    printf "%s: Completed section 2, system will reboot now.\\n" "$function_name"

    # Perform system reboot without prompting
    reboot
}

# Finalizes or reverts GPU passthrough setup by configuring or removing VFIO-PCI IDs and blacklisting specific GPU drivers
# gpu passthrough step 3
# <enable|disable>
gpu-pt3() {
    local action="$1"
    local vfio_conf="/etc/modprobe.d/vfio.conf"
    # Use a distinct blacklist file for passthrough-specific blacklisting
    local passthrough_blacklist_conf="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"
    # local blacklist_conf_generic="/etc/modprobe.d/blacklist.conf" # No longer primarily used by this function for GPU blacklisting
    local modules_file="/etc/modules"
    # local pci_ids=() # Not directly used, vendor_device_ids are used
    local all_gpu_pci_ids_details=() # Store "vendor_id:device_id pci_slot_id"
    local nvidia_gpus_for_vfio=false
    local amd_gpus_for_vfio=false
    local function_name="${FUNCNAME[0]}"


    # Technical Description:
    #   Configures or reverts GPU passthrough.
    #   'enable':
    #     - Identifies NVIDIA and AMD GPU PCI IDs.
    #     - Writes these IDs to /etc/modprobe.d/vfio.conf for vfio-pci.
    #     - Blacklists nouveau, nvidia, radeon, amdgpu drivers in a dedicated
    #       passthrough blacklist file (/etc/modprobe.d/zz-vfio-gpu-blacklist.conf)
    #       if corresponding GPUs are assigned to vfio-pci.
    #     - Ensures vfio modules are in /etc/modules.
    #   'disable':
    #     - Removes /etc/modprobe.d/vfio.conf.
    #     - Removes the passthrough-specific blacklist file.
    #     - Comments out vfio modules in /etc/modules.
    #   Prompts for reboot after changes.
    # Dependencies:
    #   - lspci
    #   - /etc/modprobe.d/, /etc/modules
    #   - Sudo privileges for file modifications and initramfs update.
    # Interactions:
    #   - Works in conjunction with gpu-pt1 and gpu-pt2 for full passthrough setup.
    #   - `gpu-nds` creates a separate blacklist for nouveau for host NVIDIA preference,
    #     which is not touched by `gpu-pt3 disable`.
    #   - `gpu-pta` and `gpu-ptd` handle dynamic (non-persistent) driver binding.

    if [ -z "$action" ]; then
        printf "Usage: %s <enable|disable>\\n" "$function_name"
        return 1
    fi
    
    # Define colors using printf, similar to lo1 module
    local YELLOW=$(printf '\\033[0;33m')
    local NC=$(printf '\\033[0m') # No Color

    printf "Fetching GPU PCI IDs and Vendor IDs...\\n"
    local lspci_output
    lspci_output=$(lspci -nn)
    while IFS= read -r line; do
        if echo "$line" | grep -qE "VGA compatible controller|3D controller"; then
            local pci_slot_id=$(echo "$line" | awk '{print $1}')
            # Correctly extract vendor:device ID, e.g., 10de:1f82 from [10de:1f82]
            local vendor_device_id=$(echo "$line" | grep -oP '\[\K[0-9a-fA-F]{4}:[0-9a-fA-F]{4}(?=\])')
            if [ -n "$vendor_device_id" ]; then
                 all_gpu_pci_ids_details+=("$vendor_device_id $pci_slot_id")
            else
                printf "${YELLOW}Warning: Could not parse vendor/device ID for line: %s${NC}\\n" "$line"
            fi
        fi
    done <<< "$lspci_output"

    if [ ${#all_gpu_pci_ids_details[@]} -eq 0 ]; then
        printf "No GPU devices found. Nothing to do.\\n"
        return 0
    fi

    if [ "$action" == "enable" ]; then
        printf "Configuring GPU passthrough (vfio-pci)...\\n"
        local vfio_options_line="options vfio-pci ids="
        local ids_to_add=""

        for detail in "${all_gpu_pci_ids_details[@]}"; do
            local vendor_device_id=$(echo "$detail" | awk '{print $1}')
            local pci_slot_id=$(echo "$detail" | awk '{print $2}') 
            local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)

            printf "Found GPU: %s with ID %s\\n" "$pci_slot_id" "$vendor_device_id"
            # For now, assume all detected GPUs are intended for passthrough.
            # A more advanced version could allow selecting specific GPUs.
            if [ -n "$ids_to_add" ]; then
                ids_to_add+=","
            fi
            ids_to_add+="$vendor_device_id"

            if [ "$vendor_id" == "10de" ]; then # NVIDIA
                nvidia_gpus_for_vfio=true
            elif [ "$vendor_id" == "1002" ]; then # AMD
                amd_gpus_for_vfio=true
            fi
        done

        if [ -z "$ids_to_add" ]; then
            printf "${YELLOW}No specific GPU IDs identified to add to %s. This might be an error or no supported GPUs found.${NC}\\n" "$vfio_conf"
        else
            vfio_options_line+="$ids_to_add"
            printf "Ensuring %s exists and adding/updating GPU IDs...\\n" "$vfio_conf"
            echo "$vfio_options_line" | tee "$vfio_conf" > /dev/null
            printf "Content of %s:\\n" "$vfio_conf"
            cat "$vfio_conf"
        fi

        printf "Ensuring GPU drivers are blacklisted for passthrough in %s...\\n" "$passthrough_blacklist_conf"
        # Create or clear the passthrough-specific blacklist file
        rm -f "$passthrough_blacklist_conf"
        touch "$passthrough_blacklist_conf" # Ensure it exists even if no drivers are blacklisted

        if $nvidia_gpus_for_vfio; then
            printf "NVIDIA GPU(s) targeted for passthrough. Blacklisting nouveau and nvidia in %s.\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist nouveau"
                echo "options nouveau modeset=0" # Important for nouveau
                echo "blacklist nvidia"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        elif lspci -nn | grep -qE "VGA compatible controller.*\[10de:" || lspci -nn | grep -qE "3D controller.*\[10de:"; then
             # This case handles if no NVIDIA GPUs were explicitly added to vfio.conf (e.g. ids_to_add was empty for NVIDIA)
             # but NVIDIA GPUs are present on the system. This is a fallback/general blacklisting.
             printf "${YELLOW}General NVIDIA GPU(s) detected on system (though not specifically added to %s by this run).${NC}\\n" "$vfio_conf"
             printf "${YELLOW}Adding precautionary blacklist for nouveau and nvidia in %s.${NC}\\n" "$passthrough_blacklist_conf"
             {
                echo "blacklist nouveau"
                echo "options nouveau modeset=0"
                echo "blacklist nvidia"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        fi

        if $amd_gpus_for_vfio; then
            printf "AMD GPU(s) targeted for passthrough. Blacklisting radeon and amdgpu in %s.\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist radeon"
                echo "blacklist amdgpu"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        elif lspci -nn | grep -qE "VGA compatible controller.*\[1002:" || lspci -nn | grep -qE "3D controller.*\[1002:"; then
            # Similar fallback for AMD
            printf "${YELLOW}General AMD GPU(s) detected on system (though not specifically added to %s by this run).${NC}\\n" "$vfio_conf"
            printf "${YELLOW}Adding precautionary blacklist for radeon and amdgpu in %s.${NC}\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist radeon"
                echo "blacklist amdgpu"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        fi
        
        if [ -s "$passthrough_blacklist_conf" ]; then # Check if file is not empty
            printf "Content of %s:\\n" "$passthrough_blacklist_conf"
            cat "$passthrough_blacklist_conf"
        else
            printf "No specific drivers were blacklisted in %s during this run (e.g., no NVIDIA/AMD GPUs for VFIO or detected generally). File is empty or only contains comments.\\n" "$passthrough_blacklist_conf"
        fi


        printf "Ensuring vfio modules are loaded at boot via %s...\\n" "$modules_file"
        for module in vfio vfio_iommu_type1 vfio_pci; do
            if ! grep -qP "^\s*${module}\s*(#.*)?$" "$modules_file"; then # Check if module exists, possibly commented
                echo "$module" | tee -a "$modules_file" > /dev/null
                printf "Added %s to %s.\\n" "$module" "$modules_file"
            else
                # Ensure not commented out
                if grep -qP "^\s*#\s*${module}" "$modules_file"; then
                    sed -i "s/^\s*#\s*${module}/${module}/" "$modules_file"
                    printf "Uncommented %s in %s.\\n" "$module" "$modules_file"
                else
                    printf "%s already correctly listed in %s.\\n" "$module" "$modules_file"
                fi
            fi
        done

    elif [ "$action" == "disable" ]; then
        printf "Reverting GPU passthrough configuration...\\n"
        printf "Removing %s...\\n" "$vfio_conf"
        rm -f "$vfio_conf"
        printf "Removing the passthrough-specific blacklist file %s...\\n" "$passthrough_blacklist_conf"
        rm -f "$passthrough_blacklist_conf"
        # Note: We do NOT touch /etc/modprobe.d/blacklist-nouveau.conf if created by gpu-nds

        printf "Commenting out vfio modules in %s (they can be auto-loaded if needed)...\\n" "$modules_file"
        for module in vfio vfio_iommu_type1 vfio_pci; do
            if grep -qP "^\s*${module}" "$modules_file"; then # If module is present and not commented
                sed -i "s/^\s*${module}/# ${module}/" "$modules_file"
                printf "Commented out %s in %s.\\n" "$module" "$modules_file"
            else
                printf "%s not found or already commented in %s.\\n" "$module" "$modules_file"
            fi
        done
    else
        printf "Invalid action: %s. Use 'enable' or 'disable'.\\n" "$action"
        return 1
    fi

    printf "Updating initramfs...\\n"
    update-initramfs -u -k all # Use -k all to update all kernels, or just -u for current
    printf "Configuration applied. A reboot is required for changes to take full effect.\\n"
}

# Detaches the GPU from the host system, making it available for VM passthrough
# gpu passthrough detach
#
gpu-ptd() {
    local function_name="${FUNCNAME[0]}"
    echo "--- Debug: Entering ${function_name} ---"
    local -a gpus_to_process
    local gpu_id_arg="$1" # Optional: specific GPU PCI ID to detach

    # Define colors using printf
    local YELLOW=$(printf '\\033[0;33m')
    local RED=$(printf '\\033[0;31m')
    local NC=$(printf '\\033[0m') # No Color

    # Check for IOMMU (Intel VT-d or AMD-Vi)
    # A more robust check for IOMMU active in kernel cmdline
    if ! grep -q 'iommu=pt\|intel_iommu=on\|amd_iommu=on' /proc/cmdline; then
        printf "${YELLOW}WARNING: IOMMU (intel_iommu=on/amd_iommu=on or iommu=pt) does not appear to be enabled in kernel command line.${NC}\\n"
        printf "${YELLOW}         VFIO passthrough may not work. Please ensure it is configured in your bootloader (e.g., GRUB).${NC}\\n"
        # Allow to continue, but it's likely to fail later if IOMMU isn't truly on.
    fi

    # Technical Description:
    #   Dynamically detaches GPU(s) from host drivers for VM passthrough.
    #   Attempts to be self-sufficient by loading necessary VFIO modules.
    #   Unloads standard GPU drivers (nouveau, nvidia, amdgpu, radeon) if they are in use by the target GPU(s).
    #   Loads 'vfio-pci' driver if not already loaded.
    #   Identifies GPU(s) via 'lspci -nn'. If an argument is provided, only that GPU is targeted.
    #   For each target GPU: unbinds from current driver, sets 'driver_override' to 'vfio-pci', binds to 'vfio-pci'.
    #   Changes are NOT persistent across reboots.
    # Dependencies:
    #   - `lspci`, `grep`, `find`, `ls`, `modprobe`, `awk`, `echo`, `tee`.
    #   - Root privileges for driver/module operations and sysfs access.
    #   - IOMMU enabled in BIOS/UEFI and kernel.

    printf "INFO: Starting GPU detachment process\\n"

    printf "INFO: Ensuring VFIO modules are loaded...\\n"
    for module in vfio vfio_iommu_type1 vfio_pci; do
        if lsmod | grep -q "^${module}"; then
            printf "INFO: Module %s is already loaded.\\n" "$module"
        else
            printf "INFO: Attempting to load module %s...\\n" "$module"
            if modprobe "$module"; then
                printf "INFO: Module %s loaded successfully.\\n" "$module"
            else
                printf "${RED}ERROR: Failed to load module %s. GPU detachment may fail.${NC}\\n" "$module"
                if [ "$module" == "vfio_pci" ]; then
                    printf "${RED}Exiting due to failure to load critical module vfio_pci.${NC}\\n"
                    return 1
                fi
            fi
        fi
    done

    printf "INFO: Identifying target GPU(s)...\\n"
    # Source the site-specific configuration to get PCI IDs
    # CONFIG_FUN should be set by the script's initial variable definitions (e.g., from SITE_CONFIG_FILE)
    if [ -n "$CONFIG_FUN" ] && [ -f "$CONFIG_FUN" ]; then
        # shellcheck source=/dev/null
        source "$CONFIG_FUN"
        printf "INFO: Successfully sourced PCI configuration from %s\\n" "$CONFIG_FUN"
    else
        printf "${YELLOW}WARNING: PCI Configuration file '%s' not found or not set. PCI IDs will not be sourced from it.${NC}\\n" "$CONFIG_FUN"
    fi

    if [ -n "$gpu_id_arg" ]; then
        # Validate the provided PCI ID format (basic check)
        if ! [[ "$gpu_id_arg" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then # e.g., 01:00.0
            printf "${RED}ERROR: Invalid PCI ID format provided: %s. Expected format like 01:00.0${NC}\\n" "$gpu_id_arg"
            return 1
        fi
        # Check if this GPU exists and is a VGA/3D controller or Audio device
        # If a specific ID is given, we process that one. User might want to target only audio or video.
        if lspci -s "$gpu_id_arg" -nn | grep -qE "VGA compatible controller|3D controller|Audio device"; then
            gpus_to_process+=("$gpu_id_arg")
            printf "INFO: Targeting specified PCI device: %s\\n" "$gpu_id_arg"
        else
            printf "${RED}ERROR: Specified PCI ID %s is not a VGA/3D/Audio controller or does not exist.${NC}\\n" "$gpu_id_arg"
            return 1
        fi
    else
        current_hostname=$(hostname -s)
        printf "INFO: No specific GPU ID provided. Attempting to use PCI IDs from configuration for hostname '%s'.\\n" "$current_hostname"
        
        pci0_var_name="${current_hostname}_node_pci0"
        pci1_var_name="${current_hostname}_node_pci1"

        # Check if the variables are set (indirect expansion)
        if [ -n "${!pci0_var_name}" ] && [ -n "${!pci1_var_name}" ]; then
            pci0_full_val=${!pci0_var_name} # Expected format "0000:xx:xx.x"
            pci1_full_val=${!pci1_var_name} # Expected format "0000:xx:xx.x"

            # Strip "0000:" prefix for the 'id' variable used in the loop
            short_pci0_val=${pci0_full_val#"0000:"}
            short_pci1_val=${pci1_full_val#"0000:"}

            # Validate format after stripping (e.g., 3b:00.0)
            if [[ "$short_pci0_val" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]] && \
               [[ "$short_pci1_val" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then
                
                lspci_output_for_check=$(lspci -nnk) # Get kernel driver info

                # Check VGA/3D controller
                if echo "$lspci_output_for_check" | grep -A3 "^$short_pci0_val" | grep -q "Kernel driver in use: vfio-pci"; then
                    printf "INFO: Configured device %s (%s) is already using vfio-pci. Skipping detachment planning.\\n" "$short_pci0_val" "$pci0_var_name"
                else
                    gpus_to_process+=("$short_pci0_val")
                    printf "INFO: Identified device %s (%s) from config for detachment.\\n" "$short_pci0_val" "$pci0_var_name"
                fi

                # Check Audio device
                if echo "$lspci_output_for_check" | grep -A3 "^$short_pci1_val" | grep -q "Kernel driver in use: vfio-pci"; then
                    printf "INFO: Configured device %s (%s) is already using vfio-pci. Skipping detachment planning.\\n" "$short_pci1_val" "$pci1_var_name"
                else
                    gpus_to_process+=("$short_pci1_val")
                    printf "INFO: Identified device %s (%s) from config for detachment.\\n" "$short_pci1_val" "$pci1_var_name"
                fi
            else
                printf "${YELLOW}WARNING: PCI IDs from config ('%s=%s', '%s=%s') are not in the expected format (e.g., 0000:xx:xx.x -> xx:xx.x). Falling back to lspci scan.${NC}\\n" "$pci0_var_name" "$pci0_full_val" "$pci1_var_name" "$pci1_full_val"
                # Fallback to original lspci scan (see below)
                unset pci0_var_name # Unset to trigger fallback
            fi
        else
            printf "${YELLOW}WARNING: PCI ID variables ('%s', '%s') not found in config for hostname '%s'. Falling back to lspci scan for all VGA/3D controllers.${NC}\\n" "$pci0_var_name" "$pci1_var_name" "$current_hostname"
            # Fallback to original lspci scan (see below)
        fi

        # Fallback logic if config variables were not sufficient
        if [ ${#gpus_to_process[@]} -eq 0 ] && ( [ -z "${!pci0_var_name}" ] || [ -z "${!pci1_var_name}" ] ); then
            printf "INFO: Falling back to lspci scan for all VGA/3D controllers not already on vfio-pci.\\n"
            local lspci_output
            lspci_output=$(lspci -nnk) # Get kernel driver info as well
            while IFS= read -r line; do
                if echo "$line" | grep -qE "VGA compatible controller|3D controller"; then
                    local pci_slot_id=$(echo "$line" | awk '{print $1}')
                    if echo "$lspci_output" | grep -A3 "^$pci_slot_id" | grep -q "Kernel driver in use: vfio-pci"; then
                        printf "INFO: GPU %s is already using vfio-pci. Skipping.\\n" "$pci_slot_id"
                    else
                        gpus_to_process+=("$pci_slot_id")
                        printf "INFO: Identified GPU %s for detachment from lspci scan.\\n" "$pci_slot_id"
                    fi
                fi
            done <<< "$lspci_output"
        fi
    fi

    # Remove duplicates if any were added from multiple sources or by mistake
    if [ ${#gpus_to_process[@]} -gt 0 ]; then
        gpus_to_process=($(printf "%s\\n" "${gpus_to_process[@]}" | sort -u | tr '\\n' ' '))
    fi

    if [ ${#gpus_to_process[@]} -eq 0 ]; then
        printf "No suitable GPU devices found or specified for detachment.${NC}\\n"
        return 0
    fi

    printf "INFO: Processing GPU IDs for detachment: %s\\n" "${gpus_to_process[*]}"
    echo "DEBUG: gpus_to_process array elements follow:"
    for item_debug in "${gpus_to_process[@]}"; do
        printf "DEBUG: Element: [%s]\\n" "$item_debug"
    done
    echo "DEBUG: End of gpus_to_process array elements."

    for id in "${gpus_to_process[@]}"; do
        printf "DEBUG: Current id before problematic printf: [%s]\\n" "$id"
        # Attempt to make printf more robust in case of strange $id values, though unlikely to be the cause if $id is clean
        printf -- "--- Processing GPU %s ---\\n" "$id"
        local full_pci_id="0000:$id"
        local driver_path="/sys/bus/pci/devices/$full_pci_id/driver"
        local current_driver=""

        if [ -L "$driver_path" ]; then
            current_driver=$(basename $(readlink -f "$driver_path"))
            printf "INFO: GPU %s is currently bound to driver: %s\\\\n" "$id" "$current_driver"
            if [ "$current_driver" == "vfio-pci" ]; then
                printf "INFO: GPU %s is already on vfio-pci. Skipping unbind/bind sequence.\\\\n" "$id"
                continue
            fi
            printf "INFO: Unbinding GPU %s from %s...\\\\n" "$id" "$current_driver"
            if echo "$full_pci_id" > "$driver_path/unbind"; then
                printf "INFO: Successfully unbound GPU %s from %s.\\\\n" "$id" "$current_driver"
            else
                printf "${YELLOW}WARNING: Failed to unbind GPU %s from %s. This might be an issue if the driver is in use.${NC}\\\\n" "$id" "$current_driver"
                # Attempt to remove the module as a more forceful way, if it's a known GPU driver
                if [[ " $current_driver " =~ " nvidia " || " $current_driver " =~ " nouveau " || " $current_driver " =~ " amdgpu " || " $current_driver " =~ " radeon " ]]; then
                    printf "INFO: Attempting to remove module %s to release GPU %s...\\\\n" "$current_driver" "$id"
                    if modprobe -r "$current_driver"; then
                        printf "INFO: Module %s removed.\\\\n" "$current_driver"
                    else
                        printf "${RED}ERROR: Failed to remove module %s. GPU %s may remain bound.${NC}\\\\n" "$current_driver" "$id"
                        # continue # Decide if you want to skip to next GPU or try to proceed
                    fi
                fi 
            fi
        else
            printf "INFO: GPU %s is not currently bound to any driver.\\\\n" "$id"
        fi

        printf "INFO: Setting driver_override to vfio-pci for GPU %s...\\\\n" "$id"
        local driver_override_set_successfully=false
        if echo "vfio-pci" > "/sys/bus/pci/devices/$full_pci_id/driver_override"; then
            printf "INFO: Successfully set driver_override for %s.\\\\n" "$id"
            driver_override_set_successfully=true

            printf "INFO: Triggering re-probe for %s to bind to vfio-pci...\\\\n" "$id"
            if echo "$full_pci_id" > "/sys/bus/pci/drivers_probe"; then
                printf "INFO: Re-probe triggered for %s.\\\\n" "$id"
                sleep 1 # Give time for the probe and bind to occur
            else
                printf "${YELLOW}WARNING: Failed to trigger re-probe for %s. Binding might not occur automatically.${NC}\\\\n" "$id"
            fi
        else
            printf "${RED}ERROR: Failed to set driver_override for %s. Binding may fail.${NC}\\\\n" "$id"
        fi

        # Check if vfio-pci claimed the device
        local new_driver_path="/sys/bus/pci/devices/$full_pci_id/driver"
        if [ "$driver_override_set_successfully" = true ] && [ -L "$new_driver_path" ] && [ "$(basename $(readlink -f "$new_driver_path") 2>/dev/null)" == "vfio-pci" ]; then
            printf "INFO: GPU %s successfully bound to vfio-pci (after driver_override and probe).\\\\n" "$id"
        else
            if [ "$driver_override_set_successfully" = true ]; then
                printf "INFO: GPU %s not bound to vfio-pci after driver_override and probe. Current driver: $(basename $(readlink -f "$new_driver_path") 2>/dev/null || echo "none").\\\\n" "$id"
            else
                # This means driver_override itself failed.
                printf "INFO: GPU %s not bound to vfio-pci (driver_override failed). Current driver: $(basename $(readlink -f "$new_driver_path") 2>/dev/null || echo "none").\\\\n" "$id"
            fi

            printf "INFO: Attempting direct bind to vfio-pci for %s as a fallback...\\\\n" "$id"
            if echo "$full_pci_id" > "/sys/bus/pci/drivers/vfio-pci/bind"; then
                sleep 0.5 # Give it a moment
                if [ -L "$new_driver_path" ] && [ "$(basename $(readlink -f "$new_driver_path") 2>/dev/null)" == "vfio-pci" ]; then
                    printf "INFO: GPU %s successfully bound to vfio-pci via direct bind command.\\\\n" "$id"
                else
                    printf "${RED}ERROR: GPU %s failed to bind to vfio-pci even after direct bind. Current driver: $(basename $(readlink -f "$new_driver_path") 2>/dev/null || echo "none"). See 'dmesg -e'.${NC}\\\\n" "$id"
                fi
            else
                printf "${RED}ERROR: Failed to issue direct bind command for GPU %s to VFIO-PCI (echo to bind failed). Device may not exist or vfio-pci cannot claim it. See 'dmesg -e'.${NC}\\\\n" "$id"
            fi
        fi
    done # End of for id in gpus_to_process

    printf "INFO: GPU detachment process completed.\\\\n"
}

# Attaches the GPU back to the host system
# gpu passthrough attach
# <pci_id> (optional, e.g., 01:00.0)
gpu-pta() {
    local function_name="${FUNCNAME[0]}"
    echo "--- Debug: Entering ${function_name} ---"
    local -a gpus_to_process
    local gpu_id_arg="$1" # Optional: specific GPU PCI ID to attach
    local current_hostname

    # Define colors using printf
    local GREEN=$(printf '\\\\033[0;32m')
    local YELLOW=$(printf '\\\\033[0;33m')
    local RED=$(printf '\\\\033[0;31m')
    local CYAN=$(printf '\\\\033[0;36m')
    local NC=$(printf '\\\\033[0m') # No Color

    current_hostname=$(hostname -s) # Get current hostname

    # NVIDIA driver preference logic (will be used later per GPU)
    # Example: x2_nvidia_driver_preference="nouveau" or "nvidia"
    # This will be checked when an NVIDIA card is processed.
    # local nvidia_driver_pref_var="${current_hostname}_nvidia_driver_preference"
    # local preferred_nvidia_driver="${!nvidia_driver_pref_var:-nvidia}" # Default to 'nvidia'
    # printf "INFO: Default NVIDIA driver preference for host '%s' is '%s' (via var %s).\\n" "$current_hostname" "$preferred_nvidia_driver" "$nvidia_driver_pref_var"


    # Technical Description:
    #   Dynamically attaches GPU(s) back to the host system from vfio-pci.
    #   Identifies GPU PCI slot IDs. If an argument is provided, only that GPU is targeted (if on vfio-pci).
    #   If no argument, uses hostname-specific config variables (e.g., x2_node_pci0) if the devices are on vfio-pci.
    #   Falls back to scanning all relevant devices (VGA/3D/Audio) on vfio-pci.
    #   For each target GPU:
    #     - Unbinds from vfio-pci (if bound).
    #     - Clears 'driver_override'.
    #     - Determines appropriate host driver (amdgpu, or nvidia/nouveau based on site config/preference).
    #     - Attempts to load the host driver module if not already loaded.
    #     - Binds the host driver.
    # Dependencies:
    #   - `lspci`, `modprobe`, `awk`, `echo`, `hostname`, `grep`, `readlink`, `basename`, `sleep`.
    #   - Root privileges for driver/module operations and sysfs access.
    # Interactions:
    #   - Uses `*_nvidia_driver_preference` from site config for NVIDIA GPUs.

    printf "INFO: Starting GPU attachment process...\\\\n"
    printf "INFO: Identifying target GPU(s)...\\\\n"

    # Source the site-specific configuration
    # CONFIG_FUN should be set by the script's initial variable definitions (e.g., from SITE_CONFIG_FILE)
    if [ -n "$CONFIG_FUN" ] && [ -f "$CONFIG_FUN" ]; then
        # shellcheck source=/dev/null
        source "$CONFIG_FUN"
        printf "INFO: Successfully sourced PCI configuration from %s\\\\n" "$CONFIG_FUN"
    else
        printf "${YELLOW}WARNING: PCI Configuration file '%s' not found or not set. PCI IDs will not be sourced from it for attachment preference.${NC}\\\\n" "$CONFIG_FUN"
    fi

    if [ -n "$gpu_id_arg" ]; then
        if ! [[ "$gpu_id_arg" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then
            printf "${RED}ERROR: Invalid PCI ID format provided: %s. Expected format like 01:00.0${NC}\\\\n" "$gpu_id_arg"
            return 1
        fi
        
        local lspci_specific_output
        lspci_specific_output=$(lspci -s "$gpu_id_arg" -nnk 2>/dev/null)
        if [ -n "$lspci_specific_output" ]; then
            if echo "$lspci_specific_output" | grep -qE "VGA compatible controller|3D controller|Audio device"; then
                if echo "$lspci_specific_output" | grep -A2 "Kernel driver in use:" | grep -q "vfio-pci"; then
                    gpus_to_process+=("$gpu_id_arg")
                    printf "INFO: Targeting specified PCI device %s (on vfio-pci) for attachment.\\\\n" "$gpu_id_arg"
                else
                    printf "${YELLOW}WARNING: Specified PCI ID %s is a relevant device type but not currently on vfio-pci. Cannot attach.${NC}\\\\n" "$gpu_id_arg"
                fi
            else
                 printf "${YELLOW}WARNING: Specified PCI ID %s is not a VGA/3D/Audio controller. Skipping.${NC}\\\\n" "$gpu_id_arg"
            fi
        else
            printf "${RED}ERROR: Specified PCI ID %s does not exist or could not be queried.${NC}\\\\n" "$gpu_id_arg"
        fi
    else
        printf "INFO: No specific GPU ID provided. Attempting to use PCI IDs from configuration for hostname '%s' if they are on vfio-pci.\\\\n" "$current_hostname"
        
        local pci0_var_name="${current_hostname}_node_pci0"
        local pci1_var_name="${current_hostname}_node_pci1"
        local lspci_full_output_check # To avoid multiple full lspci calls
        lspci_full_output_check=$(lspci -nnk)

        # Check PCI0 from config
        if [ -n "${!pci0_var_name}" ]; then
            local pci0_full_val=${!pci0_var_name}
            local short_pci0_val=${pci0_full_val#0000:}
            if [[ "$short_pci0_val" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then
                if echo "$lspci_full_output_check" | grep -A3 "^$short_pci0_val" | grep -qE "VGA compatible controller|3D controller|Audio device"; then
                    if echo "$lspci_full_output_check" | grep -A3 "^$short_pci0_val" | grep -q "Kernel driver in use: vfio-pci"; then
                        gpus_to_process+=("$short_pci0_val")
                        printf "INFO: Identified device %s (%s from config) on vfio-pci for attachment.\\\\n" "$short_pci0_val" "$pci0_var_name"
                    else
                        printf "INFO: Configured device %s (%s) is a relevant type but not on vfio-pci. Skipping for attachment.\\\\n" "$short_pci0_val" "$pci0_var_name"
                    fi
                else
                    printf "INFO: Configured device %s (%s) is not a VGA/3D/Audio controller. Skipping for attachment.\\\\n" "$short_pci0_val" "$pci0_var_name"
                fi
            else
                printf "${YELLOW}WARNING: Configured PCI ID %s value ('%s') is not in the expected format (xx:xx.x after stripping 0000:).${NC}\\\\n" "$pci0_var_name" "$pci0_full_val"
            fi
        else
            printf "INFO: PCI ID variable '%s' not found in config for hostname '%s'.\\\\n" "$pci0_var_name" "$current_hostname"
        fi

        # Check PCI1 from config
        if [ -n "${!pci1_var_name}" ]; then
            local pci1_full_val=${!pci1_var_name}
            local short_pci1_val=${pci1_full_val#0000:}
            if [[ "$short_pci1_val" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then
                 if echo "$lspci_full_output_check" | grep -A3 "^$short_pci1_val" | grep -qE "VGA compatible controller|3D controller|Audio device"; then
                    if echo "$lspci_full_output_check" | grep -A3 "^$short_pci1_val" | grep -q "Kernel driver in use: vfio-pci"; then
                        gpus_to_process+=("$short_pci1_val")
                        printf "INFO: Identified device %s (%s from config) on vfio-pci for attachment.\\\\n" "$short_pci1_val" "$pci1_var_name"
                    else
                        printf "INFO: Configured device %s (%s) is a relevant type but not on vfio-pci. Skipping for attachment.\\\\n" "$short_pci1_val" "$pci1_var_name"
                    fi
                else
                    printf "INFO: Configured device %s (%s) is not a VGA/3D/Audio controller. Skipping for attachment.\\\\n" "$short_pci1_val" "$pci1_var_name"
                fi
            else
                printf "${YELLOW}WARNING: Configured PCI ID %s value ('%s') is not in the expected format (xx:xx.x after stripping 0000:).${NC}\\\\n" "$pci1_var_name" "$pci1_full_val"
            fi
        else
            printf "INFO: PCI ID variable '%s' not found in config for hostname '%s'.\\\\n" "$pci1_var_name" "$current_hostname"
        fi
        
        if [ ${#gpus_to_process[@]} -eq 0 ]; then
            printf "INFO: No GPUs specified via argument or found via host-specific config on vfio-pci. Falling back to scanning all relevant devices on vfio-pci.\\\\n"
            
            if [ -z "$lspci_full_output_check" ]; then # Should have been set, but as a safeguard
                lspci_full_output_check=$(lspci -nnk)
            fi

            local current_pci_id_fb=""
            local current_block_content_fb=""
            local is_relevant_device_fb=false 

            while IFS= read -r line_fb; do
                if [[ "$line_fb" =~ ^([0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]) ]]; then 
                    if [ -n "$current_pci_id_fb" ] && $is_relevant_device_fb; then
                        if echo "$current_block_content_fb" | grep -A2 "Kernel driver in use:" | grep -q "vfio-pci"; then
                            gpus_to_process+=("$current_pci_id_fb")
                            printf "INFO: Identified device %s (from lspci scan) on vfio-pci for attachment.\\\\n" "$current_pci_id_fb"
                        fi
                    fi
                    current_pci_id_fb="${BASH_REMATCH[1]}"
                    current_block_content_fb="$line_fb" 
                    if echo "$line_fb" | grep -qE "VGA compatible controller|3D controller|Audio device"; then
                        is_relevant_device_fb=true
                    else
                        is_relevant_device_fb=false
                    fi
                elif [ -n "$current_pci_id_fb" ]; then 
                    current_block_content_fb+=$'\\\\n'"$line_fb"
                fi
            done <<< "$lspci_full_output_check"

            if [ -n "$current_pci_id_fb" ] && $is_relevant_device_fb; then
                if echo "$current_block_content_fb" | grep -A2 "Kernel driver in use:" | grep -q "vfio-pci"; then
                    gpus_to_process+=("$current_pci_id_fb")
                    printf "INFO: Identified device %s (from lspci scan, last block) on vfio-pci for attachment.\\\\n" "$current_pci_id_fb"
                fi
            fi
        fi
    fi

    if [ ${#gpus_to_process[@]} -gt 0 ]; then
        gpus_to_process=($(printf "%s\\\\n" "${gpus_to_process[@]}" | sort -u | tr '\\\\n' ' '))
    fi

    if [ ${#gpus_to_process[@]} -eq 0 ]; then
        printf "${YELLOW}No suitable GPU devices found on vfio-pci (via argument, config, or scan) to attach.${NC}\\\\n"
        return 0 
    fi

    printf "INFO: Final list of GPU IDs to process for attachment: %s\\\\n" "${gpus_to_process[*]}"

    for pci_slot_id in "${gpus_to_process[@]}"; do
        local full_pci_id="0000:$pci_slot_id"
        local sysfs_device_path="/sys/bus/pci/devices/$full_pci_id"
        local current_driver_path="$sysfs_device_path/driver"
        local host_driver="" 

        printf "${CYAN}--- Processing GPU %s for attachment to host ---${NC}\\\\n" "$pci_slot_id"

        if [ -L "$current_driver_path" ] && [ "$(basename "$(readlink -f "$current_driver_path")")" == "vfio-pci" ]; then
            printf "INFO: GPU %s is on vfio-pci. Unbinding...\\\\n" "$pci_slot_id"
            if echo "$full_pci_id" > "$current_driver_path/unbind"; then
                printf "INFO: Successfully unbound %s from vfio-pci.\\\\n" "$pci_slot_id"
            else
                printf "${RED}ERROR: Failed to unbind %s from vfio-pci. Skipping attachment for this device.${NC}\\\\n" "$pci_slot_id"
                continue
            fi
        elif [ -L "$current_driver_path" ]; then
             local current_bound_driver
             current_bound_driver=$(basename "$(readlink -f "$current_driver_path")")
             printf "INFO: GPU %s is already bound to %s, not vfio-pci. Assuming it's already attached or no action needed by this script.\\\\n" "$pci_slot_id" "$current_bound_driver"
             continue 
        else
            printf "INFO: GPU %s is not currently bound to any driver. Proceeding to attach.\\\\n" "$pci_slot_id"
        fi

        if [ -f "$sysfs_device_path/driver_override" ]; then
            printf "INFO: Clearing driver_override for %s...\\\\n" "$pci_slot_id"
            if ! echo > "$sysfs_device_path/driver_override"; then
                 printf "${YELLOW}WARNING: Failed to clear driver_override for %s. Attachment might be affected.${NC}\\\\n" "$pci_slot_id"
            fi
        fi

        local vendor_id device_id
        # Extract vendor and device IDs from lspci output
        # Example output: "0a:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU117 [GeForce GTX 1650] [10de:1f82] (rev a1)"
        local lspci_device_info
        lspci_device_info=$(lspci -s "$pci_slot_id" -nn 2>/dev/null)
        if [ -n "$lspci_device_info" ]; then
            # Extract vendor:device ID from the [xxxx:yyyy] pattern at the end
            vendor_id=$(echo "$lspci_device_info" | grep -o '\[10[0-9a-f][0-9a-f]:[0-9a-f][0-9a-f][0-9a-f][0-9a-f]\]' | tail -1 | sed 's/\[//' | sed 's/\]//' | cut -d':' -f1)
            device_id=$(echo "$lspci_device_info" | grep -o '\[10[0-9a-f][0-9a-f]:[0-9a-f][0-9a-f][0-9a-f][0-9a-f]\]' | tail -1 | sed 's/\[//' | sed 's/\]//' | cut -d':' -f2)
        fi
        
        # If the specific pattern above didn't work, try a more general approach
        if [ -z "$vendor_id" ]; then
            vendor_id=$(echo "$lspci_device_info" | grep -o '\[[0-9a-f][0-9a-f][0-9a-f][0-9a-f]:[0-9a-f][0-9a-f][0-9a-f][0-9a-f]\]' | tail -1 | sed 's/\[//' | sed 's/\]//' | cut -d':' -f1)
            device_id=$(echo "$lspci_device_info" | grep -o '\[[0-9a-f][0-9a-f][0-9a-f][0-9a-f]:[0-9a-f][0-9a-f][0-9a-f][0-9a-f]\]' | tail -1 | sed 's/\[//' | sed 's/\]//' | cut -d':' -f2)
        fi
        
        printf "DEBUG: lspci_device_info: %s\\\\n" "$lspci_device_info"
        printf "DEBUG: Extracted vendor_id: '%s', device_id: '%s'\\\\n" "$vendor_id" "$device_id"

        if [ "$vendor_id" == "10de" ]; then 
            local nvidia_driver_pref_var="${current_hostname}_nvidia_driver_preference"
            # Default to "nvidia" if var is not set or empty, otherwise use its value.
            local preferred_nvidia_driver
            if [ -n "${!nvidia_driver_pref_var}" ]; then
                preferred_nvidia_driver="${!nvidia_driver_pref_var}"
            else
                preferred_nvidia_driver="nvidia"
            fi
            
            if [ "$preferred_nvidia_driver" == "nouveau" ]; then
                if [ -f "/etc/modprobe.d/blacklist-nouveau.conf" ] && grep -q "blacklist nouveau" "/etc/modprobe.d/blacklist-nouveau.conf"; then
                    printf "${YELLOW}WARNING: Preferred driver for %s is 'nouveau', but it appears to be blacklisted (e.g., by gpu-nds). Attachment might fail or use an alternative.${NC}\\\\n" "$pci_slot_id"
                fi
            fi
            host_driver="$preferred_nvidia_driver"
            printf "INFO: Target host driver for NVIDIA GPU %s: %s (based on var %s, site default: nvidia)\\\\n" "$pci_slot_id" "$host_driver" "$nvidia_driver_pref_var"
        elif [ "$vendor_id" == "1002" ]; then 
            host_driver="amdgpu" 
            printf "INFO: Target host driver for AMD GPU %s: %s\\\\n" "$pci_slot_id" "$host_driver"
        else
            printf "${YELLOW}WARNING: Unknown GPU vendor ID %s for %s. Cannot determine host driver.${NC}\\\\n" "$vendor_id" "$pci_slot_id"
            continue
        fi

        if ! lsmod | grep -q "^${host_driver//-/_}"; then
            printf "INFO: Host driver module %s not loaded. Attempting to load...\\\\n" "$host_driver"
            if modprobe "$host_driver"; then
                printf "INFO: Module %s loaded successfully.\\\\n" "$host_driver"
            else
                printf "${RED}ERROR: Failed to load module %s. Cannot attach GPU %s.${NC}\\\\n" "$host_driver" "$pci_slot_id"
                continue
            fi
        else
            printf "INFO: Host driver module %s is already loaded.\\\\n" "$host_driver"
        fi

        printf "INFO: Attempting to bind GPU %s to driver %s...\\\\n" "$pci_slot_id" "$host_driver"
        local pci_driver_bind_path="/sys/bus/pci/drivers/$host_driver/bind"
        if [ ! -d "/sys/bus/pci/drivers/$host_driver" ]; then
             printf "${RED}ERROR: PCI driver directory for %s (%s) does not exist. Cannot bind %s.${NC}\\\\n" "$host_driver" "$pci_driver_bind_path" "$pci_slot_id"
             continue
        fi

        if echo "$full_pci_id" > "$pci_driver_bind_path"; then
            printf "INFO: Successfully initiated bind for GPU %s to %s.\\\\n" "$pci_slot_id" "$host_driver"
            sleep 1 
            if [ -L "$current_driver_path" ] && [ "$(basename "$(readlink -f "$current_driver_path")")" == "$host_driver" ]; then
                printf "${GREEN}SUCCESS: GPU %s is now bound to %s.${NC}\\\\n" "$pci_slot_id" "$host_driver"
            else
                printf "${YELLOW}WARNING: GPU %s may not have bound to %s correctly. Current driver: $(basename "$(readlink -f "$current_driver_path")" 2>/dev/null || echo "none").${NC}\\\\n" "$pci_slot_id" "$host_driver"
            fi
        else
            printf "${RED}ERROR: Failed to bind GPU %s to %s (echo to bind file failed).${NC}\\\\n" "$pci_slot_id" "$host_driver"
        fi
    done
    printf "INFO: GPU attachment process completed.\\\\n"
}

# Checks the current status of the GPU
# gpu passthrough status
#
gpu-pts() {
    local function_name="${FUNCNAME[0]}"
    # Flags for overall summary, populated by lspci parsing
    local is_any_gpu_on_vfio=false
    local host_drivers_for_gpu=""
    local vga_devices_processed=false # True if any VGA/3D device was found by lspci and processed

    # Define colors using printf
    local GREEN=$(printf '\\033[0;32m')
    local CYAN=$(printf '\\033[0;36m')
    local MAGENTA=$(printf '\\033[0;35m')
    local YELLOW=$(printf '\\033[0;33m')
    local INDIGO_BLUE=$(printf '\\033[38;2;75;0;130m')
    local RED=$(printf '\\033[0;31m')
    local NC=$(printf '\\033[0m') # No Color

    # Workflow Configuration Checklist & Runtime Status Symbols
    local CHECK_MARK="${GREEN}âœ“${NC}"
    local CROSS_MARK="${RED}âœ—${NC}"
    local QUESTION_MARK="${YELLOW}?${NC}"

    # Define paths needed for the checklist
    local modules_file_path="/etc/modules"
    local blacklist_nouveau_conf_path="/etc/modprobe.d/blacklist-nouveau.conf"
    local vfio_conf_path="/etc/modprobe.d/vfio.conf"
    local passthrough_blacklist_conf_path="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"

    # Initialize variables for lspci -nnk processing results
    local all_device_blocks_output=""
    local gpu_details_for_checklist=""

    # --- Hoisted lspci -nnk processing ---
    # This section populates all_device_blocks_output, gpu_details_for_checklist,
    # is_any_gpu_on_vfio, host_drivers_for_gpu, and vga_devices_processed.
    local temp_lspci_gpu_blocks="/tmp/gpu_pts_blocks.$$"
    lspci -nnk | grep -A4 -iE "VGA compatible controller|3D controller" > "$temp_lspci_gpu_blocks"
    
    if [ -s "$temp_lspci_gpu_blocks" ]; then
        local current_gpu_block_content=""
        # vga_devices_processed is already false, will be set true if blocks are processed or temp file had content

        while IFS= read -r line_from_grep; do
            if echo "$line_from_grep" | grep -qE "VGA compatible controller|3D controller"; then
                # Process previous block if it exists and was a GPU block
                if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
                    all_device_blocks_output+="${YELLOW}Device Block:${NC}\\n$current_gpu_block_content\\n\\n"
                    vga_devices_processed=true # A device block was processed
                    local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')
                    
                    local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
                    local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
                    local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
                    local device_name_short=$(echo "$device_name_raw" | sed 's/ \\[.*\\]//g' | awk '{print $1, $2}')
                    local device_name_for_checklist="$device_vendor_name $device_name_short"
                    device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
                    if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

                    if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                        is_any_gpu_on_vfio=true
                        gpu_details_for_checklist+="     ${CHECK_MARK} GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\\n"
                    elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                        local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                        if [ -n "$driver_name" ]; then
                            if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                            gpu_details_for_checklist+="     ${INDIGO_BLUE}*${NC} GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\\n"
                        else
                            gpu_details_for_checklist+="     ${RED}âœ—${NC} GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\\n"
                        fi
                    else
                        gpu_details_for_checklist+="     ${RED}âœ—${NC} GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\\n"
                    fi
                fi
                current_gpu_block_content="$line_from_grep" # Start new block
            elif [[ "$line_from_grep" == "--" ]]; then # Separator line from grep -A
                if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
                    all_device_blocks_output+="${YELLOW}Device Block:${NC}\\n$current_gpu_block_content\\n\\n"
                    vga_devices_processed=true
                    local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')

                    local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
                    local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
                    local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
                    local device_name_short=$(echo "$device_name_raw" | sed 's/ \\[.*\\]//g' | awk '{print $1, $2}')
                    local device_name_for_checklist="$device_vendor_name $device_name_short"
                    device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
                    if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

                    if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                        is_any_gpu_on_vfio=true
                        gpu_details_for_checklist+="     ${CHECK_MARK} GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\\n"
                    elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                        local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                        if [ -n "$driver_name" ]; then
                           if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                           gpu_details_for_checklist+="     ${INDIGO_BLUE}*${NC} GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\\n"
                        else
                           gpu_details_for_checklist+="     ${RED}âœ—${NC} GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\\n"
                        fi
                    else
                        gpu_details_for_checklist+="     ${RED}âœ—${NC} GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\\n"
                    fi
                fi
                current_gpu_block_content="" # Reset for next block
            elif [ -n "$current_gpu_block_content" ]; then # Accumulate lines for the current block
                current_gpu_block_content="$current_gpu_block_content"$'\\n'"$line_from_grep"
            fi
        done < "$temp_lspci_gpu_blocks"
        
        # Process the last block after loop ends
        if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
            all_device_blocks_output+="${YELLOW}Device Block:${NC}\\n$current_gpu_block_content\\n\\n"
            vga_devices_processed=true
            local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')

            local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
            local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
            local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
            local device_name_short=$(echo "$device_name_raw" | sed 's/ \\[.*\\]//g' | awk '{print $1, $2}')
            local device_name_for_checklist="$device_vendor_name $device_name_short"
            device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
            if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

            if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                is_any_gpu_on_vfio=true
                gpu_details_for_checklist+="     ${CHECK_MARK} GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\\n"
            elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                if [ -n "$driver_name" ]; then
                    if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                    gpu_details_for_checklist+="     ${INDIGO_BLUE}*${NC} GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\\n"
                else
                    gpu_details_for_checklist+="     ${RED}âœ—${NC} GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\\n"
                fi
            else
                 gpu_details_for_checklist+="     ${RED}âœ—${NC} GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\\n"
            fi
        fi
        
        # If lspci | grep found devices, but loop didn't process them (e.g. parsing issue),
        # still mark vga_devices_processed as true and try basic flag population.
        if ! $vga_devices_processed && [ -s "$temp_lspci_gpu_blocks" ]; then
            vga_devices_processed=true 
            local full_lspci_check_output_from_temp=$(cat "$temp_lspci_gpu_blocks")
            if echo "$full_lspci_check_output_from_temp" | grep -q "Kernel driver in use: vfio-pci"; then
                is_any_gpu_on_vfio=true
            fi
            if ! $is_any_gpu_on_vfio; then
                local found_host_driver_names=$(echo "$full_lspci_check_output_from_temp" | grep "Kernel driver in use:" | awk -F': ' '{print $2}' | sort -u | tr '\\n' ',' | sed 's/,$//')
                if [ -n "$found_host_driver_names" ] && [ "$found_host_driver_names" != "vfio-pci" ]; then
                     host_drivers_for_gpu="$found_host_driver_names"
                fi
            fi
        fi
    else
        # temp_lspci_gpu_blocks is empty, so lspci | grep found no VGA/3D controllers.
        # vga_devices_processed remains false.
        : # No action needed, flags remain at their defaults
    fi
    rm -f "$temp_lspci_gpu_blocks"
    # --- End of Hoisted lspci -nnk processing ---

    # --- IOMMU Groups (Details) ---
    echo -e "\\n${CYAN}--- IOMMU Groups (Details) ---${NC}"
    if ! command -v find &>/dev/null || ! command -v ls &>/dev/null || ! command -v lspci &>/dev/null; then
        echo -e "${RED}Required commands (find, ls, lspci) not available to list IOMMU groups.${NC}"
    else
        local gpu_pci_bus_ids=()
        while IFS= read -r line; do
            gpu_pci_bus_ids+=("$(echo "$line" | awk '{print $1}')")
        done < <(lspci -nn | grep -iE "VGA compatible controller|3D controller")

        if [ ${#gpu_pci_bus_ids[@]} -eq 0 ]; then
            echo -e "${YELLOW}No VGA/3D controllers found to filter IOMMU groups.${NC}"
        else
            local overall_iommu_groups_found=false
            local gpu_related_group_displayed=false

            for iommu_group_dir in $(find /sys/kernel/iommu_groups/ -mindepth 1 -maxdepth 1 -type d 2>/dev/null | sort -V); do
                overall_iommu_groups_found=true
                local group_id=$(basename "$iommu_group_dir")
                local devices_in_group_output=""
                local this_group_contains_gpu=false

                for device_symlink_name in $(ls -1 "$iommu_group_dir"/devices/ 2>/dev/null | sort -V); do
                    local device_pci_bus_id_full="$device_symlink_name"
                    local device_pci_bus_id_short="${device_symlink_name#0000:}"
                    local device_info
                    device_info=$(lspci -nns "$device_pci_bus_id_full" 2>/dev/null)
                    
                    if [ -n "$device_info" ]; then
                        devices_in_group_output+="  ${device_info}\\n"
                        for gpu_id in "${gpu_pci_bus_ids[@]}"; do
                            if [[ "$device_pci_bus_id_short" == "$gpu_id" ]]; then
                                this_group_contains_gpu=true
                                break 
                            fi
                        done
                    else
                        devices_in_group_output+="  ${RED}Error reading device info for $device_pci_bus_id_full${NC}\\n"
                    fi
                done

                if $this_group_contains_gpu; then
                    echo -e "\\n${YELLOW}IOMMU Group ${group_id}:${NC}"
                    echo -e "${devices_in_group_output%\\n}" 
                    gpu_related_group_displayed=true
                fi
            done

            if ! $overall_iommu_groups_found; then
                echo -e "${YELLOW}No IOMMU groups found by 'find' command.${NC}"
            elif ! $gpu_related_group_displayed; then
                echo -e "${YELLOW}No IOMMU groups found containing the identified GPU(s):${NC} ${gpu_pci_bus_ids[*]}"
            fi
        fi
    fi

    # --- GPU Device Details (lspci -nnk) --- (NEW SECTION)
    echo -e "\\n${CYAN}--- GPU Device Details (lspci -nnk) ---${NC}"
    if [ -n "$all_device_blocks_output" ]; then
        echo -e "${all_device_blocks_output%\\n\\n}" # Print accumulated blocks, remove last two newlines
    elif $vga_devices_processed; then 
         echo -e "${YELLOW}No detailed GPU device blocks were formatted (e.g. lspci output was unusual or parsing issue).${NC}"
    else 
         echo -e "${YELLOW}No VGA compatible or 3D controllers found by lspci to display details.${NC}"
    fi
    # --- End of GPU Device Details ---

    # --- Loaded Kernel Modules (GPU related) ---
    echo -e "\\n${CYAN}--- Loaded Kernel Modules (GPU related) ---${NC}"
    if lsmod | command grep -E "vfio|nvidia|nouveau|amdgpu|radeon" --color=never; then
        : 
    else
        echo -e "${YELLOW}No common GPU-related modules (vfio, nvidia, nouveau, amdgpu, radeon) loaded.${NC}"
    fi

    echo -e "\\n${CYAN}--- GPU Passthrough Workflow & Runtime Status ---${NC}"
    echo -e "${CYAN}==================================================${NC}"

    # 1. Initial System Preparation (Configuration)
    echo -e "${MAGENTA}1. Initial System Preparation (Configuration):${NC}"
    
    local iommu_enabled_cmdline_status="$CROSS_MARK"
    # Use full path to grep and -E for extended regex for robust pattern matching
    if /usr/bin/grep -E -q 'iommu=pt|intel_iommu=on|amd_iommu=on' /proc/cmdline; then
        iommu_enabled_cmdline_status="$CHECK_MARK"
    fi
    echo -e "   ${iommu_enabled_cmdline_status} IOMMU Enabled in Kernel Command Line (gpu-pt1)"

    local vfio_modules_in_etc_modules_status="$CROSS_MARK"
    local vfio_ok=0; local vfio_iommu_type1_ok=0; local vfio_pci_ok=0
    if [ -f "$modules_file_path" ]; then
        if grep -qP "^\\s*vfio(\\s|$|#)" "$modules_file_path"; then vfio_ok=1; fi
        if grep -qP "^\\s*vfio_iommu_type1(\\s|$|#)" "$modules_file_path"; then vfio_iommu_type1_ok=1; fi
        if grep -qP "^\\s*vfio_pci(\\s|$|#)" "$modules_file_path"; then vfio_pci_ok=1; fi
    fi
    if [ "$vfio_ok" -eq 1 ] && [ "$vfio_iommu_type1_ok" -eq 1 ] && [ "$vfio_pci_ok" -eq 1 ]; then
        vfio_modules_in_etc_modules_status="$CHECK_MARK"
    fi
    echo -e "   ${vfio_modules_in_etc_modules_status} Core VFIO modules in $modules_file_path (gpu-pt2)"

    # 2. NVIDIA Host Driver Setup (Optional Configuration)
    echo -e "${MAGENTA}2. NVIDIA Host Driver Setup (Optional Configuration):${NC}"
    local nouveau_blacklisted_status="$QUESTION_MARK (Not found)"
    if [ -f "$blacklist_nouveau_conf_path" ]; then
        if grep -q "blacklist nouveau" "$blacklist_nouveau_conf_path"; then
            nouveau_blacklisted_status="$CHECK_MARK"
        else
            nouveau_blacklisted_status="$CROSS_MARK (Found, but 'blacklist nouveau' missing)"
        fi
    fi
    echo -e "   ${nouveau_blacklisted_status} Nouveau blacklisted for NVIDIA driver ($blacklist_nouveau_conf_path) (gpu-nds)"

    # 3. Persistent GPU Passthrough Configuration (gpu-pt3 enable status)
    echo -e "${MAGENTA}3. Persistent GPU Passthrough Configuration (gpu-pt3 enable):${NC}"
    local persistent_vfio_ids_status="$CROSS_MARK"
    if [ -f "$vfio_conf_path" ] && grep -qE "^options vfio-pci ids=" "$vfio_conf_path"; then
        persistent_vfio_ids_status="$CHECK_MARK"
    fi
    echo -e "   ${persistent_vfio_ids_status} GPUs assigned to vfio-pci at boot ($vfio_conf_path)"

    local persistent_blacklist_status="$CROSS_MARK"
    if [ -f "$passthrough_blacklist_conf_path" ] && grep -qE "blacklist (nvidia|nouveau|amdgpu|radeon)" "$passthrough_blacklist_conf_path"; then
        persistent_blacklist_status="$CHECK_MARK"
    fi
    echo -e "   ${persistent_blacklist_status} Host drivers blacklisted for selected GPUs ($passthrough_blacklist_conf_path)"

    # 4. Runtime Kernel Module & Driver Status
    echo -e "${MAGENTA}4. Runtime Kernel Module & Driver Status:${NC}"
    local vfio_pci_loaded_status="$CROSS_MARK"
    if lsmod | grep -q "vfio_pci"; then
        vfio_pci_loaded_status="$CHECK_MARK"
    fi
    echo -e "   ${vfio_pci_loaded_status} VFIO-PCI kernel module currently loaded"

    local other_gpu_modules_loaded="-"
    if lsmod | command grep -qE "nvidia|nouveau|amdgpu|radeon"; then # Use command grep
        other_gpu_modules_loaded="${GREEN}Host drivers (nvidia/nouveau/amdgpu/radeon) loaded${NC}"
    else
        other_gpu_modules_loaded="${YELLOW}No common host GPU drivers loaded${NC}"
    fi
    if lsmod | command grep -qE "vfio_iommu_type1|vfio"; then # Use command grep
        if [[ "$other_gpu_modules_loaded" == "-" || "$other_gpu_modules_loaded" == *"${YELLOW}No common host GPU drivers loaded${NC}"* ]]; then
            other_gpu_modules_loaded="${GREEN}VFIO modules (vfio/vfio_iommu_type1) loaded${NC}"
        else
            other_gpu_modules_loaded+=", ${GREEN}VFIO modules (vfio/vfio_iommu_type1) loaded${NC}"
        fi 
    fi
    echo -e "   - Other relevant modules: $other_gpu_modules_loaded"
    
    # 4.3 Kernel Drivers for GPU(s) Summary (MODIFIED SECTION)
    echo -e "   ${CYAN}--- Kernel Drivers for GPU(s) Summary ---${NC}" # Renamed title
    if [ -n "$gpu_details_for_checklist" ]; then
        echo -e "${gpu_details_for_checklist%\\n}" # Print accumulated checklist lines
    elif $vga_devices_processed; then 
         echo -e "     ${YELLOW}?${NC} No summary for GPU driver bindings could be generated."
    else 
         echo -e "     ${YELLOW}?${NC} No VGA/3D controllers found to summarize driver bindings."
    fi

    # Overall GPU State Determination
    echo -e "\n--- Overall GPU State Summary ---"
    if $is_any_gpu_on_vfio; then
        echo -e "[SUMMARY] GPU State: DETACHED (for VM use)"
        echo -e "  - At least one GPU device (VGA/3D controller) is bound to vfio-pci."
        if [ -n "$host_drivers_for_gpu" ]; then
             echo -e "  - Note: Other GPU devices might be ATTACHED to host (driver(s): $host_drivers_for_gpu). State could be MIXED."
        fi
    elif [ -n "$host_drivers_for_gpu" ]; then
        echo -e "[SUMMARY] GPU State: ATTACHED to host (driver(s): $host_drivers_for_gpu)"
        echo -e "  - GPU device(s) (VGA/3D controller) are using host driver(s): $host_drivers_for_gpu."
        echo -e "  - No VGA/3D controller found using vfio-pci."
    else
        if $vga_devices_processed; then # Check if any GPU was processed
            echo -e "[SUMMARY] GPU State: UNCLEAR / INDETERMINATE"
            echo -e "  - A VGA/3D controller was detected, but it's not using 'vfio-pci' nor any other recognized host driver."
            echo -e "  - It might be unbound. Check 'GPU Device Details' and IOMMU details."
        else # No VGA/3D controllers were found by lspci at all
            echo -e "[SUMMARY] GPU State: N/A (No VGA/3D GPU controllers detected)"
        fi
        echo -e "  - Review IOMMU groups and loaded kernel modules above."
    fi

    echo -e "\n--- GPU status check completed. (${function_name}) ---"
    echo -e "===================================================="
}
