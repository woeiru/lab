#!/bin/bash

# Enhanced auxiliary logging fallback mechanism
if [[ -z "$RC_SOURCED" ]]; then
    # Calculate relative path from current script location
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    AUX_PATH="$SCRIPT_DIR/../gen/aux"
    source "$AUX_PATH"
fi

# ============================================================================
# GPU Driver and Passthrough Function Interactions - Technical Overview
#
# This section outlines crucial interactions between different GPU management
# functions, particularly concerning driver blacklisting, preferences, and
# dynamic vs. persistent configurations.
#
# Persistent Configuration (Primarily via `gpu_pt1`, `gpu_pt2`, `gpu_pt3`):
#   - These functions establish the boot-time default state for GPU passthrough.
#   - `gpu_pt3 enable`: Configures the system to assign specified GPUs to `vfio-pci`
#     at boot, creating persistent blacklists for host drivers for those GPUs.
#     This is the most robust way to dedicate GPUs for passthrough.
#   - `gpu_pt3 disable`: Reverts the persistent passthrough configuration, allowing
#     host drivers to claim GPUs at boot (defaulting to host usage).
#
# Dynamic Switching (`gpu_ptd`, `gpu_pta`):
#   - These functions allow runtime switching of GPU control between the host and
#     `vfio-pci` (for VMs) *without requiring a reboot*.
#   - They are designed to be as self-sufficient as possible, meaning they can
#     often work even if the persistent state set by `gpu_pt3` is for host usage
#     (i.e., after `gpu_pt3 disable` or if `gpu_pt3 enable` was never run).
#   - `gpu_ptd` (Detach): Attempts to dynamically prepare the system for passthrough
#     by loading necessary VFIO modules, unbinding host drivers, and binding `vfio-pci`.
#   - `gpu_pta` (Attach): Attempts to dynamically return GPU control to the host by
#     unbinding `vfio-pci`, loading necessary host drivers, and binding them.
#
# Blacklist Files:
#   - `/etc/modprobe.d/blacklist-nouveau.conf`:
#       - Created/Managed by: `gpu_nds` (NVIDIA Driver Setup).
#       - Purpose: Persistently blacklists `nouveau` if NVIDIA proprietary drivers
#         are installed for host use.
#
#   - `/etc/modprobe.d/zz-vfio-gpu-blacklist.conf`:
#       - Created/Managed by: `gpu_pt3 enable`.
#       - Removed by: `gpu_pt3 disable`.
#       - Purpose: Persistently blacklists host drivers (`nouveau`, `nvidia`,
#         `amdgpu`, `radeon`) for GPUs intended for passthrough at boot.
#         `gpu_ptd` does *not* create this file but attempts to achieve a similar
#         effect for the current session by unloading host drivers.
#
# Key Interactions & Considerations:
#
# 1. `gpu_nds` (NVIDIA Driver Setup):
#    - Installs NVIDIA proprietary drivers and can blacklist `nouveau` for host use.
#    This is independent of passthrough but affects which driver (`nvidia` or `nouveau`)
#    `gpu_pta` might try to load for an NVIDIA GPU if it's the host preference.
#
# 2. `gpu_pt1`, `gpu_pt2` (Initial Passthrough Setup):
#    - These set up essential prerequisites like IOMMU kernel parameters and ensure
#      VFIO kernel modules are listed in `/etc/modules`.
#    - While `gpu_ptd` attempts to load VFIO modules dynamically, having these base
#      prerequisites met (especially IOMMU in kernel cmdline) is highly recommended
#      for `gpu_ptd` to succeed reliably.
#
# 3. `gpu_pt3 enable` (Persistent Passthrough):
#    - Provides the most stable environment for passthrough by ensuring GPUs are
#      bound to `vfio-pci` from boot and host drivers are blacklisted.
#    - If you primarily use GPUs for passthrough, this is the recommended default state.
#
# 4. `gpu_pt3 disable` (Persistent Host GPU Usage):
#    - Sets the system to use GPUs with host drivers by default after boot.
#    - This is the state from which `gpu_ptd` can be used to dynamically switch
#      a GPU to `vfio-pci` for a VM session.
#
# 5. `gpu_ptd` (Dynamic Detach for VM):
#    - Will attempt to load `vfio`, `vfio_iommu_type1`, and `vfio_pci` if not present.
#    - Will attempt to unbind host drivers (e.g., `nvidia`, `nouveau`, `amdgpu`).
#    - Uses `driver_override` to bind `vfio-pci` to the target GPU(s).
#    - Its changes are not persistent across reboots.
#    - If IOMMU is not enabled in the kernel, `gpu_ptd` will warn but cannot fix it.
#
# 6. `gpu_pta` (Dynamic Attach to Host):
#    - Unbinds `vfio-pci`.
#    - Clears `driver_override`.
#    - Attempts to load the preferred/appropriate host driver (e.g., `nvidia`, `nouveau`, `amdgpu`)
#      if not already loaded, and binds it to the GPU.
#    - Considers hostname-specific `*_NVIDIA_DRIVER_PREFERENCE` from site config for NVIDIA GPUs.
#      Warns if `nouveau` is preferred but blacklisted by `gpu_nds`.
#
# General Workflow Examples:
#
#   Initial One-Time Setup (Common for both scenarios below):
#     1. Run `gpu_pt1`: Configures GRUB/EFI for IOMMU, installs related packages.
#        Reboot as prompted.
#     2. Run `gpu_pt2`: Adds necessary kernel modules (vfio, vfio_iommu_type1, vfio_pci)
#        to /etc/modules for them to be loaded at boot, updates initramfs.
#        Reboot as prompted.
#     3. (Optional) Run `gpu_nds`: If using an NVIDIA GPU and proprietary drivers are
#        desired for host usage (when GPU is attached to host). This blacklists nouveau
#        and installs the NVIDIA driver. Reboot if prompted.
#
#   Scenario A: GPU Primarily for Passthrough (Persistent Setup)
#     (After completing the Initial One-Time Setup)
#     a. Run `gpu_pt3 enable`: Assigns specified GPU(s) to `vfio-pci` at boot.
#        This involves creating /etc/modprobe.d/vfio.conf with the GPU IDs and
#        blacklisting their host drivers (e.g., nvidia, amdgpu, nouveau) in
#        /etc/modprobe.d/zz-vfio-gpu-blacklist.conf. Reboot.
#        The GPU is now persistently configured for VM passthrough.
#     b. (Optional) If the GPU needs to be temporarily used by the host:
#        Run `gpu_pta` to attach it to host drivers.
#        Run `gpu_ptd` to detach it again for VM use. (No reboots needed for these).
#
#   Scenario B: GPU Primarily for Host, Dynamic Passthrough as Needed
#     (After completing the Initial One-Time Setup)
#     a. Ensure the system boots with the GPU attached to host drivers.
#        This is the default state if `gpu_pt3 enable` has not been run for the GPU,
#        or if `gpu_pt3 disable` was run to revert a persistent passthrough setup.
#        (If `gpu_nds` was run, the NVIDIA GPU will use proprietary drivers; otherwise,
#        it might use nouveau or amdgpu for AMD).
#     b. When passthrough is needed for a VM: Run `gpu_ptd` for the target GPU.
#        This dynamically attempts to:
#          - Ensure VFIO modules are loaded (they should be if `gpu_pt2` was run).
#          - Unbind the GPU from its host driver.
#          - Bind the GPU to `vfio-pci`.
#     c. After VM use: Run `gpu_pta` for the target GPU. This dynamically:
#          - Unbinds the GPU from `vfio-pci`.
#          - Rebinds the GPU to its appropriate host driver.
#     d. Note: If dynamic switching (`gpu_ptd`) faces issues (e.g., host drivers
#        re-asserting control, or instability), using `gpu_pt3 enable` (Scenario A)
#        for a persistent passthrough setup is generally more robust.
#
# ============================================================================

# Define directory and file variables
DIR_FUN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
FILE_FUN=$(basename "$BASH_SOURCE")
BASE_FUN="${FILE_FUN%.*}"
FILEPATH_FUN="${DIR_FUN}/${FILE_FUN}"
CONFIG_FUN="${SITE_CONFIG_FILE}"

# Dynamically create variables based on the base name
eval "FILEPATH_${BASE_FUN}=\$FILEPATH_FUN"
eval "FILE_${BASE_FUN}=\$FILE_FUN"
eval "BASE_${BASE_FUN}=\$BASE_FUN"
eval "CONFIG_${BASE_FUN}=\$CONFIG_FUN"

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

# Initialize color constants for GPU management output formatting
# GPU Initialize Colors - GIC
# 
_gpu_init_colors() {
    aux_dbg "Function ${FUNCNAME[0]} started" "component=gpu,operation=color_init"
    
    readonly GREEN=$(printf '\033[0;32m')
    readonly YELLOW=$(printf '\033[0;33m')
    readonly RED=$(printf '\033[0;31m')
    readonly CYAN=$(printf '\033[0;36m')
    readonly MAGENTA=$(printf '\033[0;35m')
    readonly INDIGO_BLUE=$(printf '\033[38;2;75;0;130m')
    readonly NC=$(printf '\033[0m') # No Color
    readonly CHECK_MARK="${GREEN}✓${NC}"
    readonly CROSS_MARK="${RED}✗${NC}"
    readonly QUESTION_MARK="${YELLOW}?${NC}"
    
    aux_dbg "Color constants initialized" "component=gpu,operation=color_init,status=complete"
}

# Validate PCI ID format using regex pattern matching
# GPU Validate PCI ID - GVP
# <pci_id>
_gpu_validate_pci_id() {
    local pci_id="$1"
    aux_dbg "Validating PCI ID format" "component=gpu,operation=validate_pci,pci_id=${pci_id}"
    
    if [[ "$pci_id" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then
        aux_dbg "PCI ID validation successful" "component=gpu,operation=validate_pci,pci_id=${pci_id},result=valid"
        return 0
    else
        aux_warn "PCI ID validation failed" "component=gpu,operation=validate_pci,pci_id=${pci_id},result=invalid,expected_format=XX:XX.X"
        return 1
    fi
}

# Extract vendor and device IDs from lspci output for PCI device
# GPU Extract Vendor Device ID - GEV
# <pci_id>
_gpu_extract_vendor_device_id() {
    local pci_id="$1"
    aux_dbg "Extracting vendor and device ID" "component=gpu,operation=extract_vendor_device,pci_id=${pci_id}"
    
    local lspci_device_info
    lspci_device_info=$(lspci -s "$pci_id" -nn 2>/dev/null)
    
    if [ -n "$lspci_device_info" ]; then
        # Extract vendor:device ID from the [xxxx:yyyy] pattern
        local vendor_device_id=$(echo "$lspci_device_info" | grep -o '\[[^]]*:[^]]*\]' | tail -1 | sed 's/\[\|\]//g')
        
        if [ -n "$vendor_device_id" ]; then
            local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
            local device_id=$(echo "$vendor_device_id" | cut -d':' -f2)
            aux_dbg "Successfully extracted vendor and device ID" "component=gpu,operation=extract_vendor_device,pci_id=${pci_id},vendor_id=${vendor_id},device_id=${device_id}"
            echo "${vendor_id}:${device_id}"
            return 0
        else
            aux_warn "Could not parse vendor:device ID from lspci output" "component=gpu,operation=extract_vendor_device,pci_id=${pci_id},lspci_info=${lspci_device_info}"
        fi
    else
        aux_warn "No lspci output for PCI device" "component=gpu,operation=extract_vendor_device,pci_id=${pci_id}"
    fi
    return 1
}

# Get current driver bound to specified PCI device
# GPU Get Current Driver - GGC
# <pci_id>
_gpu_get_current_driver() {
    local pci_id="$1"
    local full_pci_id="0000:$pci_id"
    local driver_path="/sys/bus/pci/devices/$full_pci_id/driver"
    
    aux_dbg "Checking current driver binding" "component=gpu,operation=get_current_driver,pci_id=${pci_id},driver_path=${driver_path}"
    
    if [ -L "$driver_path" ]; then
        local driver_name=$(basename "$(readlink -f "$driver_path")")
        aux_dbg "Driver found" "component=gpu,operation=get_current_driver,pci_id=${pci_id},driver=${driver_name}"
        echo "$driver_name"
    else
        aux_dbg "No driver bound" "component=gpu,operation=get_current_driver,pci_id=${pci_id},driver=none"
        echo "none"
    fi
}

# Check if PCI device is GPU-related hardware type
# GPU Is GPU Device - GIG
# <pci_id>
_gpu_is_gpu_device() {
    local pci_id="$1"
    aux_dbg "Checking if device is GPU-related" "component=gpu,operation=is_gpu_device,pci_id=${pci_id}"
    
    if lspci -s "$pci_id" -nn 2>/dev/null | grep -qE "VGA compatible controller|3D controller|Audio device"; then
        aux_dbg "Device confirmed as GPU-related" "component=gpu,operation=is_gpu_device,pci_id=${pci_id},result=true"
        return 0
    else
        aux_dbg "Device is not GPU-related" "component=gpu,operation=is_gpu_device,pci_id=${pci_id},result=false"
        return 1
    fi
}

# Load GPU configuration file if available and readable
# GPU Load Config - GLC
# 
_gpu_load_config() {
    aux_info "Loading GPU configuration" "component=gpu,operation=load_config,config_file=${CONFIG_FUN}"
    
    if [ -n "$CONFIG_FUN" ] && [ -f "$CONFIG_FUN" ]; then
        # shellcheck source=/dev/null
        source "$CONFIG_FUN"
        aux_info "Successfully sourced PCI configuration" "component=gpu,operation=load_config,config_file=${CONFIG_FUN},status=success"
        printf "INFO: Successfully sourced PCI configuration from %s\n" "$CONFIG_FUN"
        return 0
    else
        aux_warn "PCI Configuration file not found or not set" "component=gpu,operation=load_config,config_file=${CONFIG_FUN},status=failed"
        printf "${YELLOW}WARNING: PCI Configuration file '%s' not found or not set.${NC}\n" "$CONFIG_FUN"
        return 1
    fi
}

# Get PCI IDs from hostname-based configuration variables
# GPU Get Config PCI IDs - GGP
# <hostname>
_gpu_get_config_pci_ids() {
    local hostname="$1"
    local -a pci_ids=()
    
    local pci0_var_name="${hostname}_NODE_PCI0"
    local pci1_var_name="${hostname}_NODE_PCI1"
    
    if [ -n "${!pci0_var_name}" ]; then
        local pci0_full_val=${!pci0_var_name}
        local short_pci0_val=${pci0_full_val#0000:}
        if _gpu_validate_pci_id "$short_pci0_val"; then
            pci_ids+=("$short_pci0_val")
        fi
    fi
    
    if [ -n "${!pci1_var_name}" ]; then
        local pci1_full_val=${!pci1_var_name}
        local short_pci1_val=${pci1_full_val#0000:}
        if _gpu_validate_pci_id "$short_pci1_val"; then
            pci_ids+=("$short_pci1_val")
        fi
    fi
    
    printf "%s\n" "${pci_ids[@]}"
}

# Find all GPU devices via lspci scan with optional driver filter
# GPU Find All GPUs - GFA
# <filter_driver>
_gpu_find_all_gpus() {
    local filter_driver="$1" # Optional: only return GPUs using specific driver
    local -a gpu_ids=()
    
    local lspci_output
    lspci_output=$(lspci -nnk)
    
    while IFS= read -r line; do
        if echo "$line" | grep -qE "VGA compatible controller|3D controller"; then
            local pci_slot_id=$(echo "$line" | awk '{print $1}')
            
            if [ -n "$filter_driver" ]; then
                local current_driver
                current_driver=$(_gpu_get_current_driver "$pci_slot_id")
                if [ "$current_driver" = "$filter_driver" ]; then
                    gpu_ids+=("$pci_slot_id")
                fi
            else
                gpu_ids+=("$pci_slot_id")
            fi
        fi
    done <<< "$lspci_output"
    
    printf "%s\n" "${gpu_ids[@]}"
}

# Get target GPUs for processing with comprehensive selection logic
# GPU Get Target GPUs - GGT
# <gpu_id_arg> <hostname> <filter_driver>
_gpu_get_target_gpus() {
    local gpu_id_arg="$1"
    local hostname="$2"
    local filter_driver="$3" # Optional: vfio-pci, nvidia, etc.
    local -a gpus_to_process=()
    
    if [ -n "$gpu_id_arg" ]; then
        # Specific GPU ID provided
        if ! _gpu_validate_pci_id "$gpu_id_arg"; then
            printf "${RED}ERROR: Invalid PCI ID format provided: %s. Expected format like 01:00.0${NC}\n" "$gpu_id_arg"
            return 1
        fi
        
        if _gpu_is_gpu_device "$gpu_id_arg"; then
            if [ -n "$filter_driver" ]; then
                local current_driver
                current_driver=$(_gpu_get_current_driver "$gpu_id_arg")
                if [ "$current_driver" = "$filter_driver" ]; then
                    gpus_to_process+=("$gpu_id_arg")
                fi
            else
                gpus_to_process+=("$gpu_id_arg")
            fi
        else
            printf "${RED}ERROR: Specified PCI ID %s is not a VGA/3D/Audio controller or does not exist.${NC}\n" "$gpu_id_arg"
            return 1
        fi
    else
        # Try to get from configuration
        local -a config_ids
        readarray -t config_ids < <(_gpu_get_config_pci_ids "$hostname")
        
        # Process config IDs, but only add non-empty ones
        for pci_id in "${config_ids[@]}"; do
            if [ -n "$pci_id" ]; then
                if [ -n "$filter_driver" ]; then
                    local current_driver
                    current_driver=$(_gpu_get_current_driver "$pci_id")
                    if [ "$current_driver" = "$filter_driver" ]; then
                        gpus_to_process+=("$pci_id")
                    fi
                else
                    gpus_to_process+=("$pci_id")
                fi
            fi
        done
        
        # Fallback to lspci scan if no config IDs found
        if [ ${#gpus_to_process[@]} -eq 0 ]; then
            local -a fallback_ids
            readarray -t fallback_ids < <(_gpu_find_all_gpus "$filter_driver")
            gpus_to_process+=("${fallback_ids[@]}")
        fi
    fi
    
    # Remove duplicates
    if [ ${#gpus_to_process[@]} -gt 0 ]; then
        readarray -t gpus_to_process < <(printf "%s\n" "${gpus_to_process[@]}" | sort -u)
    fi
    
    printf "%s\n" "${gpus_to_process[@]}"
}

# Ensure required VFIO kernel modules are loaded for GPU passthrough
# GPU Ensure VFIO Modules - GEV
# 
_gpu_ensure_vfio_modules() {
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=ensure_vfio_modules"
    local modules=(vfio vfio_iommu_type1 vfio_pci)
    
    for module in "${modules[@]}"; do
        if lsmod | grep -q "^${module}"; then
            aux_info "Module already loaded" "component=gpu,operation=ensure_vfio_modules,module=${module},status=already_loaded"
            printf "INFO: Module %s is already loaded.\n" "$module"
        else
            aux_info "Attempting to load module" "component=gpu,operation=ensure_vfio_modules,module=${module},status=loading"
            printf "INFO: Attempting to load module %s...\n" "$module"
            if modprobe "$module"; then
                aux_info "Module loaded successfully" "component=gpu,operation=ensure_vfio_modules,module=${module},status=loaded"
                printf "INFO: Module %s loaded successfully.\n" "$module"
            else
                aux_err "Failed to load module" "component=gpu,operation=ensure_vfio_modules,module=${module},status=failed,error=modprobe_failed"
                printf "${RED}ERROR: Failed to load module %s.${NC}\n" "$module"
                if [ "$module" = "vfio_pci" ]; then
                    aux_err "Critical module vfio_pci failed to load, exiting" "component=gpu,operation=ensure_vfio_modules,module=${module},status=critical_failure"
                    printf "${RED}Exiting due to failure to load critical module vfio_pci.${NC}\n"
                    return 1
                fi
            fi
        fi
    done
    aux_info "VFIO modules check completed" "component=gpu,operation=ensure_vfio_modules,status=complete"
    return 0
}

# Unbind PCI device from its current driver safely
# GPU Unbind Device - GUD
# <pci_id>
_gpu_unbind_device() {
    local pci_id="$1"
    local full_pci_id="0000:$pci_id"
    local driver_path="/sys/bus/pci/devices/$full_pci_id/driver"
    
    aux_info "Attempting to unbind device" "component=gpu,operation=unbind_device,pci_id=${pci_id},driver_path=${driver_path}"
    
    if [ -L "$driver_path" ]; then
        local current_driver
        current_driver=$(basename "$(readlink -f "$driver_path")")
        aux_info "Unbinding GPU from current driver" "component=gpu,operation=unbind_device,pci_id=${pci_id},current_driver=${current_driver}"
        printf "INFO: Unbinding GPU %s from %s...\n" "$pci_id" "$current_driver"
        
        if echo "$full_pci_id" > "$driver_path/unbind"; then
            aux_info "Successfully unbound GPU" "component=gpu,operation=unbind_device,pci_id=${pci_id},previous_driver=${current_driver},status=success"
            printf "INFO: Successfully unbound GPU %s from %s.\n" "$pci_id" "$current_driver"
            return 0
        else
            aux_warn "Failed to unbind GPU" "component=gpu,operation=unbind_device,pci_id=${pci_id},current_driver=${current_driver},status=failed"
            printf "${YELLOW}WARNING: Failed to unbind GPU %s from %s.${NC}\n" "$pci_id" "$current_driver"
            return 1
        fi
    else
        aux_info "GPU not bound to any driver" "component=gpu,operation=unbind_device,pci_id=${pci_id},status=not_bound"
        printf "INFO: GPU %s is not currently bound to any driver.\n" "$pci_id"
        return 0
    fi
}

# Bind PCI device to specified driver with vendor/device ID setup
# GPU Bind Device - GBD
# <pci_id> <target_driver>
_gpu_bind_device() {
    local pci_id="$1"
    local target_driver="$2"
    local full_pci_id="0000:$pci_id"
    local driver_bind_path="/sys/bus/pci/drivers/$target_driver/bind"
    
    aux_info "Attempting to bind device to driver" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver}"
    
    # Check if driver directory exists
    if [ ! -d "/sys/bus/pci/drivers/$target_driver" ]; then
        aux_err "PCI driver directory does not exist" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},error=driver_dir_missing"
        printf "${RED}ERROR: PCI driver directory for %s does not exist.${NC}\n" "$target_driver"
        return 1
    fi
    
    printf "INFO: Attempting to bind GPU %s to driver %s...\n" "$pci_id" "$target_driver"
    
    if echo "$full_pci_id" > "$driver_bind_path"; then
        aux_info "Bind operation initiated" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},status=initiated"
        printf "INFO: Successfully initiated bind for GPU %s to %s.\n" "$pci_id" "$target_driver"
        sleep 1
        
        # Verify binding
        local current_driver
        current_driver=$(_gpu_get_current_driver "$pci_id")
        if [ "$current_driver" = "$target_driver" ]; then
            aux_info "GPU successfully bound to target driver" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},current_driver=${current_driver},status=success"
            printf "${GREEN}SUCCESS: GPU %s is now bound to %s.${NC}\n" "$pci_id" "$target_driver"
            return 0
        else
            aux_warn "GPU may not have bound correctly" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},current_driver=${current_driver},status=verification_failed"
            printf "${YELLOW}WARNING: GPU %s may not have bound to %s correctly. Current driver: %s${NC}\n" "$pci_id" "$target_driver" "$current_driver"
            return 1
        fi
    else
        aux_err "Failed to bind GPU to driver" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},status=failed,error=bind_operation_failed"
        printf "${RED}ERROR: Failed to bind GPU %s to %s.${NC}\n" "$pci_id" "$target_driver"
        return 1
    fi
}

# Determine appropriate host driver for GPU based on vendor ID
# GPU Get Host Driver - GGH
# <pci_id> <hostname>
_gpu_get_host_driver() {
    local pci_id="$1"
    local hostname="$2"
    
    local vendor_device_id
    vendor_device_id=$(_gpu_extract_vendor_device_id "$pci_id")
    
    if [ $? -ne 0 ] || [ -z "$vendor_device_id" ]; then
        printf "${YELLOW}WARNING: Could not determine vendor ID for %s.${NC}\n" "$pci_id"
        return 1
    fi
    
    local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
    
    case "$vendor_id" in
        "10de") # NVIDIA
            local nvidia_driver_pref_var="${hostname}_NVIDIA_DRIVER_PREFERENCE"
            local preferred_nvidia_driver
            if [ -n "${!nvidia_driver_pref_var}" ]; then
                preferred_nvidia_driver="${!nvidia_driver_pref_var}"
            else
                preferred_nvidia_driver="nvidia"
            fi
            echo "$preferred_nvidia_driver"
            ;;
        "1002") # AMD
            echo "amdgpu"
            ;;
        *)
            printf "${YELLOW}WARNING: Unknown GPU vendor ID %s for %s.${NC}\n" "$vendor_id" "$pci_id"
            return 1
            ;;
    esac
}

# Determine appropriate host driver for GPU with explicit parameters
# GPU Get Host Driver Parameterized - GHP
# <pci_id> <hostname> <nvidia_driver_preference>
_gpu_get_host_driver_parameterized() {
    local pci_id="$1"
    local hostname="$2"
    local nvidia_driver_preference="${3:-nvidia}"
    
    local vendor_device_id
    vendor_device_id=$(_gpu_extract_vendor_device_id "$pci_id")
    
    if [ $? -ne 0 ] || [ -z "$vendor_device_id" ]; then
        printf "${YELLOW}WARNING: Could not determine vendor ID for %s.${NC}\n" "$pci_id"
        return 1
    fi
    
    local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
    
    case "$vendor_id" in
        "10de") # NVIDIA
            echo "$nvidia_driver_preference"
            ;;
        "1002") # AMD
            echo "amdgpu"
            ;;
        *)
            printf "${YELLOW}WARNING: Unknown GPU vendor ID %s for %s.${NC}\n" "$vendor_id" "$pci_id"
            return 1
            ;;
    esac
}

# Get PCI IDs from explicit parameters without hostname lookup
# GPU Get Config PCI IDs Parameterized - GCP
# <pci0_id> <pci1_id>
_gpu_get_config_pci_ids_parameterized() {
    local pci0_id="$1"
    local pci1_id="$2"
    local -a pci_ids=()
    
    if [ -n "$pci0_id" ]; then
        local short_pci0_val=${pci0_id#0000:}
        if _gpu_validate_pci_id "$short_pci0_val"; then
            pci_ids+=("$short_pci0_val")
        fi
    fi
    
    if [ -n "$pci1_id" ]; then
        local short_pci1_val=${pci1_id#0000:}
        if _gpu_validate_pci_id "$short_pci1_val"; then
            pci_ids+=("$short_pci1_val")
        fi
    fi
    
    printf "%s\n" "${pci_ids[@]}"
}

# Get target GPUs for processing with explicit parameters
# GPU Get Target GPUs Parameterized - GTP
# <gpu_id_arg> <hostname> <filter_driver> <pci0_id> <pci1_id>
_gpu_get_target_gpus_parameterized() {
    local gpu_id_arg="$1"
    local hostname="$2"
    local filter_driver="$3" # Optional: vfio-pci, nvidia, etc.
    local pci0_id="$4"
    local pci1_id="$5"
    local -a gpus_to_process=()
    
    if [ -n "$gpu_id_arg" ]; then
        # Specific GPU ID provided
        if ! _gpu_validate_pci_id "$gpu_id_arg"; then
            printf "${RED}ERROR: Invalid PCI ID format provided: %s. Expected format like 01:00.0${NC}\n" "$gpu_id_arg"
            return 1
        fi
        
        if _gpu_is_gpu_device "$gpu_id_arg"; then
            if [ -n "$filter_driver" ]; then
                local current_driver
                current_driver=$(_gpu_get_current_driver "$gpu_id_arg")
                if [ "$current_driver" = "$filter_driver" ]; then
                    gpus_to_process+=("$gpu_id_arg")
                fi
            else
                gpus_to_process+=("$gpu_id_arg")
            fi
        else
            printf "${RED}ERROR: Specified PCI ID %s is not a VGA/3D/Audio controller or does not exist.${NC}\n" "$gpu_id_arg"
            return 1
        fi
    else
        # Try to get from configuration
        local -a config_ids
        readarray -t config_ids < <(_gpu_get_config_pci_ids_parameterized "$pci0_id" "$pci1_id")
        
        # Process config IDs, but only add non-empty ones
        for pci_id in "${config_ids[@]}"; do
            if [ -n "$pci_id" ]; then
                if [ -n "$filter_driver" ]; then
                    local current_driver
                    current_driver=$(_gpu_get_current_driver "$pci_id")
                    if [ "$current_driver" = "$filter_driver" ]; then
                        gpus_to_process+=("$pci_id")
                    fi
                else
                    gpus_to_process+=("$pci_id")
                fi
            fi
        done
        
        # Fallback to lspci scan if no config IDs found
        if [ ${#gpus_to_process[@]} -eq 0 ]; then
            local -a fallback_ids
            readarray -t fallback_ids < <(_gpu_find_all_gpus "$filter_driver")
            gpus_to_process+=("${fallback_ids[@]}")
        fi
    fi
    
    # Remove duplicates
    if [ ${#gpus_to_process[@]} -gt 0 ]; then
        readarray -t gpus_to_process < <(printf "%s\n" "${gpus_to_process[@]}" | sort -u)
    fi
    
    printf "%s\n" "${gpus_to_process[@]}"
}

# Get IOMMU groups containing GPU devices for passthrough validation
# GPU Get IOMMU Groups - GIG
# 
_gpu_get_iommu_groups() {
    local -a gpu_pci_bus_ids=()
    while IFS= read -r line; do
        gpu_pci_bus_ids+=("$(echo "$line" | awk '{print $1}')")
    done < <(lspci -nn | grep -iE "VGA compatible controller|3D controller")

    if [ ${#gpu_pci_bus_ids[@]} -eq 0 ]; then
        echo "No VGA/3D controllers found to filter IOMMU groups."
        return 1
    fi

    local overall_iommu_groups_found=false
    local gpu_related_group_displayed=false

    for iommu_group_dir in $(find /sys/kernel/iommu_groups/ -mindepth 1 -maxdepth 1 -type d 2>/dev/null | sort -V); do
        overall_iommu_groups_found=true
        local group_id=$(basename "$iommu_group_dir")
        local devices_in_group_output=""
        local this_group_contains_gpu=false

        for device_symlink_name in $(ls -1 "$iommu_group_dir"/devices/ 2>/dev/null | sort -V); do
            local device_pci_bus_id_full="$device_symlink_name"
            local device_pci_bus_id_short="${device_symlink_name#0000:}"
            local device_info
            device_info=$(lspci -nns "$device_pci_bus_id_full" 2>/dev/null)
            
            if [ -n "$device_info" ]; then
                devices_in_group_output+="  ${device_info}\n"
                for gpu_id in "${gpu_pci_bus_ids[@]}"; do
                    if [[ "$device_pci_bus_id_short" == "$gpu_id" ]]; then
                        this_group_contains_gpu=true
                        break 
                    fi
                done
            else
                devices_in_group_output+="  Error reading device info for $device_pci_bus_id_full\n"
            fi
        done

        if $this_group_contains_gpu; then
            echo -e "\nIOMMU Group ${group_id}:"
            echo -e "${devices_in_group_output%\n}" 
            gpu_related_group_displayed=true
        fi
    done

    if ! $overall_iommu_groups_found; then
        echo "No IOMMU groups found by 'find' command."
        return 1
    elif ! $gpu_related_group_displayed; then
        echo "No IOMMU groups found containing the identified GPU(s): ${gpu_pci_bus_ids[*]}"
        return 1
    fi
}

# Get detailed GPU device information including driver status and bindings
# GPU Get Detailed Device Info - GDI
# 
_gpu_get_detailed_device_info() {
    local temp_lspci_gpu_blocks="/tmp/gpu_pts_blocks.$$"
    lspci -nnk | grep -A4 -iE "VGA compatible controller|3D controller" > "$temp_lspci_gpu_blocks"
    
    local all_device_blocks_output=""
    local gpu_details_for_checklist=""
    local is_any_gpu_on_vfio=false
    local host_drivers_for_gpu=""
    local vga_devices_processed=false
    
    if [ -s "$temp_lspci_gpu_blocks" ]; then
        local current_gpu_block_content=""

        while IFS= read -r line_from_grep; do
            if echo "$line_from_grep" | grep -qE "VGA compatible controller|3D controller"; then
                # Process previous block if it exists and was a GPU block
                if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
                    all_device_blocks_output+="Device Block:\n$current_gpu_block_content\n\n"
                    vga_devices_processed=true
                    
                    local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')
                    local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
                    local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
                    local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
                    local device_name_short=$(echo "$device_name_raw" | sed 's/ \[.*\]//g' | awk '{print $1, $2}')
                    local device_name_for_checklist="$device_vendor_name $device_name_short"
                    device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
                    if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

                    if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                        is_any_gpu_on_vfio=true
                        gpu_details_for_checklist+="     ✓ GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\n"
                    elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                        local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                        if [ -n "$driver_name" ]; then
                            if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                            gpu_details_for_checklist+="     * GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\n"
                        else
                            gpu_details_for_checklist+="     ✗ GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\n"
                        fi
                    else
                        gpu_details_for_checklist+="     ✗ GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\n"
                    fi
                fi
                current_gpu_block_content="$line_from_grep"
            elif [[ "$line_from_grep" == "--" ]]; then
                current_gpu_block_content=""
            elif [ -n "$current_gpu_block_content" ]; then
                current_gpu_block_content="$current_gpu_block_content"$'\n'"$line_from_grep"
            fi
        done < "$temp_lspci_gpu_blocks"
        
        # Process the last block
        if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
            all_device_blocks_output+="Device Block:\n$current_gpu_block_content\n\n"
            vga_devices_processed=true
            
            local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')
            local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
            local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
            local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
            local device_name_short=$(echo "$device_name_raw" | sed 's/ \[.*\]//g' | awk '{print $1, $2}')
            local device_name_for_checklist="$device_vendor_name $device_name_short"
            device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
            if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

            if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                is_any_gpu_on_vfio=true
                gpu_details_for_checklist+="     ✓ GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\n"
            elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                if [ -n "$driver_name" ]; then
                    if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                    gpu_details_for_checklist+="     * GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\n"
                else
                    gpu_details_for_checklist+="     ✗ GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\n"
                fi
            else
                gpu_details_for_checklist+="     ✗ GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\n"
            fi
        fi
    fi
    
    rm -f "$temp_lspci_gpu_blocks"
    
    # Output results as delimited strings for parsing
    echo "DEVICE_BLOCKS:$all_device_blocks_output"
    echo "GPU_CHECKLIST:$gpu_details_for_checklist"
    echo "VFIO_STATUS:$is_any_gpu_on_vfio"
    echo "HOST_DRIVERS:$host_drivers_for_gpu"
    echo "DEVICES_PROCESSED:$vga_devices_processed"
}

# ============================================================================
# MAIN FUNCTIONS (Refactored)
# ============================================================================

# Shows function overview with optional filtering
# overview functions
# [function_name_filter]
gpu_fun() {
    # Technical Description:
    #   Displays formatted list of all functions in the GPU module with detailed documentation.
    #   Uses ana_laf utility to parse function definitions and extract comprehensive documentation
    #   including function names, descriptions, mnemonics, and parameter requirements.
    #   Provides optional filtering by function name pattern for focused analysis.
    #   Shows both helper functions (prefixed with _gpu_) and main user-facing functions.
    # Dependencies:
    #   - ana_laf function from auxiliary library for function analysis
    #   - Read access to GPU module file for parsing function definitions
    #   - grep for pattern matching if filtering is used
    # Arguments:
    #   $1: function_name_filter (optional) - pattern to filter function names
    #   --help/-h: Display technical usage information
    # Usage Examples:
    #   gpu_fun                    # List all functions in GPU module
    #   gpu_fun "vfio"            # Show only functions containing "vfio"
    #   gpu_fun "^gpu_pt"         # Show only passthrough functions
    
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=function_overview,filter=${1:-none}"
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_info "Displaying help information" "component=gpu,operation=function_overview,action=help"
        aux_tec
        return 0
    fi
    
    aux_perf "Starting function analysis" "component=gpu,operation=function_overview,file=${FILEPATH_gpu}"
    ana_laf "$FILEPATH_gpu" "$@"
    aux_info "Function overview completed" "component=gpu,operation=function_overview,status=complete"
}

# Shows configuration variables overview
# overview variables
# -x (execute)
gpu_var() {
    # Technical Description:
    #   Displays comprehensive overview of configuration variables specific to GPU module
    #   Uses ana_acu utility to scan and format variable definitions from configuration files
    #   Shows variable names, current values, and usage context within GPU management
    #   Provides organized view of GPU-related environment and configuration settings
    #   including hostname-specific PCI device mappings and driver preferences.
    # Dependencies:
    #   - ana_acu function from auxiliary library for configuration analysis
    #   - Read access to configuration files for variable extraction
    #   - CONFIG_gpu environment variable must be set for proper operation
    # Arguments:
    #   $1: -x - explicit execution flag required for consistency with module pattern
    #   --help/-h: Display technical usage information
    # Configuration Variables Displayed:
    #   - ${hostname}_NODE_PCI0: Primary GPU PCI ID for the current host
    #   - ${hostname}_NODE_PCI1: Secondary GPU PCI ID for the current host
    #   - ${hostname}_NVIDIA_DRIVER_PREFERENCE: Preferred NVIDIA driver (nvidia/nouveau)
    #   - Site-wide GPU configuration parameters
    
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=variable_overview,args=$*"
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_info "Displaying help information" "component=gpu,operation=variable_overview,action=help"
        aux_tec
        return 0
    fi
    
    if [ $# -ne 1 ] || [ "$1" != "-x" ]; then
        aux_warn "Invalid arguments provided" "component=gpu,operation=variable_overview,expected=-x,received=$*"
        aux_use
        return 1
    fi
    
    aux_info "Analyzing GPU configuration variables" "component=gpu,operation=variable_overview,config_file=${CONFIG_gpu}"
    ana_acu -o "$CONFIG_gpu" "$DIR_FUN/.."
    aux_info "Variable overview completed" "component=gpu,operation=variable_overview,status=complete"
}

# Downloads and installs NVIDIA drivers, blacklisting Nouveau drivers first
# nvidia driver setup
# [driver_version]
gpu_nds() {
    # Technical Description:
    #   Comprehensive NVIDIA proprietary driver installation process that handles
    #   the complete workflow from Nouveau blacklisting to driver compilation and installation.
    #   Automatically downloads specified NVIDIA driver version, prepares system by
    #   blacklisting conflicting Nouveau drivers, installs necessary build dependencies,
    #   and compiles drivers with DKMS support for kernel module persistence.
    #   Creates persistent blacklist configuration to prevent Nouveau from loading.
    # Dependencies:
    #   - Internet connectivity for driver download from NVIDIA servers
    #   - Root privileges for system modification (blacklists, driver installation)
    #   - Build tools: dkms, build-essential, kernel headers
    #   - update-initramfs for initrd regeneration
    # Arguments:
    #   $1: driver_version (optional) - NVIDIA driver version (default: 550.142)
    # System Changes:
    #   - Creates /etc/modprobe.d/blacklist-nouveau.conf with Nouveau blacklist
    #   - Installs NVIDIA driver with DKMS support for automatic kernel rebuilds
    #   - Updates initramfs to apply blacklist changes
    #   - Prompts for system reboot to activate new driver configuration
    # Post-Installation:
    #   - Verify installation with: nvidia-smi
    #   - Driver will be automatically rebuilt for kernel updates via DKMS
    local drv_ver="${1:-550.142}"
    local url="https://us.download.nvidia.com/XFree86/Linux-x86_64/${drv_ver}/NVIDIA-Linux-x86_64-${drv_ver}.run"
    local installer="NVIDIA-Linux-x86_64-${drv_ver}.run"

    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=nvidia_driver_setup,driver_version=${drv_ver}"
    aux_business "NVIDIA driver installation initiated" "component=gpu,operation=nvidia_driver_setup,driver_version=${drv_ver},installer_url=${url}"
    
    _gpu_init_colors

    aux_info "Blacklisting Nouveau driver" "component=gpu,operation=nvidia_driver_setup,step=blacklist_nouveau"
    echo "1) Blacklisting Nouveau..."
    cat <<EOF | tee /etc/modprobe.d/blacklist-nouveau.conf
blacklist nouveau
options nouveau modeset=0
EOF
    aux_perf "Updating initramfs" "component=gpu,operation=nvidia_driver_setup,step=update_initramfs"
    update-initramfs -u

    aux_info "Installing prerequisites" "component=gpu,operation=nvidia_driver_setup,step=install_prerequisites"
    echo "2) Installing prerequisites..."
    apt update
    apt install -y dkms build-essential pve-headers-$(uname -r)

    aux_info "Downloading NVIDIA driver" "component=gpu,operation=nvidia_driver_setup,step=download,driver_version=${drv_ver},url=${url}"
    echo "3) Downloading NVIDIA driver ${drv_ver}..."
    if wget -q "${url}" -O "${installer}"; then
        aux_info "Driver download successful" "component=gpu,operation=nvidia_driver_setup,step=download,status=success,file=${installer}"
        chmod +x "${installer}"
    else
        aux_err "Driver download failed" "component=gpu,operation=nvidia_driver_setup,step=download,status=failed,url=${url}"
        echo "Download failed"
        return 1
    fi

    aux_info "Installing NVIDIA driver with DKMS" "component=gpu,operation=nvidia_driver_setup,step=install_driver,installer=${installer}"
    echo "4) Installing driver with DKMS support..."
    ./"${installer}" --dkms --silent
    aux_business "NVIDIA driver installation completed" "component=gpu,operation=nvidia_driver_setup,step=install_driver,status=complete,driver_version=${drv_ver}"

    echo "5) Finalizing—reboot recommended."
    read -p "Reboot now? [y/N] " yn
    if [[ "$yn" =~ ^[Yy]$ ]]; then
        aux_info "System reboot initiated by user" "component=gpu,operation=nvidia_driver_setup,step=reboot,action=immediate"
        reboot
    else
        aux_info "System reboot deferred by user" "component=gpu,operation=nvidia_driver_setup,step=reboot,action=deferred"
        echo "You can reboot later with: reboot"
    fi

    echo "6) After reboot, verify with: nvidia-smi"
    aux_info "NVIDIA driver setup completed" "component=gpu,operation=nvidia_driver_setup,status=complete,verification_command=nvidia-smi"
}

# Configures initial GRUB and EFI settings for GPU passthrough
# passthrough step one
#
gpu_pt1() {
    # Technical Description:
    #   First phase of GPU passthrough setup that establishes fundamental system-level
    #   requirements for IOMMU (Input-Output Memory Management Unit) functionality.
    #   Modifies GRUB configuration to enable IOMMU support in kernel command line,
    #   which is essential for hardware virtualization and PCI device isolation.
    #   Updates both GRUB legacy and GRUB2 configurations and ensures EFI boot manager
    #   compatibility. This step must be completed before VFIO module setup.
    # Dependencies:
    #   - Root privileges for GRUB configuration modification
    #   - EFI boot environment support
    #   - grub-efi-amd64 package installation capability
    # System Changes:
    #   - Modifies /etc/default/grub to add "iommu=pt" to GRUB_CMDLINE_LINUX_DEFAULT
    #   - Regenerates GRUB configuration files via update-grub and update-grub2
    #   - Installs grub-efi-amd64 package for EFI system compatibility
    #   - Requires system reboot for kernel command line changes to take effect
    # Prerequisites:
    #   - System must support hardware virtualization (VT-x/AMD-V)
    #   - IOMMU must be supported and enabled in BIOS/UEFI settings
    # Next Steps:
    #   - After reboot, proceed with gpu_pt2 for VFIO kernel module setup
    local function_name="${FUNCNAME[0]}"
    
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=passthrough_step1,phase=initial_setup"
    aux_business "GPU passthrough phase 1 initiated" "component=gpu,operation=passthrough_step1,phase=iommu_setup,step=grub_configuration"
    
    echo "Executing section 1:"
    aux_info "Checking EFI boot manager status" "component=gpu,operation=passthrough_step1,step=efi_check"
    efibootmgr -v

    # Edit GRUB configuration
    aux_info "Modifying GRUB configuration for IOMMU" "component=gpu,operation=passthrough_step1,step=grub_config,parameter=iommu=pt"
    sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="quiet"/GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt"/' /etc/default/grub
    
    aux_perf "Updating GRUB configuration" "component=gpu,operation=passthrough_step1,step=grub_update"
    update-grub
    update-grub2

    # Install grub-efi-amd64
    aux_info "Installing GRUB EFI package" "component=gpu,operation=passthrough_step1,step=grub_efi_install,package=grub-efi-amd64"
    apt install grub-efi-amd64 -y

    aux_business "GPU passthrough phase 1 completed, system reboot required" "component=gpu,operation=passthrough_step1,phase=complete,next_step=reboot"
    printf "%s: Completed section 1, system will reboot now.\n" "$function_name"
    aux_info "System reboot initiated" "component=gpu,operation=passthrough_step1,step=reboot,reason=iommu_kernel_parameter"
    reboot
}

# Adds necessary kernel modules for GPU passthrough
# passthrough step two
#
gpu_pt2() {
    # Technical Description:
    #   Second phase of GPU passthrough setup that configures essential VFIO kernel modules
    #   for Virtual Function I/O support. Adds vfio, vfio_iommu_type1, and vfio_pci modules
    #   to /etc/modules to ensure they load automatically at boot time. These modules are
    #   fundamental for PCI device passthrough functionality, enabling the kernel to
    #   manage device isolation and provide secure device access to virtual machines.
    #   Updates initramfs to include VFIO modules in early boot environment.
    # Dependencies:
    #   - Successful completion of gpu_pt1 (IOMMU enabled in kernel command line)
    #   - Root privileges for system file modification
    #   - initramfs-tools for initrd generation
    # System Changes:
    #   - Adds "vfio", "vfio_iommu_type1", "vfio_pci" entries to /etc/modules
    #   - Regenerates initramfs for all kernel versions via update-initramfs
    #   - Ensures VFIO modules are available during early boot process
    #   - Requires system reboot for module loading changes to take effect
    # Module Functions:
    #   - vfio: Core Virtual Function I/O framework
    #   - vfio_iommu_type1: IOMMU driver for device isolation and memory management
    #   - vfio_pci: PCI device binding interface for device passthrough
    # Next Steps:
    #   - After reboot, proceed with gpu_pt3 for specific GPU device configuration
    local function_name="${FUNCNAME[0]}"
    
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=passthrough_step2,phase=vfio_modules"
    aux_business "GPU passthrough phase 2 initiated" "component=gpu,operation=passthrough_step2,phase=vfio_setup,step=module_configuration"
    
    echo "Executing section 2:"

    # Add modules to /etc/modules
    local modules=(vfio vfio_iommu_type1 vfio_pci)
    aux_info "Adding VFIO modules to /etc/modules" "component=gpu,operation=passthrough_step2,step=add_modules,modules=${modules[*]}"
    for module in "${modules[@]}"; do
        aux_dbg "Adding module to /etc/modules" "component=gpu,operation=passthrough_step2,step=add_modules,module=${module}"
        echo "$module" >> /etc/modules
    done

    # Update initramfs
    aux_perf "Updating initramfs for all kernels" "component=gpu,operation=passthrough_step2,step=update_initramfs"
    update-initramfs -u -k all

    aux_business "GPU passthrough phase 2 completed, system reboot required" "component=gpu,operation=passthrough_step2,phase=complete,next_step=reboot"
    printf "%s: Completed section 2, system will reboot now.\n" "$function_name"
    aux_info "System reboot initiated" "component=gpu,operation=passthrough_step2,step=reboot,reason=vfio_module_loading"
    reboot
}

# Finalizes or reverts GPU passthrough setup
# passthrough step three
# <enable|disable>
gpu_pt3() {
    # Technical Description:
    #   Final configuration phase for persistent GPU passthrough setup that handles
    #   device-specific binding and driver blacklisting. Creates or removes modprobe
    #   configurations that permanently assign detected GPU devices to vfio-pci driver
    #   and blacklists conflicting host drivers. When enabling, scans all GPU devices
    #   and generates vendor:device ID mappings for VFIO assignment. When disabling,
    #   removes persistent configurations to restore normal host GPU functionality.
    # Dependencies:
    #   - Successful completion of gpu_pt1 and gpu_pt2 (IOMMU and VFIO modules)
    #   - Root privileges for modprobe configuration management
    #   - lspci utility for GPU device detection and ID extraction
    # Arguments:
    #   $1: action - "enable" to configure persistent passthrough, "disable" to revert
    # System Changes (enable):
    #   - Creates /etc/modprobe.d/vfio.conf with "options vfio-pci ids=<gpu_ids>"
    #   - Creates /etc/modprobe.d/zz-vfio-gpu-blacklist.conf blacklisting host drivers
    #   - Blacklists nvidia, nouveau (NVIDIA), amdgpu, radeon (AMD) as appropriate
    #   - Updates initramfs to apply driver binding changes
    # System Changes (disable):
    #   - Removes /etc/modprobe.d/vfio.conf and zz-vfio-gpu-blacklist.conf
    #   - Restores normal host driver binding for GPU devices
    #   - Updates initramfs to apply configuration removal
    # Device Detection:
    #   - Automatically detects all VGA and 3D controller devices via lspci
    #   - Extracts vendor:device IDs for VFIO binding configuration
    #   - Handles both NVIDIA (10de) and AMD (1002) GPU vendors
    local action="$1"
    local function_name="${FUNCNAME[0]}"
    
    _gpu_init_colors
    
    if [ -z "$action" ]; then
        printf "Usage: %s <enable|disable>\n" "$function_name"
        return 1
    fi

    local vfio_conf="/etc/modprobe.d/vfio.conf"
    local passthrough_blacklist_conf="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"
    local modules_file="/etc/modules"

    if [ "$action" = "enable" ]; then
        printf "Configuring GPU passthrough (vfio-pci)...\n"
        
        # Get all GPU vendor:device IDs
        local -a gpu_ids
        readarray -t gpu_ids < <(_gpu_find_all_gpus)
        
        if [ ${#gpu_ids[@]} -eq 0 ]; then
            printf "No GPU devices found. Nothing to do.\n"
            return 0
        fi

        local vfio_options_line="options vfio-pci ids="
        local ids_to_add=""
        local nvidia_gpus_for_vfio=false
        local amd_gpus_for_vfio=false

        for pci_id in "${gpu_ids[@]}"; do
            local vendor_device_id
            vendor_device_id=$(_gpu_extract_vendor_device_id "$pci_id")
            
            if [ $? -eq 0 ] && [ -n "$vendor_device_id" ]; then
                printf "Found GPU: %s with ID %s\n" "$pci_id" "$vendor_device_id"
                
                if [ -n "$ids_to_add" ]; then
                    ids_to_add+=","
                fi
                ids_to_add+="$vendor_device_id"

                local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
                case "$vendor_id" in
                    "10de") nvidia_gpus_for_vfio=true ;;
                    "1002") amd_gpus_for_vfio=true ;;
                esac
            fi
        done

        if [ -n "$ids_to_add" ]; then
            vfio_options_line+="$ids_to_add"
            echo "$vfio_options_line" | tee "$vfio_conf" > /dev/null
            printf "Content of %s:\n" "$vfio_conf"
            cat "$vfio_conf"
        fi

        # Create blacklist file
        printf "Creating GPU driver blacklist for passthrough...\n"
        rm -f "$passthrough_blacklist_conf"
        touch "$passthrough_blacklist_conf"

        if $nvidia_gpus_for_vfio; then
            {
                echo "blacklist nouveau"
                echo "options nouveau modeset=0"
                echo "blacklist nvidia"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        fi

        if $amd_gpus_for_vfio; then
            {
                echo "blacklist radeon"
                echo "blacklist amdgpu"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        fi

        # Ensure VFIO modules are in /etc/modules
        local vfio_modules=(vfio vfio_iommu_type1 vfio_pci)
        for module in "${vfio_modules[@]}"; do
            if ! grep -qP "^\s*${module}\s*(#.*)?$" "$modules_file"; then
                echo "$module" | tee -a "$modules_file" > /dev/null
                printf "Added %s to %s.\n" "$module" "$modules_file"
            else
                # Uncomment if commented
                if grep -qP "^\s*#\s*${module}" "$modules_file"; then
                    sed -i "s/^\s*#\s*${module}/${module}/" "$modules_file"
                    printf "Uncommented %s in %s.\n" "$module" "$modules_file"
                fi
            fi
        done

    elif [ "$action" = "disable" ]; then
        printf "Reverting GPU passthrough configuration...\n"
        
        # Remove configuration files
        rm -f "$vfio_conf"
        rm -f "$passthrough_blacklist_conf"

        # Comment out VFIO modules
        local vfio_modules=(vfio vfio_iommu_type1 vfio_pci)
        for module in "${vfio_modules[@]}"; do
            if grep -qP "^\s*${module}" "$modules_file"; then
                sed -i "s/^\s*${module}/# ${module}/" "$modules_file"
                printf "Commented out %s in %s.\n" "$module" "$modules_file"
            fi
        done
        
    else
        printf "Invalid action: %s. Use 'enable' or 'disable'.\n" "$action"
        return 1
    fi

    printf "Updating initramfs...\n"
    update-initramfs -u -k all
    printf "Configuration applied. A reboot is required for changes to take full effect.\n"
}

# Detaches the GPU from the host system for VM passthrough
# passthrough detach
# [gpu_id] [hostname] [config_file] [pci0_id] [pci1_id] [nvidia_driver_preference]
gpu_ptd() {
    # Technical Description:
    #   Dynamic GPU detachment function that performs runtime switching of GPU control
    #   from host drivers to vfio-pci for virtual machine passthrough without requiring
    #   system reboot. Attempts to be self-sufficient by loading required VFIO modules,
    #   unbinding current host drivers, and binding target GPUs to vfio-pci using
    #   driver_override mechanism. Validates IOMMU availability and provides warnings
    #   for potential issues. Designed to work with both persistent and non-persistent
    #   GPU passthrough configurations.
    # Dependencies:
    #   - IOMMU enabled in kernel (set by gpu_pt1, verified but not enforced)
    #   - VFIO kernel modules available (loaded dynamically if needed)
    #   - Root privileges for driver binding operations
    #   - Target GPU devices not critical for current display output
    # Arguments:
    #   $1: gpu_id_arg (optional) - specific PCI ID (e.g., "01:00.0") to process
    #   $2: hostname (optional) - hostname for config lookup (default: current hostname)
    #   $3: config_file (optional) - configuration file path (default: SITE_CONFIG_FILE)
    #   $4: pci0_id (optional) - explicit primary PCI ID parameter
    #   $5: pci1_id (optional) - explicit secondary PCI ID parameter
    #   $6: nvidia_driver_preference (optional) - preferred NVIDIA driver (default: nvidia)
    # Operations Performed:
    #   - Validates IOMMU kernel parameter presence in /proc/cmdline
    #   - Loads vfio, vfio_iommu_type1, vfio_pci modules if not already loaded
    #   - Identifies target GPU devices from parameters or configuration
    #   - Unbinds GPUs from current host drivers (nvidia, nouveau, amdgpu, radeon)
    #   - Sets driver_override to "vfio-pci" and triggers driver re-probe
    #   - Verifies successful binding to vfio-pci driver
    # Device Selection Priority:
    #   1. Explicit gpu_id_arg parameter
    #   2. Configuration file hostname-specific PCI IDs
    #   3. Automatic detection of all GPU devices via lspci scan
    local gpu_id_arg="$1"
    local hostname="${2:-$(hostname -s)}"
    local config_file="${3:-$CONFIG_FUN}"
    local pci0_id="$4"
    local pci1_id="$5"  
    local nvidia_driver_preference="${6:-nvidia}"
    
    local function_name="${FUNCNAME[0]}"
    
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=passthrough_detach,hostname=${hostname},gpu_id=${gpu_id_arg:-auto},config_file=${config_file}"
    aux_business "GPU detachment process initiated" "component=gpu,operation=passthrough_detach,target_state=vfio-pci,hostname=${hostname}"
    
    _gpu_init_colors
    
    printf "INFO: Starting GPU detachment process\n"

    # Check IOMMU
    if ! grep -q 'iommu=pt\|intel_iommu=on\|amd_iommu=on' /proc/cmdline; then
        aux_warn "IOMMU not enabled in kernel command line" "component=gpu,operation=passthrough_detach,check=iommu,status=disabled,impact=vfio_may_fail"
        printf "${YELLOW}WARNING: IOMMU does not appear to be enabled in kernel command line.${NC}\n"
        printf "${YELLOW}         VFIO passthrough may not work.${NC}\n"
    else
        aux_info "IOMMU enabled in kernel" "component=gpu,operation=passthrough_detach,check=iommu,status=enabled"
    fi

    # Load configuration if config file provided
    if [ -n "$config_file" ] && [ -f "$config_file" ]; then
        # shellcheck source=/dev/null
        source "$config_file"
        aux_info "GPU configuration loaded for detachment" "component=gpu,operation=passthrough_detach,config_file=${config_file},status=success"
        printf "INFO: Successfully sourced GPU configuration from %s\n" "$config_file"
    else
        aux_warn "GPU configuration file not available for detachment" "component=gpu,operation=passthrough_detach,config_file=${config_file},status=unavailable"
    fi

    # Unload GPU drivers to prevent conflicts (critical fix from old gpu-ptd)
    local drivers_to_unload=(nouveau nvidia amdgpu radeon)
    for driver in "${drivers_to_unload[@]}"; do
        if lsmod | grep -q "^${driver//-/_}"; then
            aux_info "Unloading GPU driver for clean detachment" "component=gpu,operation=passthrough_detach,driver=${driver},action=unload"
            printf "INFO: Unloading %s driver for clean detachment...\n" "$driver"
            if modprobe -r "$driver" 2>/dev/null; then
                aux_info "GPU driver unloaded successfully" "component=gpu,operation=passthrough_detach,driver=${driver},status=success"
                printf "INFO: Successfully unloaded %s driver.\n" "$driver"
            else
                aux_warn "GPU driver unload failed" "component=gpu,operation=passthrough_detach,driver=${driver},status=failed"
                printf "${YELLOW}WARNING: Failed to unload %s driver. Continuing anyway.${NC}\n" "$driver"
            fi
        else
            printf "INFO: %s driver not loaded.\n" "$driver"
        fi
    done

    # Ensure VFIO modules are loaded
    if ! _gpu_ensure_vfio_modules; then
        aux_err "VFIO modules loading failed" "component=gpu,operation=passthrough_detach,step=vfio_modules,status=failed"
        return 1
    fi

    # Get target GPUs (exclude those already on vfio-pci)
    local -a gpus_to_process
    readarray -t gpus_to_process < <(_gpu_get_target_gpus_parameterized "$gpu_id_arg" "$hostname" "" "$pci0_id" "$pci1_id")
    
    # Filter out GPUs already on vfio-pci
    local -a filtered_gpus=()
    for pci_id in "${gpus_to_process[@]}"; do
        local current_driver
        current_driver=$(_gpu_get_current_driver "$pci_id")
        if [ "$current_driver" != "vfio-pci" ]; then
            filtered_gpus+=("$pci_id")
        else
            printf "INFO: GPU %s is already on vfio-pci. Skipping.\n" "$pci_id"
        fi
    done

    if [ ${#filtered_gpus[@]} -eq 0 ]; then
        aux_warn "No suitable GPU devices found for detachment" "component=gpu,operation=passthrough_detach,target_gpus=0,reason=no_suitable_devices"
        printf "No suitable GPU devices found for detachment.\n"
        return 0
    fi

    aux_info "Processing GPUs for detachment" "component=gpu,operation=passthrough_detach,gpu_count=${#filtered_gpus[@]},gpu_ids=${filtered_gpus[*]}"
    printf "INFO: Processing GPU IDs for detachment: %s\n" "${filtered_gpus[*]}"

    # Process each GPU
    for pci_id in "${filtered_gpus[@]}"; do
        aux_info "Processing individual GPU for detachment" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=individual_processing"
        printf "--- Processing GPU %s ---\n" "$pci_id"
        
        # Unbind from current driver
        _gpu_unbind_device "$pci_id"
        
        # Set driver override and bind to vfio-pci
        local full_pci_id="0000:$pci_id"
        printf "INFO: Setting driver_override to vfio-pci for GPU %s...\n" "$pci_id"
        
        if echo "vfio-pci" > "/sys/bus/pci/devices/$full_pci_id/driver_override"; then
            aux_info "Driver override set successfully" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},driver_override=vfio-pci,status=success"
            printf "INFO: Successfully set driver_override for %s.\n" "$pci_id"
            
            # Trigger re-probe
            if echo "$full_pci_id" > "/sys/bus/pci/drivers_probe"; then
                aux_info "Driver re-probe triggered" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=reprobe,status=success"
                printf "INFO: Re-probe triggered for %s.\n" "$pci_id"
                sleep 1
            else
                aux_warn "Driver re-probe failed" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=reprobe,status=failed"
            fi
        else
            aux_err "Failed to set driver override" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},driver_override=vfio-pci,status=failed"
        fi
        
        # Verify binding or try direct bind
        local current_driver
        current_driver=$(_gpu_get_current_driver "$pci_id")
        if [ "$current_driver" = "vfio-pci" ]; then
            aux_info "GPU successfully bound to vfio-pci" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},final_driver=vfio-pci,status=success"
            printf "INFO: GPU %s successfully bound to vfio-pci.\n" "$pci_id"
        else
            aux_info "Attempting direct bind to vfio-pci" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=direct_bind,current_driver=${current_driver}"
            printf "INFO: Attempting direct bind to vfio-pci for %s...\n" "$pci_id"
            if ! _gpu_bind_device "$pci_id" "vfio-pci"; then
                aux_err "Direct bind to vfio-pci failed" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=direct_bind,status=failed"
            fi
        fi
    done

    aux_business "GPU detachment process completed" "component=gpu,operation=passthrough_detach,processed_gpus=${#filtered_gpus[@]},status=complete"
    printf "INFO: GPU detachment process completed.\n"
}

# Attaches the GPU back to the host system
# passthrough attach
# [gpu_id] [hostname] [config_file] [pci0_id] [pci1_id] [nvidia_driver_preference]
gpu_pta() {
    # Technical Description:
    #   Dynamic GPU attachment function that performs runtime switching of GPU control
    #   from vfio-pci back to appropriate host drivers for normal system usage.
    #   Intelligently determines correct host driver based on GPU vendor ID and
    #   configuration preferences, handles driver loading if necessary, and manages
    #   the complete transition from virtualization-ready state back to host control.
    #   Provides the reverse operation of gpu_ptd for flexible GPU management workflows.
    # Dependencies:
    #   - Target GPU devices currently bound to vfio-pci driver
    #   - Appropriate host drivers available (nvidia, nouveau, amdgpu, radeon)
    #   - Root privileges for driver binding and module loading operations
    #   - Configuration file access for driver preference settings
    # Arguments:
    #   $1: gpu_id_arg (optional) - specific PCI ID (e.g., "01:00.0") to process
    #   $2: hostname (optional) - hostname for config lookup (default: current hostname)
    #   $3: config_file (optional) - configuration file path (default: SITE_CONFIG_FILE)
    #   $4: pci0_id (optional) - explicit primary PCI ID parameter
    #   $5: pci1_id (optional) - explicit secondary PCI ID parameter
    #   $6: nvidia_driver_preference (optional) - preferred NVIDIA driver (default: nvidia)
    # Operations Performed:
    #   - Identifies GPUs currently bound to vfio-pci driver
    #   - Unbinds target GPUs from vfio-pci driver
    #   - Clears driver_override setting to allow normal driver binding
    #   - Determines appropriate host driver based on vendor ID and preferences
    #   - Loads required host driver module if not already loaded
    #   - Binds GPU devices to determined host driver
    #   - Verifies successful attachment to host driver
    # Driver Selection Logic:
    #   - NVIDIA GPUs (10de): Uses hostname_NVIDIA_DRIVER_PREFERENCE or "nvidia" default
    #   - AMD GPUs (1002): Uses "amdgpu" driver
    #   - Considers blacklist conflicts and warns if preferred driver is unavailable
    # Configuration Integration:
    #   - Respects ${hostname}_NVIDIA_DRIVER_PREFERENCE configuration variable
    #   - Warns if nouveau is preferred but blacklisted by gpu_nds installation
    local gpu_id_arg="$1"
    local hostname="${2:-$(hostname -s)}"
    local config_file="${3:-$CONFIG_FUN}"
    local pci0_id="$4"
    local pci1_id="$5"
    local nvidia_driver_preference="${6:-nvidia}"
    
    local function_name="${FUNCNAME[0]}"
    
    _gpu_init_colors
    
    printf "INFO: Starting GPU attachment process...\n"

    # Load configuration if config file provided
    if [ -n "$config_file" ] && [ -f "$config_file" ]; then
        # shellcheck source=/dev/null
        source "$config_file"
        printf "INFO: Successfully sourced GPU configuration from %s\n" "$config_file"
    fi

    # Get target GPUs (only those on vfio-pci)
    local -a gpus_to_process
    readarray -t gpus_to_process < <(_gpu_get_target_gpus_parameterized "$gpu_id_arg" "$hostname" "vfio-pci" "$pci0_id" "$pci1_id")

    if [ ${#gpus_to_process[@]} -eq 0 ]; then
        printf "${YELLOW}No suitable GPU devices found on vfio-pci to attach.${NC}\n"
        return 0
    fi

    printf "INFO: Final list of GPU IDs to process for attachment: %s\n" "${gpus_to_process[*]}"

    # Process each GPU
    for pci_id in "${gpus_to_process[@]}"; do
        printf "${CYAN}--- Processing GPU %s for attachment to host ---${NC}\n" "$pci_id"
        
        # Unbind from vfio-pci
        _gpu_unbind_device "$pci_id"
        
        # Clear driver override
        local full_pci_id="0000:$pci_id"
        printf "INFO: Clearing driver_override for %s...\n" "$pci_id"
        echo > "/sys/bus/pci/devices/$full_pci_id/driver_override"
        
        # Determine host driver
        local host_driver
        host_driver=$(_gpu_get_host_driver_parameterized "$pci_id" "$hostname" "$nvidia_driver_preference")
        
        if [ $? -ne 0 ] || [ -z "$host_driver" ]; then
            printf "${YELLOW}WARNING: Could not determine host driver for %s. Skipping.${NC}\n" "$pci_id"
            continue
        fi
        
        printf "INFO: Target host driver for GPU %s: %s\n" "$pci_id" "$host_driver"
        
        # Load host driver module if needed
        if ! lsmod | grep -q "^${host_driver//-/_}"; then
            printf "INFO: Host driver module %s not loaded. Attempting to load...\n" "$host_driver"
            if ! modprobe "$host_driver"; then
                printf "${RED}ERROR: Failed to load module %s. Cannot attach GPU %s.${NC}\n" "$host_driver" "$pci_id"
                continue
            fi
        fi
        
        # DEBUG: Check host_driver value
        printf "DEBUG: host_driver='%s' (comparing to 'nvidia')\n" "$host_driver"
        
        # Simple minimal attachment - based on working gpu-pta function
        printf "INFO: Attempting to bind GPU %s to driver %s using minimal approach...\n" "$pci_id" "$host_driver"
        
        # Probe for new driver (like the working old function)
        printf "INFO: Probing for new driver for GPU %s\n" "$pci_id"
        echo "0000:$pci_id" > /sys/bus/pci/drivers_probe
        
        # Explicitly bind the driver if it's not automatically bound (like the working old function)
        if [ ! -e "/sys/bus/pci/devices/0000:$pci_id/driver" ]; then
            printf "INFO: Attempting to explicitly bind %s to GPU %s\n" "$host_driver" "$pci_id"
            if echo "0000:$pci_id" > "/sys/bus/pci/drivers/$host_driver/bind" 2>/dev/null; then
                printf "${GREEN}SUCCESS: GPU %s is now bound to %s.${NC}\n" "$pci_id" "$host_driver"
            else
                printf "${RED}ERROR: Failed to bind %s to GPU %s${NC}\n" "$host_driver" "$pci_id"
                continue
            fi
        else
            printf "${GREEN}SUCCESS: GPU %s is now bound to %s.${NC}\n" "$pci_id" "$host_driver"
        fi
        
        # Skip per-GPU nvidia_drm reload - will be done once after all GPUs processed
    done

    # CRITICAL FIX: NVIDIA DRM modeset reload AFTER all GPUs processed
    # This prevents "module in use" errors when processing multiple GPUs
    printf "INFO: Checking for NVIDIA driver configuration...\n"
    
    # Check if any GPU was bound to nvidia driver
    local has_nvidia_gpu=false
    for pci_id in $gpu_list; do
        local current_driver=$(lspci -nnk -s "${pci_id}" | awk '/Kernel driver in use:/ {print $5}')
        if [ "$current_driver" = "nvidia" ]; then
            has_nvidia_gpu=true
            break
        fi
    done
    
    # If NVIDIA GPUs found, configure framebuffer support
    if [ "$has_nvidia_gpu" = "true" ]; then
        printf "INFO: NVIDIA GPUs detected - configuring framebuffer display support...\n"
        
        # Load nvidia_modeset if not loaded
        if ! lsmod | grep -q "^nvidia_modeset"; then
            printf "INFO: Loading nvidia_modeset module...\n"
            modprobe nvidia_modeset
        fi
        
        # THE CRITICAL FIX: Reload nvidia_drm with modeset=1 ONCE for all GPUs
        printf "INFO: Reloading nvidia_drm with modeset=1 for display support...\n"
        
        # Remove nvidia_drm if loaded
        if lsmod | grep -q "^nvidia_drm"; then
            printf "INFO: Removing nvidia_drm module...\n"
            if ! rmmod nvidia_drm 2>/dev/null; then
                printf "WARNING: nvidia_drm module in use, attempting modprobe -r...\n"
                modprobe -r nvidia_drm 2>/dev/null || printf "WARNING: Could not remove nvidia_drm\n"
            fi
        fi
        
        # Load nvidia_drm with modeset=1
        printf "INFO: Loading nvidia_drm with modeset=1...\n"
        if modprobe nvidia_drm modeset=1; then
            local final_modeset_status=$(cat /sys/module/nvidia_drm/parameters/modeset 2>/dev/null || echo "UNKNOWN")
            if [ "$final_modeset_status" = "Y" ]; then
                printf "SUCCESS: NVIDIA framebuffer display support enabled (modeset=Y).\n"
            else
                printf "WARNING: modeset parameter not set correctly (status: %s)\n" "$final_modeset_status"
            fi
        else
            printf "ERROR: Failed to load nvidia_drm with modeset=1\n"
        fi
    fi

    printf "INFO: GPU attachment process completed.\n"
}

# Checks the current status of the GPU (complete detailed version)
# passthrough status
# [hostname] [config_file] [pci0_id] [pci1_id] [nvidia_driver_preference]
gpu_pts() {
    # Technical Description:
    #   Comprehensive GPU passthrough status analysis function that provides detailed
    #   system state information across all aspects of GPU passthrough configuration.
    #   Performs thorough examination of IOMMU groups, device bindings, kernel modules,
    #   configuration files, and runtime status to present complete picture of current
    #   GPU passthrough setup. Includes both high-level summary and detailed technical
    #   information for troubleshooting and verification purposes.
    # Dependencies:
    #   - lspci utility for GPU device detection and information retrieval
    #   - Access to /sys/bus/pci for driver binding status
    #   - Access to /proc/cmdline for kernel parameter verification
    #   - Access to configuration files in /etc/modprobe.d and /etc/modules
    #   - find, ls utilities for IOMMU group enumeration
    # Arguments:
    #   $1: hostname (optional) - hostname for config lookup (default: current hostname)
    #   $2: config_file (optional) - configuration file path (default: SITE_CONFIG_FILE)
    #   $3: pci0_id (optional) - explicit primary PCI ID parameter
    #   $4: pci1_id (optional) - explicit secondary PCI ID parameter
    #   $5: nvidia_driver_preference (optional) - preferred NVIDIA driver (default: nvidia)
    # Information Displayed:
    #   - IOMMU Groups: Detailed device listings for GPU-containing IOMMU groups
    #   - GPU Device Details: Complete lspci output with driver binding information
    #   - Kernel Modules: Currently loaded GPU and VFIO related modules
    #   - Configuration Status: Initial system preparation checklist (gpu_pt1, gpu_pt2)
    #   - NVIDIA Setup: Driver installation and Nouveau blacklist status
    #   - Persistent Configuration: GPU assignment and blacklist file status
    #   - Runtime Status: Current module loading and driver binding state
    #   - Overall Summary: High-level determination of GPU state (ATTACHED/DETACHED/MIXED)
    # Status Classifications:
    #   - ATTACHED: GPUs bound to host drivers (nvidia, nouveau, amdgpu, radeon)
    #   - DETACHED: GPUs bound to vfio-pci for VM passthrough
    #   - MIXED: Some GPUs attached to host, others detached for VMs
    #   - UNCLEAR: Ambiguous state requiring further investigation
    local hostname="${1:-$(hostname -s)}"
    local config_file="${2:-$CONFIG_FUN}"
    local pci0_id="$3"
    local pci1_id="$4"
    local nvidia_driver_preference="${5:-nvidia}"
    
    local function_name="${FUNCNAME[0]}"
    
    _gpu_init_colors
    
    # Load configuration if config file provided
    if [ -n "$config_file" ] && [ -f "$config_file" ]; then
        # shellcheck source=/dev/null
        source "$config_file"
        printf "INFO: Successfully sourced GPU configuration from %s\n" "$config_file" >/dev/null
    fi
    
    # Get detailed device information
    local detailed_info
    detailed_info=$(_gpu_get_detailed_device_info)
    
    local all_device_blocks_output=""
    local gpu_details_for_checklist=""
    local is_any_gpu_on_vfio=false
    local host_drivers_for_gpu=""
    local vga_devices_processed=false
    
    # Parse the detailed info output
    while IFS= read -r line; do
        case "$line" in
            DEVICE_BLOCKS:*)
                all_device_blocks_output="${line#DEVICE_BLOCKS:}"
                ;;
            GPU_CHECKLIST:*)
                gpu_details_for_checklist="${line#GPU_CHECKLIST:}"
                ;;
            VFIO_STATUS:*)
                is_any_gpu_on_vfio="${line#VFIO_STATUS:}"
                ;;
            HOST_DRIVERS:*)
                host_drivers_for_gpu="${line#HOST_DRIVERS:}"
                ;;
            DEVICES_PROCESSED:*)
                vga_devices_processed="${line#DEVICES_PROCESSED:}"
                ;;
        esac
    done <<< "$detailed_info"
    
    # Define configuration file paths
    local modules_file_path="/etc/modules"
    local blacklist_nouveau_conf_path="/etc/modprobe.d/blacklist-nouveau.conf"
    local vfio_conf_path="/etc/modprobe.d/vfio.conf"
    local passthrough_blacklist_conf_path="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"

    # --- IOMMU Groups (Details) ---
    echo -e "\n${CYAN}--- IOMMU Groups (Details) ---${NC}"
    if ! command -v find &>/dev/null || ! command -v ls &>/dev/null || ! command -v lspci &>/dev/null; then
        echo -e "${RED}Required commands (find, ls, lspci) not available to list IOMMU groups.${NC}"
    else
        _gpu_get_iommu_groups
    fi

    # --- GPU Device Details (lspci -nnk) ---
    echo -e "\n${CYAN}--- GPU Device Details (lspci -nnk) ---${NC}"
    if [ -n "$all_device_blocks_output" ]; then
        echo -e "${YELLOW}${all_device_blocks_output%\\n\\n}${NC}"
    elif [ "$vga_devices_processed" = "true" ]; then 
         echo -e "${YELLOW}No detailed GPU device blocks were formatted (e.g. lspci output was unusual or parsing issue).${NC}"
    else 
         echo -e "${YELLOW}No VGA compatible or 3D controllers found by lspci to display details.${NC}"
    fi

    # --- Loaded Kernel Modules (GPU related) ---
    echo -e "\n${CYAN}--- Loaded Kernel Modules (GPU related) ---${NC}"
    if lsmod | command grep -E "vfio|nvidia|nouveau|amdgpu|radeon" --color=never; then
        : 
    else
        echo -e "${YELLOW}No common GPU-related modules (vfio, nvidia, nouveau, amdgpu, radeon) loaded.${NC}"
    fi

    echo -e "\n${CYAN}--- GPU Passthrough Workflow & Runtime Status ---${NC}"
    echo -e "${CYAN}==================================================${NC}"

    # 1. Initial System Preparation (Configuration)
    echo -e "${MAGENTA}1. Initial System Preparation (Configuration):${NC}"
    
    local iommu_enabled_cmdline_status="$CROSS_MARK"
    if /usr/bin/grep -E -q 'iommu=pt|intel_iommu=on|amd_iommu=on' /proc/cmdline; then
        iommu_enabled_cmdline_status="$CHECK_MARK"
    fi
    echo -e "   ${iommu_enabled_cmdline_status} IOMMU Enabled in Kernel Command Line (gpu_pt1)"

    local vfio_modules_in_etc_modules_status="$CROSS_MARK"
    local vfio_ok=0; local vfio_iommu_type1_ok=0; local vfio_pci_ok=0
    if [ -f "$modules_file_path" ]; then
        if grep -qP "^\\s*vfio(\\s|$|#)" "$modules_file_path"; then vfio_ok=1; fi
        if grep -qP "^\\s*vfio_iommu_type1(\\s|$|#)" "$modules_file_path"; then vfio_iommu_type1_ok=1; fi
        if grep -qP "^\\s*vfio_pci(\\s|$|#)" "$modules_file_path"; then vfio_pci_ok=1; fi
    fi
    if [ "$vfio_ok" -eq 1 ] && [ "$vfio_iommu_type1_ok" -eq 1 ] && [ "$vfio_pci_ok" -eq 1 ]; then
        vfio_modules_in_etc_modules_status="$CHECK_MARK"
    fi
    echo -e "   ${vfio_modules_in_etc_modules_status} Core VFIO modules in $modules_file_path (gpu_pt2)"

    # 2. NVIDIA Host Driver Setup (Optional Configuration)
    echo -e "${MAGENTA}2. NVIDIA Host Driver Setup (Optional Configuration):${NC}"
    local nouveau_blacklisted_status="$QUESTION_MARK (Not found)"
    if [ -f "$blacklist_nouveau_conf_path" ]; then
        if grep -q "blacklist nouveau" "$blacklist_nouveau_conf_path"; then
            nouveau_blacklisted_status="$CHECK_MARK"
        else
            nouveau_blacklisted_status="$CROSS_MARK (Found, but 'blacklist nouveau' missing)"
        fi
    fi
    echo -e "   ${nouveau_blacklisted_status} Nouveau blacklisted for NVIDIA driver ($blacklist_nouveau_conf_path) (gpu-nds)"

    # 3. Persistent GPU Passthrough Configuration (gpu_pt3 enable status)
    echo -e "${MAGENTA}3. Persistent GPU Passthrough Configuration (gpu_pt3 enable):${NC}"
    local persistent_vfio_ids_status="$CROSS_MARK"
    if [ -f "$vfio_conf_path" ] && grep -qE "^options vfio-pci ids=" "$vfio_conf_path"; then
        persistent_vfio_ids_status="$CHECK_MARK"
    fi
    echo -e "   ${persistent_vfio_ids_status} GPUs assigned to vfio-pci at boot ($vfio_conf_path)"

    local persistent_blacklist_status="$CROSS_MARK"
    if [ -f "$passthrough_blacklist_conf_path" ] && grep -qE "blacklist (nvidia|nouveau|amdgpu|radeon)" "$passthrough_blacklist_conf_path"; then
        persistent_blacklist_status="$CHECK_MARK"
    fi
    echo -e "   ${persistent_blacklist_status} Host drivers blacklisted for selected GPUs ($passthrough_blacklist_conf_path)"

    # 4. Runtime Kernel Module & Driver Status
    echo -e "${MAGENTA}4. Runtime Kernel Module & Driver Status:${NC}"
    local vfio_pci_loaded_status="$CROSS_MARK"
    if lsmod | grep -q "vfio_pci"; then
        vfio_pci_loaded_status="$CHECK_MARK"
    fi
    echo -e "   ${vfio_pci_loaded_status} VFIO-PCI kernel module currently loaded"

    local other_gpu_modules_loaded="-"
    if lsmod | command grep -qE "nvidia|nouveau|amdgpu|radeon"; then
        other_gpu_modules_loaded="${GREEN}Host drivers (nvidia/nouveau/amdgpu/radeon) loaded${NC}"
    else
        other_gpu_modules_loaded="${YELLOW}No common host GPU drivers loaded${NC}"
    fi
    if lsmod | command grep -qE "vfio_iommu_type1|vfio"; then
        if [[ "$other_gpu_modules_loaded" == "-" || "$other_gpu_modules_loaded" == *"${YELLOW}No common host GPU drivers loaded${NC}"* ]]; then
            other_gpu_modules_loaded="${GREEN}VFIO modules (vfio/vfio_iommu_type1) loaded${NC}"
        else
            other_gpu_modules_loaded+=", ${GREEN}VFIO modules (vfio/vfio_iommu_type1) loaded${NC}"
        fi 
    fi
    echo -e "   - Other relevant modules: $other_gpu_modules_loaded"
    
    # 4.3 Kernel Drivers for GPU(s) Summary
    echo -e "   ${CYAN}--- Kernel Drivers for GPU(s) Summary ---${NC}"
    if [ -n "$gpu_details_for_checklist" ]; then
        echo -e "${gpu_details_for_checklist%\\n}"
    elif [ "$vga_devices_processed" = "true" ]; then 
         echo -e "     ${YELLOW}?${NC} No summary for GPU driver bindings could be generated."
    else 
         echo -e "     ${YELLOW}?${NC} No VGA/3D controllers found to summarize driver bindings."
    fi

    # Overall GPU State Determination
    echo -e "\n--- Overall GPU State Summary ---"
    if [ "$is_any_gpu_on_vfio" = "true" ]; then
        echo -e "[SUMMARY] GPU State: DETACHED (for VM use)"
        echo -e "  - At least one GPU device (VGA/3D controller) is bound to vfio-pci."
        if [ -n "$host_drivers_for_gpu" ]; then
             echo -e "  - Note: Other GPU devices might be ATTACHED to host (driver(s): $host_drivers_for_gpu). State could be MIXED."
        fi
    elif [ -n "$host_drivers_for_gpu" ]; then
        echo -e "[SUMMARY] GPU State: ATTACHED to host (driver(s): $host_drivers_for_gpu)"
        echo -e "  - GPU device(s) (VGA/3D controller) are using host driver(s): $host_drivers_for_gpu."
        echo -e "  - No VGA/3D controller found using vfio-pci."
    else
        if [ "$vga_devices_processed" = "true" ]; then
            echo -e "[SUMMARY] GPU State: UNCLEAR / INDETERMINATE"
            echo -e "  - A VGA/3D controller was detected, but it's not using 'vfio-pci' nor any other recognized host driver."
            echo -e "  - It might be unbound. Check 'GPU Device Details' and IOMMU details."
        else
            echo -e "[SUMMARY] GPU State: N/A (No VGA/3D GPU controllers detected)"
        fi
        echo -e "  - Review IOMMU groups and loaded kernel modules above."
    fi

    echo -e "\n--- GPU status check completed. (${function_name}) ---"
    echo -e "===================================================="
}
