#!/bin/bash

# Enhanced auxiliary logging fallback mechanism
if [[ -z "$RC_SOURCED" ]]; then
    # Calculate relative path from current script location
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    AUX_PATH="$SCRIPT_DIR/../gen/aux"
    source "$AUX_PATH"
fi

# ============================================================================
# GPU Driver and Passthrough Function Interactions - Technical Overview
#
# This section outlines crucial interactions between different GPU management
# functions, particularly concerning driver blacklisting, preferences, and
# dynamic vs. persistent configurations.
#
# Persistent Configuration (Primarily via `gpu_pt1`, `gpu_pt2`, `gpu_pt3`):
#   - These functions establish the boot-time default state for GPU passthrough.
#   - `gpu_pt3 enable`: Configures the system to assign specified GPUs to `vfio-pci`
#     at boot, creating persistent blacklists for host drivers for those GPUs.
#     This is the most robust way to dedicate GPUs for passthrough.
#   - `gpu_pt3 disable`: Reverts the persistent passthrough configuration, allowing
#     host drivers to claim GPUs at boot (defaulting to host usage).
#
# Dynamic Switching (`gpu_ptd`, `gpu_pta`):
#   - These functions allow runtime switching of GPU control between the host and
#     `vfio-pci` (for VMs) *without requiring a reboot*.
#   - They are designed to be as self-sufficient as possible, meaning they can
#     often work even if the persistent state set by `gpu_pt3` is for host usage
#     (i.e., after `gpu_pt3 disable` or if `gpu_pt3 enable` was never run).
#   - `gpu_ptd` (Detach): Attempts to dynamically prepare the system for passthrough
#     by loading necessary VFIO modules, unbinding host drivers, and binding `vfio-pci`.
#   - `gpu_pta` (Attach): Attempts to dynamically return GPU control to the host by
#     unbinding `vfio-pci`, loading necessary host drivers, and binding them.
#
# Blacklist Files:
#   - `/etc/modprobe.d/blacklist-nouveau.conf`:
#       - Created/Managed by: `gpu_nds` (NVIDIA Driver Setup).
#       - Purpose: Persistently blacklists `nouveau` if NVIDIA proprietary drivers
#         are installed for host use.
#
#   - `/etc/modprobe.d/zz-vfio-gpu-blacklist.conf`:
#       - Created/Managed by: `gpu_pt3 enable`.
#       - Removed by: `gpu_pt3 disable`.
#       - Purpose: Persistently blacklists host drivers (`nouveau`, `nvidia`,
#         `amdgpu`, `radeon`) for GPUs intended for passthrough at boot.
#         `gpu_ptd` does *not* create this file but attempts to achieve a similar
#         effect for the current session by unloading host drivers.
#
# Key Interactions & Considerations:
#
# 1. `gpu_nds` (NVIDIA Driver Setup):
#    - Installs NVIDIA proprietary drivers and can blacklist `nouveau` for host use.
#    This is independent of passthrough but affects which driver (`nvidia` or `nouveau`)
#    `gpu_pta` might try to load for an NVIDIA GPU if it's the host preference.
#
# 2. `gpu_pt1`, `gpu_pt2` (Initial Passthrough Setup):
#    - These set up essential prerequisites like IOMMU kernel parameters and ensure
#      VFIO kernel modules are listed in `/etc/modules`.
#    - While `gpu_ptd` attempts to load VFIO modules dynamically, having these base
#      prerequisites met (especially IOMMU in kernel cmdline) is highly recommended
#      for `gpu_ptd` to succeed reliably.
#
# 3. `gpu_pt3 enable` (Persistent Passthrough):
#    - Provides the most stable environment for passthrough by ensuring GPUs are
#      bound to `vfio-pci` from boot and host drivers are blacklisted.
#    - If you primarily use GPUs for passthrough, this is the recommended default state.
#
# 4. `gpu_pt3 disable` (Persistent Host GPU Usage):
#    - Sets the system to use GPUs with host drivers by default after boot.
#    - This is the state from which `gpu_ptd` can be used to dynamically switch
#      a GPU to `vfio-pci` for a VM session.
#
# 5. `gpu_ptd` (Dynamic Detach for VM):
#    - Will attempt to load `vfio`, `vfio_iommu_type1`, and `vfio_pci` if not present.
#    - Will attempt to unbind host drivers (e.g., `nvidia`, `nouveau`, `amdgpu`).
#    - Uses `driver_override` to bind `vfio-pci` to the target GPU(s).
#    - Its changes are not persistent across reboots.
#    - If IOMMU is not enabled in the kernel, `gpu_ptd` will warn but cannot fix it.
#
# 6. `gpu_pta` (Dynamic Attach to Host):
#    - Unbinds `vfio-pci`.
#    - Clears `driver_override`.
#    - Attempts to load the preferred/appropriate host driver (e.g., `nvidia`, `nouveau`, `amdgpu`)
#      if not already loaded, and binds it to the GPU.
#    - Considers hostname-specific `*_NVIDIA_DRIVER_PREFERENCE` from site config for NVIDIA GPUs.
#      Warns if `nouveau` is preferred but blacklisted by `gpu_nds`.
#
# General Workflow Examples:
#
#   Initial One-Time Setup (Common for both scenarios below):
#     1. Run `gpu_pt1`: Configures GRUB/EFI for IOMMU, installs related packages.
#        Reboot as prompted.
#     2. Run `gpu_pt2`: Adds necessary kernel modules (vfio, vfio_iommu_type1, vfio_pci)
#        to /etc/modules for them to be loaded at boot, updates initramfs.
#        Reboot as prompted.
#     3. (Optional) Run `gpu_nds`: If using an NVIDIA GPU and proprietary drivers are
#        desired for host usage (when GPU is attached to host). This blacklists nouveau
#        and installs the NVIDIA driver. Reboot if prompted.
#
#   Scenario A: GPU Primarily for Passthrough (Persistent Setup)
#     (After completing the Initial One-Time Setup)
#     a. Run `gpu_pt3 enable`: Assigns specified GPU(s) to `vfio-pci` at boot.
#        This involves creating /etc/modprobe.d/vfio.conf with the GPU IDs and
#        blacklisting their host drivers (e.g., nvidia, amgpu, nouveau) in
#        /etc/modprobe.d/zz-vfio-gpu-blacklist.conf. Reboot.
#        The GPU is now persistently configured for VM passthrough.
#     b. (Optional) If the GPU needs to be temporarily used by the host:
#        Run `gpu_pta` to attach it to host drivers.
#        Run `gpu_ptd` to detach it again for VM use. (No reboots needed for these).
#
#   Scenario B: GPU Primarily for Host, Dynamic Passthrough as Needed
#     (After completing the Initial One-Time Setup)
#     a. Ensure the system boots with the GPU attached to host drivers.
#        This is the default state if `gpu_pt3 enable` has not been run for the GPU,
#        or if `gpu_pt3 disable` was run to revert a persistent passthrough setup.
#        (If `gpu_nds` was run, the NVIDIA GPU will use proprietary drivers; otherwise,
#        it might use nouveau or amdgpu for AMD).
#     b. When passthrough is needed for a VM: Run `gpu_ptd` for the target GPU.
#        This dynamically attempts to:
#          - Ensure VFIO modules are loaded (they should be if `gpu_pt2` was run).
#          - Unbind the GPU from its host driver.
#          - Bind the GPU to `vfio-pci`.
#     c. After VM use: Run `gpu_pta` for the target GPU. This dynamically:
#          - Unbinds the GPU from `vfio-pci`.
#          - Rebinds the GPU to its appropriate host driver.
#     d. Note: If dynamic switching (`gpu_ptd`) faces issues (e.g., host drivers
#        re-asserting control, or instability), using `gpu_pt3 enable` (Scenario A)
#        for a persistent passthrough setup is generally more robust.
#
# ============================================================================

# Define directory and file variables
DIR_FUN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
FILE_FUN=$(basename "$BASH_SOURCE")
BASE_FUN="${FILE_FUN%.*}"
FILEPATH_FUN="${DIR_FUN}/${FILE_FUN}"
CONFIG_FUN="${SITE_CONFIG_FILE}"

# Color definitions for output formatting (conditional to avoid conflicts)
if [[ -z "$NC" ]]; then
    readonly NC=$(printf '\033[0m')               # No Color / Reset
    readonly RED=$(printf '\033[0;31m')           # Red
    readonly GREEN=$(printf '\033[0;32m')         # Green
    readonly YELLOW=$(printf '\033[0;33m')        # Yellow
    readonly BLUE=$(printf '\033[0;34m')          # Blue
    readonly MAGENTA=$(printf '\033[0;35m')       # Magenta
    readonly CYAN=$(printf '\033[0;36m')          # Cyan
    readonly WHITE=$(printf '\033[0;37m')         # White
fi

# Status markers for display (conditional to avoid conflicts)
if [[ -z "$CHECK_MARK" ]]; then
    readonly CHECK_MARK="✓"
    readonly CROSS_MARK="✗"
    readonly QUESTION_MARK="?"
fi

# Dynamically create variables based on the base name
eval "FILEPATH_${BASE_FUN}=\$FILEPATH_FUN"
eval "FILE_${BASE_FUN}=\$FILE_FUN"
eval "BASE_${BASE_FUN}=\$BASE_FUN"
eval "CONFIG_${BASE_FUN}=\$CONFIG_FUN"

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================



# Validate PCI ID format using regex pattern matching
# GPU Validate PCI ID - GVP
# <pci_id>
_gpu_validate_pci_id() {
    local pci_id="$1"
    aux_dbg "Validating PCI ID format" "component=gpu,operation=validate_pci,pci_id=${pci_id}"
    
    if [[ "$pci_id" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then
        aux_dbg "PCI ID validation successful" "component=gpu,operation=validate_pci,pci_id=${pci_id},result=valid"
        return 0
    else
        aux_warn "PCI ID validation failed" "component=gpu,operation=validate_pci,pci_id=${pci_id},result=invalid,expected_format=XX:XX.X"
        return 1
    fi
}

# Extract vendor and device IDs from lspci output for PCI device
# GPU Extract Vendor Device ID - GEV
# <pci_id>
_gpu_extract_vendor_device_id() {
    local pci_id="$1"
    aux_dbg "Extracting vendor and device ID" "component=gpu,operation=extract_vendor_device,pci_id=${pci_id}"
    
    local lspci_device_info
    lspci_device_info=$(lspci -s "$pci_id" -nn 2>/dev/null)
    
    if [ -n "$lspci_device_info" ]; then
        # Extract vendor:device ID from the [xxxx:yyyy] pattern
        local vendor_device_id=$(echo "$lspci_device_info" | grep -o '\[[^]]*:[^]]*\]' | tail -1 | sed 's/\[\|\]//g')
        
        if [ -n "$vendor_device_id" ]; then
            local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
            local device_id=$(echo "$vendor_device_id" | cut -d':' -f2)
            aux_dbg "Successfully extracted vendor and device ID" "component=gpu,operation=extract_vendor_device,pci_id=${pci_id},vendor_id=${vendor_id},device_id=${device_id}"
            echo "${vendor_id}:${device_id}"
            return 0
        else
            aux_warn "Could not parse vendor:device ID from lspci output" "component=gpu,operation=extract_vendor_device,pci_id=${pci_id},lspci_info=${lspci_device_info}"
        fi
    else
        aux_warn "No lspci output for PCI device" "component=gpu,operation=extract_vendor_device,pci_id=${pci_id}"
    fi
    return 1
}

# Get current driver bound to specified PCI device
# GPU Get Current Driver - GGC
# <pci_id>
_gpu_get_current_driver() {
    local pci_id="$1"
    local full_pci_id="0000:$pci_id"
    local driver_path="/sys/bus/pci/devices/$full_pci_id/driver"
    
    aux_dbg "Checking current driver binding" "component=gpu,operation=get_current_driver,pci_id=${pci_id},driver_path=${driver_path}"
    
    if [ -L "$driver_path" ]; then
        local driver_name=$(basename "$(readlink -f "$driver_path")")
        aux_dbg "Driver found" "component=gpu,operation=get_current_driver,pci_id=${pci_id},driver=${driver_name}"
        echo "$driver_name"
    else
        aux_dbg "No driver bound" "component=gpu,operation=get_current_driver,pci_id=${pci_id},driver=none"
        echo "none"
    fi
}

# Check if PCI device is GPU-related hardware type
# GPU Is GPU Device - GIG
# <pci_id>
_gpu_is_gpu_device() {
    local pci_id="$1"
    aux_dbg "Checking if device is GPU-related" "component=gpu,operation=is_gpu_device,pci_id=${pci_id}"
    
    if lspci -s "$pci_id" -nn 2>/dev/null | grep -qE "VGA compatible controller|3D controller|Audio device"; then
        aux_dbg "Device confirmed as GPU-related" "component=gpu,operation=is_gpu_device,pci_id=${pci_id},result=true"
        return 0
    else
        aux_dbg "Device is not GPU-related" "component=gpu,operation=is_gpu_device,pci_id=${pci_id},result=false"
        return 1
    fi
}

# Load GPU configuration file if available and readable
# GPU Load Config - GLC
# 
_gpu_load_config() {
    aux_info "Loading GPU configuration" "component=gpu,operation=load_config,config_file=${CONFIG_FUN}"
    
    if [ -n "$CONFIG_FUN" ] && [ -f "$CONFIG_FUN" ]; then
        # shellcheck source=/dev/null
        source "$CONFIG_FUN"
        aux_info "Successfully sourced PCI configuration" "component=gpu,operation=load_config,config_file=${CONFIG_FUN},status=success"
        return 0
    else
        aux_warn "PCI Configuration file not found or not set" "component=gpu,operation=load_config,config_file=${CONFIG_FUN},status=failed"
        return 1
    fi
}

# Get PCI IDs from hostname-based configuration variables
# GPU Get Config PCI IDs - GGP
# <hostname>
_gpu_get_config_pci_ids() {
    local hostname="$1"
    local -a pci_ids=()
    
    local pci0_var_name="${hostname}_NODE_PCI0"
    local pci1_var_name="${hostname}_NODE_PCI1"
    
    if [ -n "${!pci0_var_name}" ]; then
        local pci0_full_val=${!pci0_var_name}
        local short_pci0_val=${pci0_full_val#0000:}
        if _gpu_validate_pci_id "$short_pci0_val"; then
            pci_ids+=("$short_pci0_val")
        fi
    fi
    
    if [ -n "${!pci1_var_name}" ]; then
        local pci1_full_val=${!pci1_var_name}
        local short_pci1_val=${pci1_full_val#0000:}
        if _gpu_validate_pci_id "$short_pci1_val"; then
            pci_ids+=("$short_pci1_val")
        fi
    fi
    
    printf "%s\n" "${pci_ids[@]}"
}

# Find all GPU devices via lspci scan with optional driver filter
# GPU Find All GPUs - GFA
# <filter_driver>
_gpu_find_all_gpus() {
    local filter_driver="$1" # Optional: only return GPUs using specific driver
    local -a gpu_ids=()
    
    local lspci_output
    lspci_output=$(lspci -nnk)
    
    while IFS= read -r line; do
        if echo "$line" | grep -qE "VGA compatible controller|3D controller"; then
            local pci_slot_id=$(echo "$line" | awk '{print $1}')
            
            if [ -n "$filter_driver" ]; then
                local current_driver
                current_driver=$(_gpu_get_current_driver "$pci_slot_id")
                if [ "$current_driver" = "$filter_driver" ]; then
                    gpu_ids+=("$pci_slot_id")
                fi
            else
                gpu_ids+=("$pci_slot_id")
            fi
        fi
    done <<< "$lspci_output"
    
    printf "%s\n" "${gpu_ids[@]}"
}

# Get target GPUs for processing with comprehensive selection logic
# GPU Get Target GPUs - GGT
# <gpu_id_arg> <hostname> <filter_driver>
_gpu_get_target_gpus() {
    local gpu_id_arg="$1"
    local hostname="$2"
    local filter_driver="$3" # Optional: vfio-pci, nvidia, etc.
    local -a gpus_to_process=()
    
    if [ -n "$gpu_id_arg" ]; then
        # Specific GPU ID provided
        if ! _gpu_validate_pci_id "$gpu_id_arg"; then
            aux_err "Invalid PCI ID format provided" "pci_id=${gpu_id_arg},expected_format=XX:XX.X"
            return 1
        fi
        
        if _gpu_is_gpu_device "$gpu_id_arg"; then
            if [ -n "$filter_driver" ]; then
                local current_driver
                current_driver=$(_gpu_get_current_driver "$gpu_id_arg")
                if [ "$current_driver" = "$filter_driver" ]; then
                    gpus_to_process+=("$gpu_id_arg")
                fi
            else
                gpus_to_process+=("$gpu_id_arg")
            fi
        else
            aux_err "Specified PCI ID is not a VGA/3D/Audio controller or does not exist" "pci_id=${gpu_id_arg},function=standard"
            return 1
        fi
    else
        # Try to get from configuration
        local -a config_ids
        readarray -t config_ids < <(_gpu_get_config_pci_ids "$hostname")
        
        # Process config IDs, but only add non-empty ones
        for pci_id in "${config_ids[@]}"; do
            if [ -n "$pci_id" ]; then
                if [ -n "$filter_driver" ]; then
                    local current_driver
                    current_driver=$(_gpu_get_current_driver "$pci_id")
                    if [ "$current_driver" = "$filter_driver" ]; then
                        gpus_to_process+=("$pci_id")
                    fi
                else
                    gpus_to_process+=("$pci_id")
                fi
            fi
        done
        
        # Fallback to lspci scan if no config IDs found
        if [ ${#gpus_to_process[@]} -eq 0 ]; then
            local -a fallback_ids
            readarray -t fallback_ids < <(_gpu_find_all_gpus "$filter_driver")
            gpus_to_process+=("${fallback_ids[@]}")
        fi
    fi
    
    # Remove duplicates
    if [ ${#gpus_to_process[@]} -gt 0 ]; then
        readarray -t gpus_to_process < <(printf "%s\n" "${gpus_to_process[@]}" | sort -u)
    fi
    
    printf "%s\n" "${gpus_to_process[@]}"
}

# Ensure required VFIO kernel modules are loaded for GPU passthrough
# GPU Ensure VFIO Modules - GEV
# 
_gpu_ensure_vfio_modules() {
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=ensure_vfio_modules"
    local modules=(vfio vfio_iommu_type1 vfio_pci)
    
    for module in "${modules[@]}"; do
        if lsmod | grep -q "^${module}"; then
            aux_info "Module already loaded" "component=gpu,operation=ensure_vfio_modules,module=${module},status=already_loaded"
        else
            aux_info "Attempting to load module" "component=gpu,operation=ensure_vfio_modules,module=${module},status=loading"
            if modprobe "$module"; then
                aux_info "Module loaded successfully" "component=gpu,operation=ensure_vfio_modules,module=${module},status=loaded"
            else
                aux_err "Failed to load module" "component=gpu,operation=ensure_vfio_modules,module=${module},status=failed,error=modprobe_failed"
                if [ "$module" = "vfio_pci" ]; then
                    aux_err "Critical module vfio_pci failed to load, exiting" "component=gpu,operation=ensure_vfio_modules,module=${module},status=critical_failure"
                    return 1
                fi
            fi
        fi
    done
    aux_info "VFIO modules check completed" "component=gpu,operation=ensure_vfio_modules,status=complete"
    return 0
}

# Unbind PCI device from its current driver safely
# GPU Unbind Device - GUD
# <pci_id>
_gpu_unbind_device() {
    local pci_id="$1"
    local full_pci_id="0000:$pci_id"
    local driver_path="/sys/bus/pci/devices/$full_pci_id/driver"
    
    aux_info "Attempting to unbind device" "component=gpu,operation=unbind_device,pci_id=${pci_id},driver_path=${driver_path}"
    
    if [ -L "$driver_path" ]; then
        local current_driver
        current_driver=$(basename "$(readlink -f "$driver_path")")
        aux_info "Unbinding GPU from current driver" "component=gpu,operation=unbind_device,pci_id=${pci_id},current_driver=${current_driver}"
        
        if echo "$full_pci_id" > "$driver_path/unbind"; then
            aux_info "Successfully unbound GPU" "component=gpu,operation=unbind_device,pci_id=${pci_id},previous_driver=${current_driver},status=success"
            return 0
        else
            aux_warn "Failed to unbind GPU" "component=gpu,operation=unbind_device,pci_id=${pci_id},current_driver=${current_driver},status=failed"
            return 1
        fi
    else
        aux_info "GPU not bound to any driver" "component=gpu,operation=unbind_device,pci_id=${pci_id},status=not_bound"
        return 0
    fi
}

# Bind PCI device to specified driver with vendor/device ID setup
# GPU Bind Device - GBD
# <pci_id> <target_driver>
_gpu_bind_device() {
    local pci_id="$1"
    local target_driver="$2"
    local full_pci_id="0000:$pci_id"
    local driver_bind_path="/sys/bus/pci/drivers/$target_driver/bind"
    
    aux_info "Attempting to bind device to driver" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver}"
    
    # Check if driver directory exists
    if [ ! -d "/sys/bus/pci/drivers/$target_driver" ]; then
        aux_err "PCI driver directory does not exist" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},error=driver_dir_missing"
        return 1
    fi
    
    aux_info "Attempting to bind GPU to driver" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},step=bind_attempt"
    
    if echo "$full_pci_id" > "$driver_bind_path"; then
        aux_info "Bind operation initiated" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},status=initiated"
        sleep 1
        
        # Verify binding
        local current_driver
        current_driver=$(_gpu_get_current_driver "$pci_id")
        if [ "$current_driver" = "$target_driver" ]; then
            aux_info "GPU successfully bound to target driver" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},current_driver=${current_driver},status=success"
            return 0
        else
            aux_warn "GPU may not have bound correctly" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},current_driver=${current_driver},status=verification_failed"
            return 1
        fi
    else
        aux_err "Failed to bind GPU to driver" "component=gpu,operation=bind_device,pci_id=${pci_id},target_driver=${target_driver},status=failed,error=bind_operation_failed"
        return 1
    fi
}

# Determine appropriate host driver for GPU based on vendor ID
# GPU Get Host Driver - GGH
# <pci_id> <hostname>
_gpu_get_host_driver() {
    local pci_id="$1"
    local hostname="$2"
    
    local vendor_device_id
    vendor_device_id=$(_gpu_extract_vendor_device_id "$pci_id")
    
    if [ $? -ne 0 ] || [ -z "$vendor_device_id" ]; then
        aux_warn "Could not determine vendor ID for GPU device" "component=gpu,operation=get_host_driver,pci_id=${pci_id}"
        return 1
    fi
    
    local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
    
    case "$vendor_id" in
        "10de") # NVIDIA
            local nvidia_driver_pref_var="${hostname}_NVIDIA_DRIVER_PREFERENCE"
            local preferred_nvidia_driver
            if [ -n "${!nvidia_driver_pref_var}" ]; then
                preferred_nvidia_driver="${!nvidia_driver_pref_var}"
            else
                preferred_nvidia_driver="nvidia"
            fi
            echo "$preferred_nvidia_driver"
            ;;
        "1002") # AMD
            echo "amdgpu"
            ;;
        *)
            aux_warn "Unknown GPU vendor ID for device" "component=gpu,operation=get_host_driver,vendor_id=${vendor_id},pci_id=${pci_id}"
            return 1
            ;;
    esac
}

# Determine appropriate host driver for GPU with explicit parameters
# GPU Get Host Driver Parameterized - GHP
# <pci_id> <hostname> <nvidia_driver_preference>
_gpu_get_host_driver_parameterized() {
    local pci_id="$1"
    local hostname="$2"
    local nvidia_driver_preference="${3:-nvidia}"
    
    local vendor_device_id
    vendor_device_id=$(_gpu_extract_vendor_device_id "$pci_id")
    
    if [ $? -ne 0 ] || [ -z "$vendor_device_id" ]; then
        aux_warn "Could not determine vendor ID for GPU device" "component=gpu,operation=get_host_driver_parameterized,pci_id=${pci_id}"
        return 1
    fi
    
    local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
    
    case "$vendor_id" in
        "10de") # NVIDIA
            echo "$nvidia_driver_preference"
            ;;
        "1002") # AMD
            echo "amdgpu"
            ;;
        *)
            aux_warn "Unknown GPU vendor ID for device" "component=gpu,operation=get_host_driver_parameterized,vendor_id=${vendor_id},pci_id=${pci_id}"
            return 1
            ;;
    esac
}

# Get PCI IDs from explicit parameters without hostname lookup
# GPU Get Config PCI IDs Parameterized - GCP
# <pci0_id> <pci1_id>
_gpu_get_config_pci_ids_parameterized() {
    local pci0_id="$1"
    local pci1_id="$2"
    local -a pci_ids=()
    
    if [ -n "$pci0_id" ]; then
        local short_pci0_val=${pci0_id#0000:}
        if _gpu_validate_pci_id "$short_pci0_val"; then
            pci_ids+=("$short_pci0_val")
        fi
    fi
    
    if [ -n "$pci1_id" ]; then
        local short_pci1_val=${pci1_id#0000:}
        if _gpu_validate_pci_id "$short_pci1_val"; then
            pci_ids+=("$short_pci1_val")
        fi
    fi
    
    printf "%s\n" "${pci_ids[@]}"
}

# Get target GPUs for processing with explicit parameters
# GPU Get Target GPUs Parameterized - GTP
# <gpu_id_arg> <hostname> <filter_driver> <pci0_id> <pci1_id>
_gpu_get_target_gpus_parameterized() {
    local gpu_id_arg="$1"
    local hostname="$2"
    local filter_driver="$3" # Optional: vfio-pci, nvidia, etc.
    local pci0_id="$4"
    local pci1_id="$5"
    local -a gpus_to_process=()
    
    if [ -n "$gpu_id_arg" ]; then
        # Specific GPU ID provided
        if ! _gpu_validate_pci_id "$gpu_id_arg"; then
            aux_err "Invalid PCI ID format provided" "pci_id=${gpu_id_arg},expected_format=XX:XX.X,function=parameterized"
            return 1
        fi
        
        if _gpu_is_gpu_device "$gpu_id_arg"; then
            if [ -n "$filter_driver" ]; then
                local current_driver
                current_driver=$(_gpu_get_current_driver "$gpu_id_arg")
                if [ "$current_driver" = "$filter_driver" ]; then
                    gpus_to_process+=("$gpu_id_arg")
                fi
            else
                gpus_to_process+=("$gpu_id_arg")
            fi
        else
            aux_err "Specified PCI ID is not a VGA/3D/Audio controller or does not exist" "pci_id=${gpu_id_arg},function=parameterized"
            return 1
        fi
    else
        # Try to get from configuration
        local -a config_ids
        readarray -t config_ids < <(_gpu_get_config_pci_ids_parameterized "$pci0_id" "$pci1_id")
        
        # Process config IDs, but only add non-empty ones
        for pci_id in "${config_ids[@]}"; do
            if [ -n "$pci_id" ]; then
                if [ -n "$filter_driver" ]; then
                    local current_driver
                    current_driver=$(_gpu_get_current_driver "$pci_id")
                    if [ "$current_driver" = "$filter_driver" ]; then
                        gpus_to_process+=("$pci_id")
                    fi
                else
                    gpus_to_process+=("$pci_id")
                fi
            fi
        done
        
        # Fallback to lspci scan if no config IDs found
        if [ ${#gpus_to_process[@]} -eq 0 ]; then
            local -a fallback_ids
            readarray -t fallback_ids < <(_gpu_find_all_gpus "$filter_driver")
            gpus_to_process+=("${fallback_ids[@]}")
        fi
    fi
    
    # Remove duplicates
    if [ ${#gpus_to_process[@]} -gt 0 ]; then
        readarray -t gpus_to_process < <(printf "%s\n" "${gpus_to_process[@]}" | sort -u)
    fi
    
    printf "%s\n" "${gpus_to_process[@]}"
}

# Get IOMMU groups containing GPU devices for passthrough validation
# GPU Get IOMMU Groups - GIG
# 
_gpu_get_iommu_groups() {
    local -a gpu_pci_bus_ids=()
    while IFS= read -r line; do
        gpu_pci_bus_ids+=("$(echo "$line" | awk '{print $1}')")
    done < <(lspci -nn | grep -iE "VGA compatible controller|3D controller")

    if [ ${#gpu_pci_bus_ids[@]} -eq 0 ]; then
        aux_info "No VGA/3D controllers found to filter IOMMU groups" "component=gpu,operation=get_iommu_groups,gpu_count=0"
        return 1
    fi

    local overall_iommu_groups_found=false
    local gpu_related_group_displayed=false

    for iommu_group_dir in $(find /sys/kernel/iommu_groups/ -mindepth 1 -maxdepth 1 -type d 2>/dev/null | sort -V); do
        overall_iommu_groups_found=true
        local group_id=$(basename "$iommu_group_dir")
        local devices_in_group_output=""
        local this_group_contains_gpu=false

        for device_symlink_name in $(ls -1 "$iommu_group_dir"/devices/ 2>/dev/null | sort -V); do
            local device_pci_bus_id_full="$device_symlink_name"
            local device_pci_bus_id_short="${device_symlink_name#0000:}"
            local device_info
            device_info=$(lspci -nns "$device_pci_bus_id_full" 2>/dev/null)
            
            if [ -n "$device_info" ]; then
                devices_in_group_output+="  ${device_info}\n"
                for gpu_id in "${gpu_pci_bus_ids[@]}"; do
                    if [[ "$device_pci_bus_id_short" == "$gpu_id" ]]; then
                        this_group_contains_gpu=true
                        break 
                    fi
                done
            else
                devices_in_group_output+="  Error reading device info for $device_pci_bus_id_full\n"
            fi
        done

        if $this_group_contains_gpu; then
            echo -e "\nIOMMU Group ${group_id}:"
            echo -e "${devices_in_group_output%\n}" 
            gpu_related_group_displayed=true
        fi
    done

    if ! $overall_iommu_groups_found; then
        aux_warn "No IOMMU groups found by find command" "component=gpu,operation=get_iommu_groups,status=no_groups_found"
        return 1
    elif ! $gpu_related_group_displayed; then
        aux_warn "No IOMMU groups found containing identified GPUs" "component=gpu,operation=get_iommu_groups,gpu_pci_ids=${gpu_pci_bus_ids[*]}"
        return 1
    fi
}

# Get detailed GPU device information including driver status and bindings
# GPU Get Detailed Device Info - GDI
# 
_gpu_get_detailed_device_info() {
    local temp_lspci_gpu_blocks="/tmp/gpu_pts_blocks.$$"
    lspci -nnk | grep -A4 -iE "VGA compatible controller|3D controller" > "$temp_lspci_gpu_blocks"
    
    local all_device_blocks_output=""
    local gpu_details_for_checklist=""
    local is_any_gpu_on_vfio=false
    local host_drivers_for_gpu=""
    local vga_devices_processed=false
    
    if [ -s "$temp_lspci_gpu_blocks" ]; then
        local current_gpu_block_content=""

        while IFS= read -r line_from_grep; do
            if echo "$line_from_grep" | grep -qE "VGA compatible controller|3D controller"; then
                # Process previous block if it exists and was a GPU block
                if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
                    all_device_blocks_output+="Device Block:\n$current_gpu_block_content\n\n"
                    vga_devices_processed=true
                    
                    local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')
                    local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
                    local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
                    local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
                    local device_name_short=$(echo "$device_name_raw" | sed 's/ \[.*\]//g' | awk '{print $1, $2}')
                    local device_name_for_checklist="$device_vendor_name $device_name_short"
                    device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
                    if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

                    if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                        is_any_gpu_on_vfio=true
                        gpu_details_for_checklist+="     ✓ GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\n"
                    elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                        local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                        if [ -n "$driver_name" ]; then
                            if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                            gpu_details_for_checklist+="     * GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\n"
                        else
                            gpu_details_for_checklist+="     ✗ GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\n"
                        fi
                    else
                        gpu_details_for_checklist+="     ✗ GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\n"
                    fi
                fi
                current_gpu_block_content="$line_from_grep"
            elif [[ "$line_from_grep" == "--" ]]; then
                current_gpu_block_content=""
            elif [ -n "$current_gpu_block_content" ]; then
                current_gpu_block_content="$current_gpu_block_content"$'\n'"$line_from_grep"
            fi
        done < "$temp_lspci_gpu_blocks"
        
        # Process the last block
        if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
            all_device_blocks_output+="Device Block:\n$current_gpu_block_content\n\n"
            vga_devices_processed=true
            
            local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')
            local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
            local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
            local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
            local device_name_short=$(echo "$device_name_raw" | sed 's/ \[.*\]//g' | awk '{print $1, $2}')
            local device_name_for_checklist="$device_vendor_name $device_name_short"
            device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
            if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

            if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                is_any_gpu_on_vfio=true
                gpu_details_for_checklist+="     ✓ GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\n"
            elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                if [ -n "$driver_name" ]; then
                    if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                    gpu_details_for_checklist+="     * GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\n"
                else
                    gpu_details_for_checklist+="     ✗ GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\n"
                fi
            else
                gpu_details_for_checklist+="     ✗ GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\n"
            fi
        fi
    fi
    
    rm -f "$temp_lspci_gpu_blocks"
    
    # Output results as delimited strings for parsing
    echo "DEVICE_BLOCKS:$all_device_blocks_output"
    echo "GPU_CHECKLIST:$gpu_details_for_checklist"
    echo "VFIO_STATUS:$is_any_gpu_on_vfio"
    echo "HOST_DRIVERS:$host_drivers_for_gpu"
    echo "DEVICES_PROCESSED:$vga_devices_processed"
}

# Helper function to create continuation script for multi-step reboot process
# GPU Create Continuation Script - GCC
# <action> <no_reboot>
_gpu_create_continuation_script() {
    local action="$1"
    local no_reboot="$2"
    local service_path="/etc/systemd/system/gpu-ptp-continue.service"
    local script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    local gpu_script_path="$script_dir/gpu"
    local working_dir="$PWD"
    
    aux_info "Creating continuation mechanism for GPU passthrough setup" "component=gpu,operation=create_continuation,service_path=${service_path}"
    
    # Store working directory for continuation
    echo "$working_dir" > "./.tmp/gpu_ptp_workdir"
    
    # Create systemd service for automatic continuation
    cat > "$service_path" << EOF
[Unit]
Description=GPU Passthrough Setup Continuation
After=multi-user.target
Wants=multi-user.target

[Service]
Type=oneshot
ExecStart=/bin/bash -c 'WORKDIR="\$(cat "$working_dir/.tmp/gpu_ptp_workdir" 2>/dev/null || echo "$working_dir")"; cd "\$WORKDIR" && source "$gpu_script_path" && gpu_ptp all $action $([ "$no_reboot" = "true" ] && echo "--no-reboot")'
RemainAfterExit=yes
StandardOutput=journal
StandardError=journal
User=root

[Install]
WantedBy=multi-user.target
EOF

    # Enable the service
    systemctl enable gpu-ptp-continue.service
    aux_info "Created systemd service for continuation" "component=gpu,operation=create_continuation,service=gpu-ptp-continue.service,working_dir=${working_dir}"
}

# Helper function to cleanup continuation mechanism
# GPU Cleanup Continuation Script - GCL
# 
_gpu_cleanup_continuation_script() {
    local service_path="/etc/systemd/system/gpu-ptp-continue.service"
    local progress_file="./.tmp/gpu_ptp_progress"
    local workdir_file="./.tmp/gpu_ptp_workdir"
    
    aux_info "Cleaning up GPU passthrough continuation mechanism" "component=gpu,operation=cleanup_continuation"
    
    # Disable and remove systemd service
    if [ -f "$service_path" ]; then
        systemctl disable gpu-ptp-continue.service 2>/dev/null || true
        rm -f "$service_path"
        systemctl daemon-reload
        aux_info "Removed continuation service" "component=gpu,operation=cleanup_continuation,service=gpu-ptp-continue.service"
    fi
    
    # Remove progress and working directory files
    rm -f "$progress_file" "$workdir_file"
    aux_info "Removed progress tracking files" "component=gpu,operation=cleanup_continuation,files=${progress_file},${workdir_file}"
}

# ============================================================================
# MAIN FUNCTIONS (Refactored)
# ============================================================================

# Shows function overview with optional filtering
# overview functions
# [function_name_filter]
gpu_fun() {
    # Technical Description:
    #   Displays formatted list of all functions in the GPU module with detailed documentation.
    #   Uses ana_laf utility to parse function definitions and extract comprehensive documentation
    #   including function names, descriptions, mnemonics, and parameter requirements.
    #   Provides optional filtering by function name pattern for focused analysis.
    #   Shows both helper functions (prefixed with _gpu_) and main user-facing functions.
    # Dependencies:
    #   - ana_laf function from auxiliary library for function analysis
    #   - Read access to GPU module file for parsing function definitions
    #   - grep for pattern matching if filtering is used
    # Arguments:
    #   $1: function_name_filter (optional) - pattern to filter function names
    #   --help/-h: Display technical usage information
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    # Optional parameter validation
    if [ $# -gt 1 ]; then
        aux_err "Too many parameters provided"
        aux_use
        return 1
    fi
    
    if [ $# -eq 1 ] && [ -z "$1" ]; then
        aux_err "Filter parameter cannot be empty"
        aux_use
        return 1
    fi
    
    aux_dbg "Starting function analysis" "filter=${1:-none}"
    ana_laf "$FILEPATH_gpu" "$@"
}

# Shows configuration variables overview
# overview variables
# -x (execute)
gpu_var() {
    # Technical Description:
    #   Displays comprehensive overview of configuration variables specific to GPU module
    #   Uses ana_acu utility to scan and format variable definitions from configuration files
    #   Shows variable names, current values, and usage context within GPU management
    #   Provides organized view of GPU-related environment and configuration settings
    #   including hostname-specific PCI device mappings and driver preferences.
    # Dependencies:
    #   - ana_acu function from auxiliary library for configuration analysis
    #   - Read access to configuration files for variable extraction
    #   - CONFIG_gpu environment variable must be set for proper operation
    # Arguments:
    #   $1: -x - explicit execution flag required for consistency with module pattern
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 1 ] || [ "$1" != "-x" ]; then
        aux_err "Invalid parameters - requires -x flag"
        aux_use
        return 1
    fi
    
    if ! aux_chk "variable" "CONFIG_gpu"; then
        aux_err "CONFIG_gpu environment variable not set"
        return 2
    fi
    
    aux_dbg "Analyzing GPU configuration variables" "config_file=${CONFIG_gpu}"
    ana_acu -o "$CONFIG_gpu" "$DIR_FUN/.."
}

# Downloads and installs NVIDIA drivers, blacklisting Nouveau drivers first
# nvidia driver setup
# [driver_version]
gpu_nds() {
    # Technical Description:
    #   Comprehensive NVIDIA proprietary driver installation process that handles
    #   the complete workflow from Nouveau blacklisting to driver compilation and installation.
    #   Automatically downloads specified NVIDIA driver version, prepares system by
    #   blacklisting conflicting Nouveau drivers, installs necessary build dependencies,
    #   and compiles drivers with DKMS support for kernel module persistence.
    #   Creates persistent blacklist configuration to prevent Nouveau from loading.
    # Dependencies:
    #   - Internet connectivity for driver download from NVIDIA servers
    #   - Root privileges for system modification (blacklists, driver installation)
    #   - Build tools: dkms, build-essential, kernel headers
    #   - update-initramfs for initrd regeneration
    #   - wget command for downloading drivers
    # Arguments:
    #   $1: driver_version (optional) - NVIDIA driver version (default: 550.142)
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    # Optional parameter validation
    if [ $# -gt 1 ]; then
        aux_err "Too many parameters provided"
        aux_use
        return 1
    fi
    
    if [ $# -eq 1 ] && [ -z "$1" ]; then
        aux_err "Driver version parameter cannot be empty"
        aux_use
        return 1
    fi
    
    # Check system dependencies
    if ! aux_chk "command" "wget"; then
        aux_err "wget command not found - required for driver download"
        return 127
    fi
    
    if ! aux_chk "command" "update-initramfs"; then
        aux_err "update-initramfs command not found - required for system configuration"
        return 127
    fi
    
    local drv_ver="${1:-550.142}"
    local url="https://us.download.nvidia.com/XFree86/Linux-x86_64/${drv_ver}/NVIDIA-Linux-x86_64-${drv_ver}.run"
    local installer="NVIDIA-Linux-x86_64-${drv_ver}.run"

    aux_info "Starting NVIDIA driver installation" "driver_version=${drv_ver}"
    
    aux_info "Blacklisting Nouveau driver"
    aux_info "Creating Nouveau driver blacklist configuration" "component=gpu,operation=nvidia_driver_setup,step=blacklist_nouveau"
    if aux_cmd "tee" "/etc/modprobe.d/blacklist-nouveau.conf" <<EOF
blacklist nouveau
options nouveau modeset=0
EOF
    then
        aux_info "Nouveau blacklist created successfully"
    else
        aux_err "Failed to create Nouveau blacklist"
        return 2
    fi
    
    aux_info "Updating initramfs"
    if aux_cmd "update-initramfs" "-u"; then
        aux_info "Initramfs updated successfully"
    else
        aux_err "Failed to update initramfs"
        return 2
    fi

    aux_info "Installing prerequisites"
    aux_info "Installing build prerequisites for NVIDIA driver compilation" "component=gpu,operation=nvidia_driver_setup,step=prerequisites"
    if aux_cmd "apt" "update" && aux_cmd "apt" "install" "-y" "dkms" "build-essential" "pve-headers-$(uname -r)"; then
        aux_info "Prerequisites installed successfully"
    else
        aux_err "Failed to install prerequisites"
        return 2
    fi

    aux_info "Downloading NVIDIA driver" "component=gpu,operation=nvidia_driver_setup,step=download,version=${drv_ver}"
    if aux_cmd "wget" "-q" "${url}" "-O" "${installer}"; then
        aux_info "Driver download successful" "file=${installer}"
        chmod +x "${installer}"
    else
        aux_err "Driver download failed" "url=${url}"
        return 2
    fi

    aux_info "Installing NVIDIA driver with DKMS" "component=gpu,operation=nvidia_driver_setup,step=install,installer=${installer}"
    if aux_cmd "./${installer}" "--dkms" "--silent"; then
        aux_info "NVIDIA driver installation completed" "version=${drv_ver}"
    else
        aux_err "NVIDIA driver installation failed"
        return 2
    fi

    aux_info "NVIDIA driver installation finalized" "component=gpu,operation=nvidia_driver_setup,step=finalization,reboot_recommended=true"
    local reboot_choice=$(aux_ask "Reboot now?" "N" "yes_no")
    if [[ "$reboot_choice" =~ ^[Yy]$ ]]; then
        aux_info "System reboot initiated by user"
        reboot
    else
        aux_info "System reboot deferred by user"
        aux_info "Manual reboot command available" "component=gpu,operation=nvidia_driver_setup,command=reboot"
    fi

    aux_info "Post-installation verification command available" "component=gpu,operation=nvidia_driver_setup,command=nvidia-smi"
    aux_info "NVIDIA driver setup completed" "verification_command=nvidia-smi"
}

# Unified GPU passthrough preparation function
# passthrough preparation
# <step> [action] [--no-reboot]
gpu_ptp() {
    # Technical Description:
    #   Unified GPU passthrough preparation function that consolidates the functionality
    #   of gpu_pt1, gpu_pt2, and gpu_pt3 into a single parameterized function. Handles
    #   all phases of GPU passthrough setup from IOMMU kernel parameter configuration
    #   through VFIO module setup to final device binding and driver blacklisting.
    #   Supports granular control over each step while maintaining the complete workflow.
    # Dependencies:
    #   - Root privileges for system configuration modification
    #   - EFI boot environment support (step 1)
    #   - GRUB configuration tools (step 1)
    #   - VFIO kernel modules availability (step 2)
    #   - PCI device detection utilities (step 3)
    # Arguments:
    #   $1: step - "1" (IOMMU/GRUB), "2" (VFIO modules), "3" (device binding), "all" (complete setup)
    #   $2: action (optional) - For step 3: "enable" or "disable" (default: enable)
    #   $3: --no-reboot (optional) - Skip automatic reboot after configuration
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -lt 1 ] || [ $# -gt 3 ]; then
        aux_err "Invalid number of parameters (1-3 required)"
        aux_use
        return 1
    fi
    
    local step="$1"
    local action="${2:-enable}"
    local no_reboot=false
    
    # Check for --no-reboot flag
    if [ "$2" = "--no-reboot" ] || [ "$3" = "--no-reboot" ]; then
        no_reboot=true
        # Adjust action if --no-reboot was passed as second parameter
        if [ "$2" = "--no-reboot" ]; then
            action="enable"
        fi
    fi
    
    # Validate step parameter
    if [[ ! "$step" =~ ^(1|2|3|all)$ ]]; then
        aux_err "Invalid step: $step. Must be '1', '2', '3', or 'all'"
        aux_use
        return 1
    fi
    
    # Validate action for step 3
    if [ "$step" = "3" ] || [ "$step" = "all" ]; then
        if [ "$action" != "enable" ] && [ "$action" != "disable" ] && [ "$action" != "--no-reboot" ]; then
            aux_err "Invalid action for step 3: $action. Must be 'enable' or 'disable'"
            aux_use
            return 1
        fi
        # Reset action to enable if it was --no-reboot
        if [ "$action" = "--no-reboot" ]; then
            action="enable"
        fi
    fi
    
    local function_name="${FUNCNAME[0]}"
    aux_info "Starting GPU passthrough preparation" "step=${step},action=${action},no_reboot=${no_reboot}"
    
    # Handle "all" step with proper sequencing and reboot management
    if [ "$step" = "all" ]; then
        # Ensure .tmp directory exists
        mkdir -p ./.tmp
        
        # Use a fixed progress file name (not process-specific)
        local progress_file="./.tmp/gpu_ptp_progress"
        local current_step=1
        
        # Check if we're resuming from a previous step
        if [ -f "$progress_file" ]; then
            current_step=$(cat "$progress_file")
            aux_info "Resuming GPU passthrough setup from step $current_step" "component=gpu,operation=passthrough_preparation,resume_step=${current_step}"
        else
            aux_info "Starting complete GPU passthrough setup (all steps)" "component=gpu,operation=passthrough_preparation,sequence=all"
            # Create startup script for automatic continuation after reboot
            _gpu_create_continuation_script "$action" "$no_reboot"
        fi
        
        # Execute steps sequentially with proper reboot handling
        case "$current_step" in
            1)
                aux_info "Executing step 1 of complete setup: IOMMU/GRUB configuration" "component=gpu,operation=passthrough_preparation,sequence=all,step=1"
                
                # Step 1 dependency checks
                if ! aux_chk "command" "sed"; then
                    aux_err "sed command not found - required for GRUB configuration"
                    return 127
                fi
                
                if ! aux_chk "command" "update-grub"; then
                    aux_err "update-grub command not found - required for GRUB configuration"
                    return 127
                fi
                
                if ! aux_chk "command" "apt"; then
                    aux_err "apt command not found - required for package installation"
                    return 127
                fi
                
                if ! aux_chk "command" "efibootmgr"; then
                    aux_err "efibootmgr command not found - required for EFI boot management"
                    return 127
                fi
                
                aux_business "GPU passthrough phase 1 initiated" "component=gpu,operation=passthrough_preparation,phase=iommu_setup,step=grub_configuration"
                
                aux_info "Checking EFI boot manager status" "component=gpu,operation=passthrough_preparation,step=efi_check"
                efibootmgr -v

                # Edit GRUB configuration
                aux_info "Modifying GRUB configuration for IOMMU" "component=gpu,operation=passthrough_preparation,step=grub_config,parameter=iommu=pt"
                sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="quiet"/GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt"/' /etc/default/grub
                
                aux_perf "Updating GRUB configuration" "component=gpu,operation=passthrough_preparation,step=grub_update"
                update-grub
                update-grub2

                # Install grub-efi-amd64
                aux_info "Installing GRUB EFI package" "component=gpu,operation=passthrough_preparation,step=grub_efi_install,package=grub-efi-amd64"
                apt install grub-efi-amd64 -y

                aux_business "GPU passthrough phase 1 completed" "component=gpu,operation=passthrough_preparation,phase=complete,next_step=reboot"
                
                # Set up for step 2 after reboot
                echo "2" > "$progress_file"
                
                if [ "$no_reboot" = false ]; then
                    aux_info "Step 1 complete - rebooting to continue with step 2" "component=gpu,operation=passthrough_preparation,sequence=all,next_step=2"
                    aux_info "After reboot, run: gpu_ptp all" "component=gpu,operation=passthrough_preparation,instruction=resume"
                    reboot
                else
                    aux_info "Step 1 complete - reboot required before step 2" "component=gpu,operation=passthrough_preparation,sequence=all,reboot_deferred=true"
                fi
                ;;
                
            2)
                aux_info "Executing step 2 of complete setup: VFIO modules" "component=gpu,operation=passthrough_preparation,sequence=all,step=2"
                
                # Step 2 dependency checks
                if ! aux_chk "command" "update-initramfs"; then
                    aux_err "update-initramfs command not found - required for initrd generation"
                    return 127
                fi
                
                if ! aux_chk "permission" "/etc/modules" "w"; then
                    aux_err "/etc/modules not writable - required for module configuration"
                    return 2
                fi
                
                aux_business "GPU passthrough phase 2 initiated" "component=gpu,operation=passthrough_preparation,phase=vfio_setup,step=module_configuration"

                # Add modules to /etc/modules
                local modules=(vfio vfio_iommu_type1 vfio_pci)
                aux_info "Adding VFIO modules to /etc/modules" "component=gpu,operation=passthrough_preparation,step=add_modules,modules=${modules[*]}"
                for module in "${modules[@]}"; do
                    aux_dbg "Adding module to /etc/modules" "component=gpu,operation=passthrough_preparation,step=add_modules,module=${module}"
                    echo "$module" >> /etc/modules
                done

                # Update initramfs
                aux_perf "Updating initramfs for all kernels" "component=gpu,operation=passthrough_preparation,step=update_initramfs"
                update-initramfs -u -k all

                aux_business "GPU passthrough phase 2 completed" "component=gpu,operation=passthrough_preparation,phase=complete,next_step=reboot"
                
                # Set up for step 3 after reboot
                echo "3" > "$progress_file"
                
                if [ "$no_reboot" = false ]; then
                    aux_info "Step 2 complete - rebooting to continue with step 3" "component=gpu,operation=passthrough_preparation,sequence=all,next_step=3"
                    aux_info "After reboot, run: gpu_ptp all ${action}" "component=gpu,operation=passthrough_preparation,instruction=resume"
                    reboot
                else
                    aux_info "Step 2 complete - reboot required before step 3" "component=gpu,operation=passthrough_preparation,sequence=all,reboot_deferred=true"
                fi
                ;;
                
            3)
                aux_info "Executing step 3 of complete setup: device binding" "component=gpu,operation=passthrough_preparation,sequence=all,step=3,action=${action}"
                
                # Step 3 dependency checks
                if ! aux_chk "command" "lspci"; then
                    aux_err "lspci command not found - required for GPU device detection"
                    return 127
                fi
                
                if ! aux_chk "command" "update-initramfs"; then
                    aux_err "update-initramfs command not found - required for applying changes"
                    return 127
                fi
                
                if [ ! -d "/etc/modprobe.d" ] || [ ! -w "/etc/modprobe.d" ]; then
                    aux_err "/etc/modprobe.d directory not writable - required for configuration"
                    return 2
                fi
                
                aux_info "Starting GPU passthrough device configuration" "action=${action}"

                local vfio_conf="/etc/modprobe.d/vfio.conf"
                local passthrough_blacklist_conf="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"
                local modules_file="/etc/modules"

                if [ "$action" = "enable" ]; then
                    aux_info "Configuring GPU passthrough (vfio-pci)" "component=gpu,operation=passthrough_preparation,action=enable"
                    
                    # Get all GPU vendor:device IDs
                    local -a gpu_ids
                    readarray -t gpu_ids < <(_gpu_find_all_gpus)
                    
                    if [ ${#gpu_ids[@]} -eq 0 ]; then
                        aux_info "No GPU devices found for passthrough configuration" "component=gpu,operation=passthrough_preparation,action=enable,gpu_count=0"
                        # Clean up progress file and return
                        rm -f "$progress_file"
                        return 0
                    fi

                    local vfio_options_line="options vfio-pci ids="
                    local ids_to_add=""
                    local nvidia_gpus_for_vfio=false
                    local amd_gpus_for_vfio=false

                    for pci_id in "${gpu_ids[@]}"; do
                        local vendor_device_id
                        vendor_device_id=$(_gpu_extract_vendor_device_id "$pci_id")
                        
                        if [ $? -eq 0 ] && [ -n "$vendor_device_id" ]; then
                            aux_info "Found GPU for passthrough configuration" "component=gpu,operation=passthrough_preparation,pci_id=${pci_id},vendor_device_id=${vendor_device_id}"
                            
                            if [ -n "$ids_to_add" ]; then
                                ids_to_add+=","
                            fi
                            ids_to_add+="$vendor_device_id"

                            local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
                            case "$vendor_id" in
                                "10de") nvidia_gpus_for_vfio=true ;;
                                "1002") amd_gpus_for_vfio=true ;;
                            esac
                        fi
                    done

                    if [ -n "$ids_to_add" ]; then
                        vfio_options_line+="$ids_to_add"
                        echo "$vfio_options_line" | tee "$vfio_conf" > /dev/null
                        aux_info "Created VFIO configuration file" "component=gpu,operation=passthrough_preparation,config_file=${vfio_conf},ids=${ids_to_add}"
                        cat "$vfio_conf"
                    fi

                    # Create blacklist file
                    aux_info "Creating GPU driver blacklist for passthrough" "component=gpu,operation=passthrough_preparation,action=create_blacklist"
                    rm -f "$passthrough_blacklist_conf"
                    touch "$passthrough_blacklist_conf"

                    if $nvidia_gpus_for_vfio; then
                        {
                            echo "blacklist nouveau"
                            echo "options nouveau modeset=0"
                            echo "blacklist nvidia"
                        } | tee -a "$passthrough_blacklist_conf" > /dev/null
                    fi

                    if $amd_gpus_for_vfio; then
                        {
                            echo "blacklist radeon"
                            echo "blacklist amdgpu"
                        } | tee -a "$passthrough_blacklist_conf" > /dev/null
                    fi

                    # Ensure VFIO modules are in /etc/modules
                    local vfio_modules=(vfio vfio_iommu_type1 vfio_pci)
                    for module in "${vfio_modules[@]}"; do
                        if ! grep -qP "^\s*${module}\s*(#.*)?$" "$modules_file"; then
                            echo "$module" | tee -a "$modules_file" > /dev/null
                            aux_info "Added VFIO module to modules file" "component=gpu,operation=passthrough_preparation,module=${module},file=${modules_file}"
                        else
                            # Uncomment if commented
                            if grep -qP "^\s*#\s*${module}" "$modules_file"; then
                                sed -i "s/^\s*#\s*${module}/${module}/" "$modules_file"
                                aux_info "Uncommented VFIO module in modules file" "component=gpu,operation=passthrough_preparation,module=${module},file=${modules_file}"
                            fi
                        fi
                    done

                elif [ "$action" = "disable" ]; then
                    aux_info "Reverting GPU passthrough configuration" "component=gpu,operation=passthrough_preparation,action=disable"
                    
                    # Remove configuration files
                    rm -f "$vfio_conf"
                    rm -f "$passthrough_blacklist_conf"

                    # Comment out VFIO modules
                    local vfio_modules=(vfio vfio_iommu_type1 vfio_pci)
                    for module in "${vfio_modules[@]}"; do
                        if grep -qP "^\s*${module}" "$modules_file"; then
                            sed -i "s/^\s*${module}/# ${module}/" "$modules_file"
                            aux_info "Commented out VFIO module in modules file" "component=gpu,operation=passthrough_preparation,module=${module},file=${modules_file}"
                        fi
                    done
                fi

                aux_info "Updating initramfs" "component=gpu,operation=passthrough_preparation,status=updating_initramfs"
                update-initramfs -u -k all
                
                # Clean up progress file and continuation mechanism - we're done
                _gpu_cleanup_continuation_script
                
                aux_info "All GPU passthrough preparation steps completed successfully" "component=gpu,operation=passthrough_preparation,sequence=all,status=complete"
                
                if [ "$no_reboot" = false ]; then
                    aux_info "Final reboot for complete GPU passthrough setup" "component=gpu,operation=passthrough_preparation,sequence=all,final_reboot=true"
                    reboot
                else
                    aux_info "Setup complete - manual reboot recommended for changes to take full effect" "component=gpu,operation=passthrough_preparation,sequence=all,reboot_deferred=true"
                fi
                ;;
        esac
        
        return 0
    fi

    # Individual step execution (original logic)
    # Step 1: IOMMU and GRUB configuration
    if [ "$step" = "1" ]; then
        aux_info "Executing GPU passthrough step 1: IOMMU/GRUB configuration" "component=gpu,operation=passthrough_preparation,step=1"
        
        # Check critical system dependencies for step 1
        if ! aux_chk "command" "sed"; then
            aux_err "sed command not found - required for GRUB configuration"
            return 127
        fi
        
        if ! aux_chk "command" "update-grub"; then
            aux_err "update-grub command not found - required for GRUB configuration"
            return 127
        fi
        
        if ! aux_chk "command" "apt"; then
            aux_err "apt command not found - required for package installation"
            return 127
        fi
        
        if ! aux_chk "command" "efibootmgr"; then
            aux_err "efibootmgr command not found - required for EFI boot management"
            return 127
        fi
        
        aux_business "GPU passthrough phase 1 initiated" "component=gpu,operation=passthrough_preparation,phase=iommu_setup,step=grub_configuration"
        
        aux_info "Checking EFI boot manager status" "component=gpu,operation=passthrough_preparation,step=efi_check"
        efibootmgr -v

        # Edit GRUB configuration
        aux_info "Modifying GRUB configuration for IOMMU" "component=gpu,operation=passthrough_preparation,step=grub_config,parameter=iommu=pt"
        sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="quiet"/GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt"/' /etc/default/grub
        
        aux_perf "Updating GRUB configuration" "component=gpu,operation=passthrough_preparation,step=grub_update"
        update-grub
        update-grub2

        # Install grub-efi-amd64
        aux_info "Installing GRUB EFI package" "component=gpu,operation=passthrough_preparation,step=grub_efi_install,package=grub-efi-amd64"
        apt install grub-efi-amd64 -y

        aux_business "GPU passthrough phase 1 completed" "component=gpu,operation=passthrough_preparation,phase=complete,next_step=reboot"
        
        if [ "$no_reboot" = false ]; then
            aux_info "System reboot initiated for step 1" "component=gpu,operation=passthrough_preparation,step=reboot,reason=iommu_kernel_parameter"
            reboot
        fi
    fi
    
    # Step 2: VFIO module configuration
    if [ "$step" = "2" ]; then
        aux_info "Executing GPU passthrough step 2: VFIO modules" "component=gpu,operation=passthrough_preparation,step=2"
        
        # Check critical system dependencies for step 2
        if ! aux_chk "command" "update-initramfs"; then
            aux_err "update-initramfs command not found - required for initrd generation"
            return 127
        fi
        
        if ! aux_chk "permission" "/etc/modules" "w"; then
            aux_err "/etc/modules not writable - required for module configuration"
            return 2
        fi
        
        aux_business "GPU passthrough phase 2 initiated" "component=gpu,operation=passthrough_preparation,phase=vfio_setup,step=module_configuration"

        # Add modules to /etc/modules
        local modules=(vfio vfio_iommu_type1 vfio_pci)
        aux_info "Adding VFIO modules to /etc/modules" "component=gpu,operation=passthrough_preparation,step=add_modules,modules=${modules[*]}"
        for module in "${modules[@]}"; do
            aux_dbg "Adding module to /etc/modules" "component=gpu,operation=passthrough_preparation,step=add_modules,module=${module}"
            echo "$module" >> /etc/modules
        done

        # Update initramfs
        aux_perf "Updating initramfs for all kernels" "component=gpu,operation=passthrough_preparation,step=update_initramfs"
        update-initramfs -u -k all

        aux_business "GPU passthrough phase 2 completed" "component=gpu,operation=passthrough_preparation,phase=complete,next_step=reboot"
        
        if [ "$no_reboot" = false ]; then
            aux_info "System reboot initiated for step 2" "component=gpu,operation=passthrough_preparation,step=reboot,reason=vfio_module_loading"
            reboot
        fi
    fi
    
    # Step 3: Device binding and driver configuration
    if [ "$step" = "3" ]; then
        aux_info "Executing GPU passthrough step 3: device binding" "component=gpu,operation=passthrough_preparation,sequence=all,step=3,action=${action}"
        
        # Step 3 dependency checks
        if ! aux_chk "command" "lspci"; then
            aux_err "lspci command not found - required for GPU device detection"
            return 127
        fi
        
        if ! aux_chk "command" "update-initramfs"; then
            aux_err "update-initramfs command not found - required for applying changes"
            return 127
        fi
        
        if [ ! -d "/etc/modprobe.d" ] || [ ! -w "/etc/modprobe.d" ]; then
            aux_err "/etc/modprobe.d directory not writable - required for configuration"
            return 2
        fi
        
        aux_info "Starting GPU passthrough device configuration" "action=${action}"

        local vfio_conf="/etc/modprobe.d/vfio.conf"
        local passthrough_blacklist_conf="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"
        local modules_file="/etc/modules"

        if [ "$action" = "enable" ]; then
            aux_info "Configuring GPU passthrough (vfio-pci)" "component=gpu,operation=passthrough_preparation,action=enable"
            
            # Get all GPU vendor:device IDs
            local -a gpu_ids
            readarray -t gpu_ids < <(_gpu_find_all_gpus)
            
            if [ ${#gpu_ids[@]} -eq 0 ]; then
                aux_info "No GPU devices found for passthrough configuration" "component=gpu,operation=passthrough_preparation,action=enable,gpu_count=0"
                # Clean up progress file and return
                rm -f "$progress_file"
                return 0
            fi

            local vfio_options_line="options vfio-pci ids="
            local ids_to_add=""
            local nvidia_gpus_for_vfio=false
            local amd_gpus_for_vfio=false

            for pci_id in "${gpu_ids[@]}"; do
                local vendor_device_id
                vendor_device_id=$(_gpu_extract_vendor_device_id "$pci_id")
                
                if [ $? -eq 0 ] && [ -n "$vendor_device_id" ]; then
                    aux_info "Found GPU for passthrough configuration" "component=gpu,operation=passthrough_preparation,pci_id=${pci_id},vendor_device_id=${vendor_device_id}"
                    
                    if [ -n "$ids_to_add" ]; then
                        ids_to_add+=","
                    fi
                    ids_to_add+="$vendor_device_id"

                    local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)
                    case "$vendor_id" in
                        "10de") nvidia_gpus_for_vfio=true ;;
                        "1002") amd_gpus_for_vfio=true ;;
                    esac
                fi
            done

            if [ -n "$ids_to_add" ]; then
                vfio_options_line+="$ids_to_add"
                echo "$vfio_options_line" | tee "$vfio_conf" > /dev/null
                aux_info "Created VFIO configuration file" "component=gpu,operation=passthrough_preparation,config_file=${vfio_conf},ids=${ids_to_add}"
                cat "$vfio_conf"
            fi

            # Create blacklist file
            aux_info "Creating GPU driver blacklist for passthrough" "component=gpu,operation=passthrough_preparation,action=create_blacklist"
            rm -f "$passthrough_blacklist_conf"
            touch "$passthrough_blacklist_conf"

            if $nvidia_gpus_for_vfio; then
                {
                    echo "blacklist nouveau"
                    echo "options nouveau modeset=0"
                    echo "blacklist nvidia"
                } | tee -a "$passthrough_blacklist_conf" > /dev/null
            fi

            if $amd_gpus_for_vfio; then
                {
                    echo "blacklist radeon"
                    echo "blacklist amdgpu"
                } | tee -a "$passthrough_blacklist_conf" > /dev/null
            fi

            # Ensure VFIO modules are in /etc/modules
            local vfio_modules=(vfio vfio_iommu_type1 vfio_pci)
            for module in "${vfio_modules[@]}"; do
                if ! grep -qP "^\s*${module}\s*(#.*)?$" "$modules_file"; then
                    echo "$module" | tee -a "$modules_file" > /dev/null
                    aux_info "Added VFIO module to modules file" "component=gpu,operation=passthrough_preparation,module=${module},file=${modules_file}"
                else
                    # Uncomment if commented
                    if grep -qP "^\s*#\s*${module}" "$modules_file"; then
                        sed -i "s/^\s*#\s*${module}/${module}/" "$modules_file"
                        aux_info "Uncommented VFIO module in modules file" "component=gpu,operation=passthrough_preparation,module=${module},file=${modules_file}"
                    fi
                fi
            done

        elif [ "$action" = "disable" ]; then
            aux_info "Reverting GPU passthrough configuration" "component=gpu,operation=passthrough_preparation,action=disable"
            
            # Remove configuration files
            rm -f "$vfio_conf"
            rm -f "$passthrough_blacklist_conf"

            # Comment out VFIO modules
            local vfio_modules=(vfio vfio_iommu_type1 vfio_pci)
            for module in "${vfio_modules[@]}"; do
                if grep -qP "^\s*${module}" "$modules_file"; then
                    sed -i "s/^\s*${module}/# ${module}/" "$modules_file"
                    aux_info "Commented out VFIO module in modules file" "component=gpu,operation=passthrough_preparation,module=${module},file=${modules_file}"
                fi
            done
        fi

        aux_info "Updating initramfs" "component=gpu,operation=passthrough_preparation,status=updating_initramfs"
        update-initramfs -u -k all
        
        # Clean up progress file - we're done
        rm -f "$progress_file"
        
        aux_info "All GPU passthrough preparation steps completed successfully" "component=gpu,operation=passthrough_preparation,sequence=all,status=complete"
        
        if [ "$no_reboot" = false ]; then
            aux_info "Final reboot for complete GPU passthrough setup" "component=gpu,operation=passthrough_preparation,sequence=all,final_reboot=true"
            reboot
        else
            aux_info "Setup complete - manual reboot recommended for changes to take full effect" "component=gpu,operation=passthrough_preparation,sequence=all,reboot_deferred=true"
        fi
        ;;
    esac
    
    return 0
}

# Detaches the GPU from the host system for VM passthrough
# passthrough detach
# [gpu_id] [hostname] [config_file] [pci0_id] [pci1_id] [nvidia_driver_preference]
gpu_ptd() {
    # Technical Description:
    #   Dynamic GPU detachment function that performs runtime switching of GPU control
    #   from host drivers to vfio-pci for virtual machine passthrough without requiring
    #   system reboot. Attempts to be self-sufficient by loading required VFIO modules,
    #   unbinding current host drivers, and binding target GPUs to vfio-pci using
    #   driver_override mechanism. Validates IOMMU availability and provides warnings
    #   for potential issues. Designed to work with both persistent and non-persistent
    #   GPU passthrough configurations.
    # Dependencies:
    #   - IOMMU enabled in kernel (set by gpu_pt1, verified but not enforced)
    #   - VFIO kernel modules available (loaded dynamically if needed)
    #   - Root privileges for driver binding operations
    #   - Target GPU devices not critical for current display output
    # Arguments:
    #   $1: gpu_id_arg (optional) - specific PCI ID (e.g., "01:00.0") to process
    #   $2: hostname (optional) - hostname for config lookup (default: current hostname)
    #   $3: config_file (optional) - configuration file path (default: SITE_CONFIG_FILE)
    #   $4: pci0_id (optional) - explicit primary PCI ID parameter
    #   $5: pci1_id (optional) - explicit secondary PCI ID parameter
    #   $6: nvidia_driver_preference (optional) - preferred NVIDIA driver (default: nvidia)
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    # Variable parameter count validation - max 6 parameters
    if [ $# -gt 6 ]; then
        aux_err "Too many parameters provided (maximum 6)"
        aux_use
        return 1
    fi
    
    # Validate individual parameters if provided
    for param in "$@"; do
        if ! aux_val "$param" "not_empty"; then
            aux_err "Parameter cannot be empty: $param"
            aux_use
            return 1
        fi
    done
    
    # Check critical system dependencies
    if ! aux_chk "command" "lspci"; then
        aux_err "lspci command not found - required for GPU identification"
        return 127
    fi
    
    if [ ! -d "/sys/bus/pci/drivers" ]; then
        aux_err "PCI driver system not available"
        return 2
    fi
    # Operations Performed:
    #   - Validates IOMMU kernel parameter presence in /proc/cmdline
    #   - Loads vfio, vfio_iommu_type1, vfio_pci modules if not already loaded
    #   - Identifies target GPU devices from parameters or configuration
    #   - Unbinds GPUs from current host drivers (nvidia, nouveau, amdgpu, radeon)
    #   - Sets driver_override to "vfio-pci" and triggers driver re-probe
    #   - Verifies successful binding to vfio-pci driver
    # Device Selection Priority:
    #   1. Explicit gpu_id_arg parameter
    #   2. Configuration file hostname-specific PCI IDs
    #   3. Automatic detection of all GPU devices via lspci scan
    local gpu_id_arg="$1"
    local hostname="${2:-$(hostname -s)}"
    local config_file="${3:-$CONFIG_FUN}"
    local pci0_id="$4"
    local pci1_id="$5"  
    local nvidia_driver_preference="${6:-nvidia}"
    
    local function_name="${FUNCNAME[0]}"
    
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=passthrough_detach,hostname=${hostname},gpu_id=${gpu_id_arg:-auto},config_file=${config_file}"
    aux_business "GPU detachment process initiated" "component=gpu,operation=passthrough_detach,target_state=vfio-pci,hostname=${hostname}"
    
    aux_info "Starting GPU detachment process" "component=gpu,operation=passthrough_detach,step=initialization"

    # Check IOMMU
    if ! grep -q 'iommu=pt\|intel_iommu=on\|amd_iommu=on' /proc/cmdline; then
        aux_warn "IOMMU not enabled in kernel command line - VFIO passthrough may not work" "component=gpu,operation=passthrough_detach,check=iommu,status=disabled,impact=vfio_may_fail"
    else
        aux_info "IOMMU enabled in kernel" "component=gpu,operation=passthrough_detach,check=iommu,status=enabled"
    fi

    # Load configuration if config file provided
    if [ -n "$config_file" ] && [ -f "$config_file" ]; then
        # shellcheck source=/dev/null
        source "$config_file"
        aux_info "GPU configuration loaded for detachment" "component=gpu,operation=passthrough_detach,config_file=${config_file},status=success"
    else
        aux_warn "GPU configuration file not available for detachment" "component=gpu,operation=passthrough_detach,config_file=${config_file},status=unavailable"
    fi

    # Unload GPU drivers to prevent conflicts (critical fix from old gpu-ptd)
    local drivers_to_unload=(nouveau nvidia amdgpu radeon)
    for driver in "${drivers_to_unload[@]}"; do
        if lsmod | grep -q "^${driver//-/_}"; then
            aux_info "Unloading GPU driver for clean detachment" "component=gpu,operation=passthrough_detach,driver=${driver},action=unload"
            if modprobe -r "$driver" 2>/dev/null; then
                aux_info "GPU driver unloaded successfully" "component=gpu,operation=passthrough_detach,driver=${driver},status=success"
            else
                aux_warn "GPU driver unload failed" "component=gpu,operation=passthrough_detach,driver=${driver},status=failed"
            fi
        else
            aux_info "GPU driver not loaded" "component=gpu,operation=passthrough_detach,driver=${driver},status=not_loaded"
        fi
    done

    # Ensure VFIO modules are loaded
    if ! _gpu_ensure_vfio_modules; then
        aux_err "VFIO modules loading failed" "component=gpu,operation=passthrough_detach,step=vfio_modules,status=failed"
        return 1
    fi

    # Get target GPUs (exclude those already on vfio-pci)
    local -a gpus_to_process
    readarray -t gpus_to_process < <(_gpu_get_target_gpus_parameterized "$gpu_id_arg" "$hostname" "" "$pci0_id" "$pci1_id")
    
    # Filter out GPUs already on vfio-pci
    local -a filtered_gpus=()
    for pci_id in "${gpus_to_process[@]}"; do
        local current_driver
        current_driver=$(_gpu_get_current_driver "$pci_id")
        if [ "$current_driver" != "vfio-pci" ]; then
            filtered_gpus+=("$pci_id")
        else
            aux_info "GPU already on vfio-pci, skipping" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},status=already_attached"
        fi
    done

    if [ ${#filtered_gpus[@]} -eq 0 ]; then
        aux_warn "No suitable GPU devices found for detachment" "component=gpu,operation=passthrough_detach,target_gpus=0,reason=no_suitable_devices"
        return 0
    fi

    aux_info "Processing GPUs for detachment" "component=gpu,operation=passthrough_detach,gpu_count=${#filtered_gpus[@]},gpu_ids=${filtered_gpus[*]}"

    # Process each GPU
    for pci_id in "${filtered_gpus[@]}"; do
        aux_info "Processing individual GPU for detachment" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=individual_processing"
        
        # Unbind from current driver
        _gpu_unbind_device "$pci_id"
        
        # Set driver override and bind to vfio-pci
        local full_pci_id="0000:$pci_id"
        aux_info "Setting driver override to vfio-pci" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=driver_override"
        
        if echo "vfio-pci" > "/sys/bus/pci/devices/$full_pci_id/driver_override"; then
            aux_info "Driver override set successfully" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},driver_override=vfio-pci,status=success"
            
            # Trigger re-probe
            if echo "$full_pci_id" > "/sys/bus/pci/drivers_probe"; then
                aux_info "Driver re-probe triggered" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=reprobe,status=success"
                sleep 1
            else
                aux_warn "Driver re-probe failed" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=reprobe,status=failed"
            fi
        else
            aux_err "Failed to set driver override" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},driver_override=vfio-pci,status=failed"
        fi
        
        # Verify binding or try direct bind
        local current_driver
        current_driver=$(_gpu_get_current_driver "$pci_id")
        if [ "$current_driver" = "vfio-pci" ]; then
            aux_info "GPU successfully bound to vfio-pci" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},final_driver=vfio-pci,status=success"
        else
            aux_info "Attempting direct bind to vfio-pci" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=direct_bind,current_driver=${current_driver}"
            if ! _gpu_bind_device "$pci_id" "vfio-pci"; then
                aux_err "Direct bind to vfio-pci failed" "component=gpu,operation=passthrough_detach,pci_id=${pci_id},step=direct_bind,status=failed"
            fi
        fi
    done

    aux_business "GPU detachment process completed" "component=gpu,operation=passthrough_detach,processed_gpus=${#filtered_gpus[@]},status=complete"
}

# Attaches the GPU back to the host system
# passthrough attach
# [gpu_id] [hostname] [config_file] [pci0_id] [pci1_id] [nvidia_driver_preference]
gpu_pta() {
    # Technical Description:
    #   Dynamic GPU attachment function that performs runtime switching of GPU control
    #   from vfio-pci back to appropriate host drivers for normal system usage.
    #   Intelligently determines correct host driver based on GPU vendor ID and
    #   configuration preferences, handles driver loading if necessary, and manages
    #   the complete transition from virtualization-ready state back to host control.
    #   Provides the reverse operation of gpu_ptd for flexible GPU management workflows.
    # Dependencies:
    #   - Target GPU devices currently bound to vfio-pci driver
    #   - Appropriate host drivers available (nvidia, nouveau, amdgpu, radeon)
    #   - Root privileges for driver binding and module loading operations
    #   - Configuration file access for driver preference settings
    #   - lspci command for GPU identification
    # Arguments:
    #   $1: gpu_id_arg (optional) - specific PCI ID (e.g., "01:00.0") to process
    #   $2: hostname (optional) - hostname for config lookup (default: current hostname)
    #   $3: config_file (optional) - configuration file path
    #   $4: pci0_id (optional) - explicit primary PCI ID parameter
    #   $5: pci1_id (optional) - explicit secondary PCI ID parameter
    #   $6: nvidia_driver_preference (optional) - preferred NVIDIA driver
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    # Variable parameter count validation - max 6 parameters
    if [ $# -gt 6 ]; then
        aux_err "Too many parameters provided (maximum 6)"
        aux_use
        return 1
    fi
    
    # Validate individual parameters if provided
    for param in "$@"; do
        if ! aux_val "$param" "not_empty"; then
            aux_err "Parameter cannot be empty: $param"
            aux_use
            return 1
        fi
    done
    
    # Check critical system dependencies
    if ! aux_chk "command" "lspci"; then
        aux_err "lspci command not found - required for GPU identification"
        return 127
    fi
    
    if [ ! -d "/sys/bus/pci/drivers" ]; then
        aux_err "PCI driver system not available"
        return 2
    fi
    #   $3: config_file (optional) - configuration file path (default: SITE_CONFIG_FILE)
    #   $4: pci0_id (optional) - explicit primary PCI ID parameter
    #   $5: pci1_id (optional) - explicit secondary PCI ID parameter
    #   $6: nvidia_driver_preference (optional) - preferred NVIDIA driver (default: nvidia)
    # Operations Performed:
    #   - Identifies GPUs currently bound to vfio-pci driver
    #   - Unbinds target GPUs from vfio-pci driver
    #   - Clears driver_override setting to allow normal driver binding
    #   - Determines appropriate host driver based on vendor ID and preferences
    #   - Loads required host driver module if not already loaded
    #   - Binds GPU devices to determined host driver
    #   - Verifies successful attachment to host driver
    # Driver Selection Logic:
    #   - NVIDIA GPUs (10de): Uses hostname_NVIDIA_DRIVER_PREFERENCE or "nvidia" default
    #   - AMD GPUs (1002): Uses "amdgpu" driver
    #   - Considers blacklist conflicts and warns if preferred driver is unavailable
    # Configuration Integration:
    #   - Respects ${hostname}_NVIDIA_DRIVER_PREFERENCE configuration variable
    #   - Warns if nouveau is preferred but blacklisted by gpu_nds installation
    local gpu_id_arg="$1"
    local hostname="${2:-$(hostname -s)}"
    local config_file="${3:-$CONFIG_FUN}"
    local pci0_id="$4"
    local pci1_id="$5"
    local nvidia_driver_preference="${6:-nvidia}"
    
    local function_name="${FUNCNAME[0]}"
    
    aux_info "Starting GPU attachment process" "component=gpu,operation=passthrough_attach,step=initialization"

    # Load configuration if config file provided
    if [ -n "$config_file" ] && [ -f "$config_file" ]; then
        # shellcheck source=/dev/null
        source "$config_file"
        aux_info "GPU configuration loaded for attachment" "component=gpu,operation=passthrough_attach,config_file=${config_file},status=success"
    fi

    # Get target GPUs (only those on vfio-pci)
    local -a gpus_to_process
    readarray -t gpus_to_process < <(_gpu_get_target_gpus_parameterized "$gpu_id_arg" "$hostname" "vfio-pci" "$pci0_id" "$pci1_id")

    if [ ${#gpus_to_process[@]} -eq 0 ]; then
        aux_warn "No suitable GPU devices found on vfio-pci to attach" "component=gpu,operation=passthrough_attach,target_gpus=0,reason=no_vfio_devices"
        return 0
    fi

    aux_info "Final list of GPU IDs to process for attachment" "component=gpu,operation=passthrough_attach,gpu_count=${#gpus_to_process[@]},gpu_ids=${gpus_to_process[*]}"

    # Process each GPU
    for pci_id in "${gpus_to_process[@]}"; do
        aux_info "Processing GPU for attachment to host" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},step=individual_processing"
        
        # Unbind from vfio-pci
        _gpu_unbind_device "$pci_id"
        
        # Clear driver override
        local full_pci_id="0000:$pci_id"
        aux_info "Clearing driver override" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},step=clear_override"
        echo > "/sys/bus/pci/devices/$full_pci_id/driver_override"
        
        # Determine host driver
        local host_driver
        host_driver=$(_gpu_get_host_driver_parameterized "$pci_id" "$hostname" "$nvidia_driver_preference")
        
        if [ $? -ne 0 ] || [ -z "$host_driver" ]; then
            aux_warn "Could not determine host driver, skipping GPU" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},status=skipped"
            continue
        fi
        
        aux_info "Target host driver determined" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver}"
        
        # Load host driver module if needed
        if ! lsmod | grep -q "^${host_driver//-/_}"; then
            aux_info "Loading host driver module" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver},action=load_module"
            if ! modprobe "$host_driver"; then
                aux_err "Failed to load host driver module" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver},status=failed"
                continue
            fi
        fi
        
        # Check host_driver value
        aux_dbg "Host driver determination result" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver}"
        
        # Simple minimal attachment - based on working gpu-pta function
        aux_info "Attempting to bind GPU to driver using minimal approach" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver},approach=minimal"
        
        # Probe for new driver (like the working old function)
        aux_info "Probing for new driver" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},step=probe"
        echo "0000:$pci_id" > /sys/bus/pci/drivers_probe
        
        # Explicitly bind the driver if it's not automatically bound (like the working old function)
        if [ ! -e "/sys/bus/pci/devices/0000:$pci_id/driver" ]; then
            aux_info "Attempting explicit driver bind" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver},step=explicit_bind"
            if echo "0000:$pci_id" > "/sys/bus/pci/drivers/$host_driver/bind" 2>/dev/null; then
                aux_info "GPU successfully bound to host driver" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver},status=success"
            else
                aux_err "Failed to bind driver to GPU" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver},status=failed"
                continue
            fi
        else
            aux_info "GPU successfully bound to host driver" "component=gpu,operation=passthrough_attach,pci_id=${pci_id},host_driver=${host_driver},status=success"
        fi
        
        # Skip per-GPU nvidia_drm reload - will be done once after all GPUs processed
    done

    # CRITICAL FIX: NVIDIA DRM modeset reload AFTER all GPUs processed
    # This prevents "module in use" errors when processing multiple GPUs
    aux_info "Checking for NVIDIA driver configuration" "component=gpu,operation=passthrough_attach,step=nvidia_config_check"
    
    # Check if any GPU was bound to nvidia driver
    local has_nvidia_gpu=false
    for pci_id in "${gpus_to_process[@]}"; do
        local current_driver=$(lspci -nnk -s "${pci_id}" | awk '/Kernel driver in use:/ {print $5}')
        if [ "$current_driver" = "nvidia" ]; then
            has_nvidia_gpu=true
            break
        fi
    done
    
    # If NVIDIA GPUs found, configure framebuffer support
    if [ "$has_nvidia_gpu" = "true" ]; then
        aux_info "NVIDIA GPUs detected, configuring framebuffer display support" "component=gpu,operation=passthrough_attach,step=nvidia_framebuffer_config"
        
        # Load nvidia_modeset if not loaded
        if ! lsmod | grep -q "^nvidia_modeset"; then
            aux_info "Loading nvidia_modeset module" "component=gpu,operation=passthrough_attach,step=load_nvidia_modeset"
            modprobe nvidia_modeset
        fi
        
        # THE CRITICAL FIX: Reload nvidia_drm with modeset=1 ONCE for all GPUs
        aux_info "Reloading nvidia_drm with modeset=1 for display support" "component=gpu,operation=passthrough_attach,step=nvidia_drm_reload"
        
        # Remove nvidia_drm if loaded
        if [ -d "/sys/module/nvidia_drm" ]; then
            aux_dbg "nvidia_drm module currently loaded, attempting removal" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,status=attempting"
            local module_usage=$(lsmod | grep nvidia_drm)
            aux_dbg "Module usage before removal" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,module_usage=${module_usage}"
            
            if rmmod nvidia_drm 2>/tmp/rmmod_error.log; then
                aux_dbg "rmmod nvidia_drm succeeded" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,method=rmmod,status=success"
            else
                local rmmod_exit_code=$?
                local rmmod_error=$(cat /tmp/rmmod_error.log 2>/dev/null || echo 'no error log')
                aux_dbg "rmmod nvidia_drm failed, trying modprobe -r" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,method=rmmod,status=failed,exit_code=${rmmod_exit_code},error=${rmmod_error}"
                
                if modprobe -r nvidia_drm 2>/tmp/modprobe_r_error.log; then
                    aux_dbg "modprobe -r nvidia_drm succeeded" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,method=modprobe_r,status=success"
                else
                    local modprobe_r_exit_code=$?
                    local modprobe_r_error=$(cat /tmp/modprobe_r_error.log 2>/dev/null || echo 'no error log')
                    aux_warn "Failed to remove nvidia_drm module" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,method=modprobe_r,status=failed,exit_code=${modprobe_r_exit_code},error=${modprobe_r_error}"
                fi
            fi
            
            # Verify module state after removal attempts
            if lsmod | grep -q "^nvidia_drm"; then
                local nvidia_drm_status=$(lsmod | grep nvidia_drm)
                aux_dbg "nvidia_drm still loaded after removal attempts" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,status=still_loaded,module_info=${nvidia_drm_status}"
            else
                aux_dbg "nvidia_drm successfully removed" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,status=removed"
            fi
        else
            aux_dbg "nvidia_drm module not currently loaded" "component=gpu,operation=passthrough_attach,step=nvidia_drm_removal,status=not_loaded"
        fi
        
        # Load nvidia_drm with modeset=1
        aux_info "Loading nvidia_drm with modeset=1" "component=gpu,operation=passthrough_attach,step=nvidia_drm_load"
        
        # EXECUTION CONTEXT DEBUG: Compare with manual command environment
        # Log execution context for debugging
        aux_dbg "Function execution context" "component=gpu,operation=passthrough_attach,pid=$$,ppid=$PPID,pwd=$PWD,user=$USER,shell=$SHELL,funcname_depth=${#FUNCNAME[@]},open_fds=$(ls /proc/$$/fd | wc -l)"
        
        # MODULE STATE BEFORE MODPROBE
        local nvidia_modules_before=$(lsmod | grep -E "(nvidia)" || echo "No nvidia modules found")
        aux_dbg "Module state before modprobe nvidia_drm modeset=1" "component=gpu,operation=passthrough_attach,step=pre_modprobe,nvidia_modules=${nvidia_modules_before}"
        
        # EXECUTE MODPROBE WITH DETAILED TRACKING
        aux_dbg "Executing modprobe nvidia_drm modeset=1" "component=gpu,operation=passthrough_attach,step=modprobe_execute"
        if modprobe nvidia_drm modeset=1; then
            aux_dbg "modprobe command returned success" "component=gpu,operation=passthrough_attach,step=modprobe_execute,status=success,exit_code=0"
            
            # IMMEDIATE VERIFICATION - Check module state right after modprobe
            if lsmod | grep -q "^nvidia_drm"; then
                local modeset_param=$(cat /sys/module/nvidia_drm/parameters/modeset 2>/dev/null || echo 'UNKNOWN')
                local fb_devices=$(ls /dev/fb* 2>/dev/null || echo 'NONE')
                local vga_arbiter=$(cat /sys/bus/pci/devices/0000:3b:00.0/boot_vga 2>/dev/null || echo 'UNKNOWN')
                aux_dbg "nvidia_drm module loaded successfully" "component=gpu,operation=passthrough_attach,step=verification,nvidia_drm_loaded=yes,modeset=${modeset_param},fb_devices=${fb_devices},vga_arbiter=${vga_arbiter}"
            else
                aux_err "nvidia_drm module not loaded after modprobe" "component=gpu,operation=passthrough_attach,step=verification,nvidia_drm_loaded=no"
            fi
            
            # FINAL STATUS CHECK
            local final_modeset_status=$(cat /sys/module/nvidia_drm/parameters/modeset 2>/dev/null || echo "UNKNOWN")
            if [ "$final_modeset_status" = "Y" ]; then
                aux_info "NVIDIA framebuffer display support enabled successfully" "component=gpu,operation=passthrough_attach,step=final_check,modeset=Y,status=success"
                
                # POST-SUCCESS VERIFICATION
                local fb_devices_post=$(ls /dev/fb* 2>/dev/null && echo ' (CREATED)' || echo 'NONE (MISSING!)')
                local console_drivers=$(cat /sys/class/vtconsole/vtcon*/name 2>/dev/null || echo 'UNKNOWN')
                local vt_bind_status=$(cat /sys/class/vtconsole/vtcon*/bind 2>/dev/null | paste -sd ' ' || echo 'UNKNOWN')
                aux_dbg "Post-success system state verification" "component=gpu,operation=passthrough_attach,step=post_success,fb_devices=${fb_devices_post},console_drivers=${console_drivers},vt_bind_status=${vt_bind_status}"
            else
                aux_warn "modeset parameter not set correctly" "component=gpu,operation=passthrough_attach,step=final_check,modeset_status=${final_modeset_status}"
            fi
        else
            local modprobe_exit_code=$?
            aux_err "Failed to load nvidia_drm with modeset=1" "component=gpu,operation=passthrough_attach,step=modprobe_execute,status=failed,exit_code=${modprobe_exit_code}"
        fi
    fi

    aux_business "GPU attachment process completed" "component=gpu,operation=passthrough_attach,status=complete"
}

# Checks the current status of the GPU (complete detailed version)
# passthrough status
# [hostname] [config_file] [pci0_id] [pci1_id] [nvidia_driver_preference]
gpu_pts() {
    # Technical Description:
    #   Comprehensive GPU passthrough status analysis function that provides detailed
    #   system state information across all aspects of GPU passthrough configuration.
    #   Performs thorough examination of IOMMU groups, device bindings, kernel modules,
    #   configuration files, and runtime status to present complete picture of current
    #   GPU passthrough setup. Includes both high-level summary and detailed technical
    #   information for troubleshooting and verification purposes.
    # Dependencies:
    #   - lspci utility for GPU device detection and information retrieval
    #   - Access to /sys/bus/pci for driver binding status
    #   - Access to /proc/cmdline for kernel parameter verification
    #   - Access to configuration files in /etc/modprobe.d and /etc/modules
    #   - find, ls utilities for IOMMU group enumeration
    # Arguments:
    #   $1: hostname (optional) - hostname for config lookup (default: current hostname)
    #   $2: config_file (optional) - configuration file path
    #   $3: pci0_id (optional) - explicit primary PCI ID parameter
    #   $4: pci1_id (optional) - explicit secondary PCI ID parameter
    #   $5: nvidia_driver_preference (optional) - preferred NVIDIA driver
    
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    # Variable parameter count validation - max 5 parameters
    if [ $# -gt 5 ]; then
        aux_err "Too many parameters provided (maximum 5)"
        aux_use
        return 1
    fi
    
    # Validate individual parameters if provided
    for param in "$@"; do
        if ! aux_val "$param" "not_empty"; then
            aux_err "Parameter cannot be empty: $param"
            aux_use
            return 1
        fi
    done
    
    # Check critical system dependencies
    if ! aux_chk "command" "lspci"; then
        aux_err "lspci command not found - required for GPU identification"
        return 127
    fi
    
    if [ ! -f "/proc/cmdline" ] || [ ! -r "/proc/cmdline" ]; then
        aux_err "/proc/cmdline not accessible - required for kernel parameter verification"
        return 2
    fi
    #   $2: config_file (optional) - configuration file path (default: SITE_CONFIG_FILE)
    #   $3: pci0_id (optional) - explicit primary PCI ID parameter
    #   $4: pci1_id (optional) - explicit secondary PCI ID parameter
    #   $5: nvidia_driver_preference (optional) - preferred NVIDIA driver (default: nvidia)
    # Information Displayed:
    #   - IOMMU Groups: Detailed device listings for GPU-containing IOMMU groups
    #   - GPU Device Details: Complete lspci output with driver binding information
    #   - Kernel Modules: Currently loaded GPU and VFIO related modules
    #   - Configuration Status: Initial system preparation checklist (gpu_pt1, gpu_pt2)
    #   - NVIDIA Setup: Driver installation and Nouveau blacklist status
    #   - Persistent Configuration: GPU assignment and blacklist file status
    #   - Runtime Status: Current module loading and driver binding state
    #   - Overall Summary: High-level determination of GPU state (ATTACHED/DETACHED/MIXED)
    # Status Classifications:
    #   - ATTACHED: GPUs bound to host drivers (nvidia, nouveau, amdgpu, radeon)
    #   - DETACHED: GPUs bound to vfio-pci for VM passthrough
    #   - MIXED: Some GPUs attached to host, others detached for VMs
    #   - UNCLEAR: Ambiguous state requiring further investigation
    local hostname="${1:-$(hostname -s)}"
    local config_file="${2:-$CONFIG_FUN}"
    local pci0_id="$3"
    local pci1_id="$4"
    local nvidia_driver_preference="${5:-nvidia}"
    
    local function_name="${FUNCNAME[0]}"
    
    aux_info "Function ${FUNCNAME[0]} started" "component=gpu,operation=passthrough_status,hostname=${hostname},config_file=${config_file}"
    aux_business "GPU passthrough status analysis initiated" "component=gpu,operation=passthrough_status,hostname=${hostname}"
    
    # Load configuration if config file provided
    if [ -n "$config_file" ] && [ -f "$config_file" ]; then
        # shellcheck source=/dev/null
        source "$config_file"
        aux_info "GPU configuration loaded for status check" "component=gpu,operation=passthrough_status,config_file=${config_file},status=success"
    else
        aux_dbg "No configuration file provided or file not found" "component=gpu,operation=passthrough_status,config_file=${config_file:-none}"
    fi
    
    # Get detailed device information
    aux_info "Gathering detailed GPU device information" "component=gpu,operation=passthrough_status,step=device_analysis"
    local detailed_info
    detailed_info=$(_gpu_get_detailed_device_info)
    
    local all_device_blocks_output=""
    local gpu_details_for_checklist=""
    local is_any_gpu_on_vfio=false
    local host_drivers_for_gpu=""
    local vga_devices_processed=false
    
    # Parse the detailed info output
    while IFS= read -r line; do
        case "$line" in
            DEVICE_BLOCKS:*)
                all_device_blocks_output="${line#DEVICE_BLOCKS:}"
                ;;
            GPU_CHECKLIST:*)
                gpu_details_for_checklist="${line#GPU_CHECKLIST:}"
                ;;
            VFIO_STATUS:*)
                is_any_gpu_on_vfio="${line#VFIO_STATUS:}"
                ;;
            HOST_DRIVERS:*)
                host_drivers_for_gpu="${line#HOST_DRIVERS:}"
                ;;
            DEVICES_PROCESSED:*)
                vga_devices_processed="${line#DEVICES_PROCESSED:}"
                ;;
        esac
    done <<< "$detailed_info"
    
    aux_dbg "Device information parsing completed" "component=gpu,operation=passthrough_status,vfio_status=${is_any_gpu_on_vfio},host_drivers=${host_drivers_for_gpu},devices_processed=${vga_devices_processed}"
    
    # Define configuration file paths
    local modules_file_path="/etc/modules"
    local blacklist_nouveau_conf_path="/etc/modprobe.d/blacklist-nouveau.conf"
    local vfio_conf_path="/etc/modprobe.d/vfio.conf"
    local passthrough_blacklist_conf_path="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"

    # --- IOMMU Groups (Details) ---
    echo -e "\n${CYAN}--- IOMMU Groups (Details) ---${NC}"
    if ! command -v find &>/dev/null || ! command -v ls &>/dev/null || ! command -v lspci &>/dev/null; then
        echo -e "${RED}Required commands (find, ls, lspci) not available to list IOMMU groups.${NC}"
    else
        _gpu_get_iommu_groups
    fi

    # --- GPU Device Details (lspci -nnk) ---
    echo -e "\n${CYAN}--- GPU Device Details (lspci -nnk) ---${NC}"
    if [ -n "$all_device_blocks_output" ]; then
        echo -e "${YELLOW}${all_device_blocks_output%\\n\\n}${NC}"
    elif [ "$vga_devices_processed" = "true" ]; then 
         echo -e "${YELLOW}No detailed GPU device blocks were formatted (e.g. lspci output was unusual or parsing issue).${NC}"
    else 
         echo -e "${YELLOW}No VGA compatible or 3D controllers found by lspci to display details.${NC}"
    fi

    # --- Loaded Kernel Modules (GPU related) ---
    echo -e "\n${CYAN}--- Loaded Kernel Modules (GPU related) ---${NC}"
    if lsmod | command grep -E "vfio|nvidia|nouveau|amdgpu|radeon" --color=never; then
        : 
    else
        echo -e "${YELLOW}No common GPU-related modules (vfio, nvidia, nouveau, amdgpu, radeon) loaded.${NC}"
    fi

    echo -e "\n${CYAN}--- GPU Passthrough Workflow & Runtime Status ---${NC}"
    echo -e "${CYAN}==================================================${NC}"

    # 1. Initial System Preparation (Configuration)
    echo -e "${MAGENTA}1. Initial System Preparation (Configuration):${NC}"
    
    local iommu_enabled_cmdline_status="$CROSS_MARK"
    if /usr/bin/grep -E -q 'iommu=pt|intel_iommu=on|amd_iommu=on' /proc/cmdline; then
        iommu_enabled_cmdline_status="$CHECK_MARK"
    fi
    echo -e "   ${iommu_enabled_cmdline_status} IOMMU Enabled in Kernel Command Line (gpu_pt1)"

    local vfio_modules_in_etc_modules_status="$CROSS_MARK"
    local vfio_ok=0; local vfio_iommu_type1_ok=0; local vfio_pci_ok=0
    if [ -f "$modules_file_path" ]; then
        if grep -qP "^\\s*vfio(\\s|$|#)" "$modules_file_path"; then vfio_ok=1; fi
        if grep -qP "^\\s*vfio_iommu_type1(\\s|$|#)" "$modules_file_path"; then vfio_iommu_type1_ok=1; fi
        if grep -qP "^\\s*vfio_pci(\\s|$|#)" "$modules_file_path"; then vfio_pci_ok=1; fi
    fi
    if [ "$vfio_ok" -eq 1 ] && [ "$vfio_iommu_type1_ok" -eq 1 ] && [ "$vfio_pci_ok" -eq 1 ]; then
        vfio_modules_in_etc_modules_status="$CHECK_MARK"
    fi
    echo -e "   ${vfio_modules_in_etc_modules_status} Core VFIO modules in $modules_file_path (gpu_pt2)"

    # 2. NVIDIA Host Driver Setup (Optional Configuration)
    echo -e "${MAGENTA}2. NVIDIA Host Driver Setup (Optional Configuration):${NC}"
    local nouveau_blacklisted_status="$QUESTION_MARK (Not found)"
    if [ -f "$blacklist_nouveau_conf_path" ]; then
        if grep -q "blacklist nouveau" "$blacklist_nouveau_conf_path"; then
            nouveau_blacklisted_status="$CHECK_MARK"
        else
            nouveau_blacklisted_status="$CROSS_MARK (Found, but 'blacklist nouveau' missing)"
        fi
    fi
    echo -e "   ${nouveau_blacklisted_status} Nouveau blacklisted for NVIDIA driver ($blacklist_nouveau_conf_path) (gpu-nds)"

    # 3. Persistent GPU Passthrough Configuration (gpu_pt3 enable status)
    echo -e "${MAGENTA}3. Persistent GPU Passthrough Configuration (gpu_pt3 enable):${NC}"
    local persistent_vfio_ids_status="$CROSS_MARK"
    if [ -f "$vfio_conf_path" ] && grep -qE "^options vfio-pci ids=" "$vfio_conf_path"; then
        persistent_vfio_ids_status="$CHECK_MARK"
    fi
    echo -e "   ${persistent_vfio_ids_status} GPUs assigned to vfio-pci at boot ($vfio_conf_path)"

    local persistent_blacklist_status="$CROSS_MARK"
    if [ -f "$passthrough_blacklist_conf_path" ] && grep -qE "blacklist (nvidia|nouveau|amdgpu|radeon)" "$passthrough_blacklist_conf_path"; then
        persistent_blacklist_status="$CHECK_MARK"
    fi
    echo -e "   ${persistent_blacklist_status} Host drivers blacklisted for selected GPUs ($passthrough_blacklist_conf_path)"

    # 4. Runtime Kernel Module & Driver Status
    echo -e "${MAGENTA}4. Runtime Kernel Module & Driver Status:${NC}"
    local vfio_pci_loaded_status="$CROSS_MARK"
    if lsmod | grep -q "vfio_pci"; then
        vfio_pci_loaded_status="$CHECK_MARK"
    fi
    echo -e "   ${vfio_pci_loaded_status} VFIO-PCI kernel module currently loaded"

    local other_gpu_modules_loaded="-"
    if lsmod | command grep -qE "nvidia|nouveau|amdgpu|radeon"; then
        other_gpu_modules_loaded="${GREEN}Host drivers (nvidia/nouveau/amdgpu/radeon) loaded${NC}"
    else
        other_gpu_modules_loaded="${YELLOW}No common host GPU drivers loaded${NC}"
    fi
    if lsmod | command grep -qE "vfio_iommu_type1|vfio"; then
        if [[ "$other_gpu_modules_loaded" == "-" || "$other_gpu_modules_loaded" == *"${YELLOW}No common host GPU drivers loaded${NC}"* ]]; then
            other_gpu_modules_loaded="${GREEN}VFIO modules (vfio/vfio_iommu_type1) loaded${NC}"
        else
            other_gpu_modules_loaded+=", ${GREEN}VFIO modules (vfio/vfio_iommu_type1) loaded${NC}"
        fi 
    fi
    echo -e "   - Other relevant modules: $other_gpu_modules_loaded"
    
    # 4.3 Kernel Drivers for GPU(s) Summary
    echo -e "   ${CYAN}--- Kernel Drivers for GPU(s) Summary ---${NC}"
    if [ -n "$gpu_details_for_checklist" ]; then
        echo -e "${gpu_details_for_checklist%\\n}"
    elif [ "$vga_devices_processed" = "true" ]; then 
         echo -e "     ${YELLOW}?${NC} No summary for GPU driver bindings could be generated."
    else 
         echo -e "     ${YELLOW}?${NC} No VGA/3D controllers found to summarize driver bindings."
    fi

    # Overall GPU State Determination
    echo -e "\n--- Overall GPU State Summary ---"
    if [ "$is_any_gpu_on_vfio" = "true" ]; then
        echo -e "[SUMMARY] GPU State: DETACHED (for VM use)"
        echo -e "  - At least one GPU device (VGA/3D controller) is bound to vfio-pci."
        if [ -n "$host_drivers_for_gpu" ]; then
             echo -e "  - Note: Other GPU devices might be ATTACHED to host (driver(s): $host_drivers_for_gpu). State could be MIXED."
        fi
    elif [ -n "$host_drivers_for_gpu" ]; then
        echo -e "[SUMMARY] GPU State: ATTACHED to host (driver(s): $host_drivers_for_gpu)"
        echo -e "  - GPU device(s) (VGA/3D controller) are using host driver(s): $host_drivers_for_gpu."
        echo -e "  - No VGA/3D controller found using vfio-pci."
    else
        if [ "$vga_devices_processed" = "true" ]; then
            echo -e "[SUMMARY] GPU State: UNCLEAR / INDETERMINATE"
            echo -e "  - A VGA/3D controller was detected, but it's not using 'vfio-pci' nor any other recognized host driver."
            echo -e "  - It might be unbound. Check 'GPU Device Details' and IOMMU details."
        else
            echo -e "[SUMMARY] GPU State: N/A (No VGA/3D GPU controllers detected)"
        fi
        echo -e "  - Review IOMMU groups and loaded kernel modules above."
    fi

    # Log operational summary
    local gpu_state="UNKNOWN"
    if [ "$is_any_gpu_on_vfio" = "true" ]; then
        if [ -n "$host_drivers_for_gpu" ]; then
            gpu_state="MIXED"
        else
            gpu_state="DETACHED"
        fi
    elif [ -n "$host_drivers_for_gpu" ]; then
        gpu_state="ATTACHED"
    elif [ "$vga_devices_processed" = "true" ]; then
        gpu_state="UNCLEAR"
    else
        gpu_state="N/A"
    fi
    
    aux_business "GPU passthrough status analysis completed" "component=gpu,operation=passthrough_status,gpu_state=${gpu_state},vfio_gpus=${is_any_gpu_on_vfio},host_drivers=${host_drivers_for_gpu}"
    aux_info "Function ${FUNCNAME[0]} completed" "component=gpu,operation=passthrough_status,status=complete,gpu_state=${gpu_state}"

    echo -e "\n--- GPU status check completed. (${function_name}) ---"
    echo -e "===================================================="
}
