#!/bin/bash

# ============================================================================
# gpu - Function Summary
#
#   gpu-fun : Shows a summary of selected functions in the script.
#   gpu-var : Displays an overview of specific variables defined in the configuration file.
#   gpu-nds : Downloads and installs NVIDIA drivers, blacklisting Nouveau for host use.
#   gpu-pt1 : Configures initial GRUB and EFI settings for GPU passthrough, installs packages, and reboots.
#   gpu-pt2 : Adds kernel modules for GPU passthrough, updates initramfs, and reboots.
#   gpu-pt3 : Finalizes or reverts GPU passthrough setup (VFIO-PCI IDs, blacklisting drivers for passthrough).
#   gpu-ptd : Detaches the GPU from the host system for VM passthrough (dynamic).
#   gpu-pta : Attaches the GPU back to the host system (dynamic).
#   gpu-pts : Checks the current status of the GPU passthrough setup.
#
# ============================================================================
#
# ============================================================================
# GPU Driver and Passthrough Function Interactions - Technical Overview
#
# This section outlines crucial interactions between different GPU management
# functions, particularly concerning driver blacklisting and preferences.
# Understanding these interactions is key to avoiding conflicts and ensuring
# predictable behavior.
#
# Blacklist Files:
#   - /etc/modprobe.d/blacklist-nouveau.conf:
#       - Created/Managed by: `gpu-nds` (NVIDIA Driver Setup).
#       - Purpose: Persistently blacklists the `nouveau` driver to ensure the
#         NVIDIA proprietary driver is used by the HOST system when the GPU
#         is not assigned for passthrough.
#       - Persistence: Intended to be a permanent setting for host driver preference.
#
#   - /etc/modprobe.d/zz-vfio-gpu-blacklist.conf:
#       - Created/Managed by: `gpu-pt3 enable`.
#       - Removed by: `gpu-pt3 disable`.
#       - Purpose: Temporarily blacklists host drivers (`nouveau`, `nvidia`,
#         `amdgpu`, `radeon`) for GPUs intended for passthrough. This allows
#         `vfio-pci` to claim these devices at boot.
#       - Persistence: Only active when GPU passthrough is configured via `gpu-pt3 enable`.
#
# Function Interactions:
#
# 1. `gpu-nds` (NVIDIA Driver Setup):
#    - Installs NVIDIA proprietary drivers.
#    - Creates `/etc/modprobe.d/blacklist-nouveau.conf` to make the NVIDIA
#      proprietary driver the default for the host.
#    - This function assumes you want the NVIDIA driver for host operations.
#
# 2. `gpu-pt3 enable` (Configure Passthrough):
#    - Identifies GPUs for passthrough and adds their PCI IDs to
#      `/etc/modprobe.d/vfio.conf`.
#    - Creates `/etc/modprobe.d/zz-vfio-gpu-blacklist.conf`
#      to blacklist `nouveau`, `nvidia`, `amdgpu`, `radeon` for the
#      selected GPUs. This prevents host drivers from claiming these GPUs.
#    - Does NOT touch `/etc/modprobe.d/blacklist-nouveau.conf`.
#
# 3. `gpu-pt3 disable` (Revert Passthrough):
#    - Removes `/etc/modprobe.d/vfio.conf`.
#    - Removes `/etc/modprobe.d/zz-vfio-gpu-blacklist.conf`.
#    - This allows host drivers to again claim the GPUs.
#    - Does NOT touch `/etc/modprobe.d/blacklist-nouveau.conf`. If this file
#      exists (from `gpu-nds`), the NVIDIA proprietary driver will likely
#      claim NVIDIA GPUs.
#
# 4. `gpu-pta` (Attach GPU to Host - Dynamic):
#    - Attempts to unbind `vfio-pci` (if present) and bind a host driver.
#    - For NVIDIA GPUs, it uses the `*_nvidia_driver_preference` from the
#      site configuration file (e.g., `x1_nvidia_driver_preference`).
#    - CRITICAL INTERACTION: If `*_nvidia_driver_preference` is "nouveau"
#      BUT `/etc/modprobe.d/blacklist-nouveau.conf` exists (from `gpu-nds`),
#      `nouveau` will likely fail to load. `gpu-pta` includes a warning for this.
#      In such cases, the preference should ideally be "nvidia".
#
# 5. `gpu-ptd` (Detach GPU from Host - Dynamic):
#    - Dynamically unbinds the current host driver from the GPU.
#    - Dynamically binds `vfio-pci` to the GPU.
#    - Does NOT modify any files in `/etc/modprobe.d/`. Changes are not
#      persistent across reboots unless `gpu-pt3 enable` was also used.
#
# General Workflow Example (NVIDIA GPU for Passthrough, Proprietary on Host):
#   a. Run `gpu-nds` once to install NVIDIA drivers and set them as host default. Reboot.
#   b. Run `gpu-pt1`, `gpu-pt2` for initial passthrough setup. Reboot as prompted.
#   c. Run `gpu-pt3 enable` to assign the GPU to `vfio-pci` and blacklist host drivers
#      (including `nvidia`) for passthrough. Reboot. GPU is now ready for VM.
#   d. To return GPU to host: Run `gpu-pt3 disable`. Reboot. The NVIDIA proprietary
#      driver should now claim the GPU (due to `blacklist-nouveau.conf` from `gpu-nds`).
#   e. For dynamic switching without reboot (after `gpu-pt3 enable` has been done once):
#      - `gpu-ptd`: Detach from host, make available for VM.
#      - `gpu-pta`: Attach back to host (will use NVIDIA driver if preference is set
#        and `blacklist-nouveau.conf` exists).
#
# ============================================================================

# Define directory and file variables
DIR_FUN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
FILE_FUN=$(basename "$BASH_SOURCE")
BASE_FUN="${FILE_FUN%.*}"
FILEPATH_FUN="${DIR_FUN}/${FILE_FUN}"
CONFIG_FUN="${SITE_CONFIG_FILE}"

# Dynamically create variables based on the base name
eval "FILEPATH_${BASE_FUN}=\$FILEPATH_FUN"
eval "FILE_${BASE_FUN}=\$FILE_FUN"
eval "BASE_${BASE_FUN}=\$BASE_FUN"
eval "CONFIG_${BASE_FUN}=\$CONFIG_FUN"

# Shows a summary of selected functions in the script, displaying their usage, shortname, and description
# overview functions
#
gpu-fun() {

    # Technical Description:
    #   Lists selected functions from this script.
    #   Displays usage, shortname, and description for each.
    #   Likely parses comments or uses a predefined list.
    # Dependencies:
    #   - Relies on conventions for function comments if parsing.

    # Pass all arguments directly to aux-laf
    aux-laf "$FILEPATH_gpu" "$@"
}

# Displays an overview of specific variables defined in the configuration file, showing their names, values, and usage across different files
# overview variables
#
gpu-var() {

    # Technical Description:
    #   Shows details of configuration variables.
    #   Information includes name, value, and usage context.
    #   Likely reads from SITE_CONFIG_FILE and parses it.
    # Dependencies:
    #   - SITE_CONFIG_FILE environment variable.
    #   - Text processing tools (e.g. `grep`, `awk`).

    aux-acu -o "$CONFIG_gpu" "$DIR_FUN/.."
}

# Downloads and installs NVIDIA drivers, blacklisting Nouveau
# nvidia driver setup
# <driver_version> (optional)
gpu-nds() {
  local drv_ver="${1:-550.142}"  # default driver version; override as needed
  local url="https://us.download.nvidia.com/XFree86/Linux-x86_64/${drv_ver}/NVIDIA-Linux-x86_64-${drv_ver}.run"
  local installer="NVIDIA-Linux-x86_64-${drv_ver}.run"

  # Technical Description:
  #   Downloads a specific NVIDIA driver version (default or specified).
  #   Blacklists the Nouveau driver by creating a modprobe.d configuration file.
  #   Updates the initramfs.
  #   Installs prerequisite packages (dkms, build-essential, pve-headers).
  #   Makes the downloaded driver installer executable.
  #   Runs the NVIDIA installer silently with DKMS support.
  #   Prompts the user to reboot.
  # Dependencies:
  #   - `tee`, `update-initramfs`, `apt`, `wget`, `chmod`, `reboot`.
  #   - Root privileges for most operations.
  #   - Internet connectivity for downloading the driver.
  #   - `uname -r` to get the current kernel version for PVE headers.

  echo "1) Blacklisting Nouveau..."
  cat <<EOF | sudo tee /etc/modprobe.d/blacklist-nouveau.conf
blacklist nouveau
options nouveau modeset=0
EOF
  sudo update-initramfs -u   # rebuild initramfs

  echo "2) Installing prerequisites..."
  sudo apt update
  sudo apt install -y dkms build-essential pve-headers-$(uname -r)   # DKMS & kernel headers

  echo "3) Downloading NVIDIA driver ${drv_ver}..."
  wget -q "${url}" -O "${installer}" || { echo "Download failed"; return 1; }

  chmod +x "${installer}"

  echo "4) Installing driver with DKMS support..."
  sudo ./"${installer}" --dkms --silent  # silent install, auto DKMS

  echo "5) Finalizingâ€”reboot recommended."
  read -p "Reboot now? [y/N] " yn
  if [[ "$yn" =~ ^[Yy]$ ]]; then
    sudo reboot
  else
    echo "You can reboot later with: sudo reboot"
  fi

  echo "6) After reboot, verify with: nvidia-smi"   # verify driver loaded
}


# Configures initial GRUB and EFI settings for GPU passthrough, installs necessary packages, and reboots the system
# gpu passthrough step 1
#
gpu-pt1() {
    local function_name="${FUNCNAME[0]}"
    echo "Executing section 1:"

    # Display EFI boot information
    efibootmgr -v

    # Edit GRUB configuration
    sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="quiet"/GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt"/' /etc/default/grub
    update-grub
    update-grug2

    # Technical Description:
    #   Installs 'grub-efi-amd64' package for EFI systems.
    #   Notifies completion using 'printf', using 'function_name'.
    #   Forces a system reboot. This is step 1 of GPU passthrough setup.
    # Dependencies:
    #   - `apt` command for package installation.
    #   - `printf` function (for notifications).
    #   - `reboot` command.
    #   - Root privileges for apt and reboot.

    # Install grub-efi-amd64
    apt install grub-efi-amd64 -y

    # Notify status
    printf "%s: Completed section 1, system will reboot now.\\n" "$function_name"

    # Perform system reboot without prompting
    reboot
}

# Adds necessary kernel modules for GPU passthrough to /etc/modules, updates initramfs, and reboots the system
# gpu passthrough step 2
#
gpu-pt2() {
    local function_name="${FUNCNAME[0]}"
    echo "Executing section 2:"

    # Technical Description:
    #   Appends 'vfio', 'vfio_iommu_type1', 'vfio_pci' to /etc/modules.
    #   Updates initramfs for all kernels. Notifies completion via 'printf'.
    #   Forces a system reboot. This is step 2 of GPU passthrough setup.
    # Dependencies:
    #   - `echo` (to modify /etc/modules), `update-initramfs`.
    #   - `printf` function.
    #   - `reboot` command.
    #   - Root privileges for file edits and reboot.
    #   - Assumes /etc/modules is the correct file for loading modules.

    # Add modules to /etc/modules
    echo "vfio" >> /etc/modules
    echo "vfio_iommu_type1" >> /etc/modules
    echo "vfio_pci" >> /etc/modules

    # Update initramfs
    update-initramfs -u -k all

    # Notify status
    printf "%s: Completed section 2, system will reboot now.\\n" "$function_name"

    # Perform system reboot without prompting
    reboot
}

# Finalizes or reverts GPU passthrough setup by configuring or removing VFIO-PCI IDs and blacklisting specific GPU drivers
# gpu passthrough step 3
# <enable|disable>
gpu-pt3() {
    local action="$1"
    local vfio_conf="/etc/modprobe.d/vfio.conf"
    # Use a distinct blacklist file for passthrough-specific blacklisting
    local passthrough_blacklist_conf="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"
    # local blacklist_conf_generic="/etc/modprobe.d/blacklist.conf" # No longer primarily used by this function for GPU blacklisting
    local modules_file="/etc/modules"
    # local pci_ids=() # Not directly used, vendor_device_ids are used
    local all_gpu_pci_ids_details=() # Store "vendor_id:device_id pci_slot_id"
    local nvidia_gpus_for_vfio=false
    local amd_gpus_for_vfio=false
    local function_name="${FUNCNAME[0]}"


    # Technical Description:
    #   Configures or reverts GPU passthrough.
    #   'enable':
    #     - Identifies NVIDIA and AMD GPU PCI IDs.
    #     - Writes these IDs to /etc/modprobe.d/vfio.conf for vfio-pci.
    #     - Blacklists nouveau, nvidia, radeon, amdgpu drivers in a dedicated
    #       passthrough blacklist file (/etc/modprobe.d/zz-vfio-gpu-blacklist.conf)
    #       if corresponding GPUs are assigned to vfio-pci.
    #     - Ensures vfio modules are in /etc/modules.
    #   'disable':
    #     - Removes /etc/modprobe.d/vfio.conf.
    #     - Removes the passthrough-specific blacklist file.
    #     - Comments out vfio modules in /etc/modules.
    #   Prompts for reboot after changes.
    # Dependencies:
    #   - lspci
    #   - /etc/modprobe.d/, /etc/modules
    #   - Sudo privileges for file modifications and initramfs update.
    # Interactions:
    #   - Works in conjunction with gpu-pt1 and gpu-pt2 for full passthrough setup.
    #   - `gpu-nds` creates a separate blacklist for nouveau for host NVIDIA preference,
    #     which is not touched by `gpu-pt3 disable`.
    #   - `gpu-pta` and `gpu-ptd` handle dynamic (non-persistent) driver binding.

    if [ -z "$action" ]; then
        printf "Usage: %s <enable|disable>\\n" "$function_name"
        return 1
    fi
    
    # Define colors using printf, similar to lo1 module
    local YELLOW=$(printf '\\033[0;33m')
    local NC=$(printf '\\033[0m') # No Color

    printf "Fetching GPU PCI IDs and Vendor IDs...\\n"
    local lspci_output
    lspci_output=$(lspci -nn)
    while IFS= read -r line; do
        if echo "$line" | grep -qE "VGA compatible controller|3D controller"; then
            local pci_slot_id=$(echo "$line" | awk '{print $1}')
            # Correctly extract vendor:device ID, e.g., 10de:1f82 from [10de:1f82]
            local vendor_device_id=$(echo "$line" | grep -oP '\[\K[0-9a-fA-F]{4}:[0-9a-fA-F]{4}(?=\])')
            if [ -n "$vendor_device_id" ]; then
                 all_gpu_pci_ids_details+=("$vendor_device_id $pci_slot_id")
            else
                printf "${YELLOW}Warning: Could not parse vendor/device ID for line: %s${NC}\\n" "$line"
            fi
        fi
    done <<< "$lspci_output"

    if [ ${#all_gpu_pci_ids_details[@]} -eq 0 ]; then
        printf "No GPU devices found. Nothing to do.\\n"
        return 0
    fi

    if [ "$action" == "enable" ]; then
        printf "Configuring GPU passthrough (vfio-pci)...\\n"
        local vfio_options_line="options vfio-pci ids="
        local ids_to_add=""

        for detail in "${all_gpu_pci_ids_details[@]}"; do
            local vendor_device_id=$(echo "$detail" | awk '{print $1}')
            local pci_slot_id=$(echo "$detail" | awk '{print $2}') 
            local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)

            printf "Found GPU: %s with ID %s\\n" "$pci_slot_id" "$vendor_device_id"
            # For now, assume all detected GPUs are intended for passthrough.
            # A more advanced version could allow selecting specific GPUs.
            if [ -n "$ids_to_add" ]; then
                ids_to_add+=","
            fi
            ids_to_add+="$vendor_device_id"

            if [ "$vendor_id" == "10de" ]; then # NVIDIA
                nvidia_gpus_for_vfio=true
            elif [ "$vendor_id" == "1002" ]; then # AMD
                amd_gpus_for_vfio=true
            fi
        done

        if [ -z "$ids_to_add" ]; then
            printf "${YELLOW}No specific GPU IDs identified to add to %s. This might be an error or no supported GPUs found.${NC}\\n" "$vfio_conf"
        else
            vfio_options_line+="$ids_to_add"
            printf "Ensuring %s exists and adding/updating GPU IDs...\\n" "$vfio_conf"
            echo "$vfio_options_line" | sudo tee "$vfio_conf" > /dev/null
            printf "Content of %s:\\n" "$vfio_conf"
            sudo cat "$vfio_conf"
        fi

        printf "Ensuring GPU drivers are blacklisted for passthrough in %s...\\n" "$passthrough_blacklist_conf"
        # Create or clear the passthrough-specific blacklist file
        sudo rm -f "$passthrough_blacklist_conf"
        sudo touch "$passthrough_blacklist_conf" # Ensure it exists even if no drivers are blacklisted

        if $nvidia_gpus_for_vfio; then
            printf "NVIDIA GPU(s) targeted for passthrough. Blacklisting nouveau and nvidia in %s.\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist nouveau"
                echo "options nouveau modeset=0" # Important for nouveau
                echo "blacklist nvidia"
            } | sudo tee -a "$passthrough_blacklist_conf" > /dev/null
        elif lspci -nn | grep -qE "VGA compatible controller.*\[10de:" || lspci -nn | grep -qE "3D controller.*\[10de:"; then
             # This case handles if no NVIDIA GPUs were explicitly added to vfio.conf (e.g. ids_to_add was empty for NVIDIA)
             # but NVIDIA GPUs are present on the system. This is a fallback/general blacklisting.
             printf "${YELLOW}General NVIDIA GPU(s) detected on system (though not specifically added to %s by this run).${NC}\\n" "$vfio_conf"
             printf "${YELLOW}Adding precautionary blacklist for nouveau and nvidia in %s.${NC}\\n" "$passthrough_blacklist_conf"
             {
                echo "blacklist nouveau"
                echo "options nouveau modeset=0"
                echo "blacklist nvidia"
            } | sudo tee -a "$passthrough_blacklist_conf" > /dev/null
        fi

        if $amd_gpus_for_vfio; then
            printf "AMD GPU(s) targeted for passthrough. Blacklisting radeon and amdgpu in %s.\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist radeon"
                echo "blacklist amdgpu"
            } | sudo tee -a "$passthrough_blacklist_conf" > /dev/null
        elif lspci -nn | grep -qE "VGA compatible controller.*\[1002:" || lspci -nn | grep -qE "3D controller.*\[1002:"; then
            # Similar fallback for AMD
            printf "${YELLOW}General AMD GPU(s) detected on system (though not specifically added to %s by this run).${NC}\\n" "$vfio_conf"
            printf "${YELLOW}Adding precautionary blacklist for radeon and amdgpu in %s.${NC}\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist radeon"
                echo "blacklist amdgpu"
            } | sudo tee -a "$passthrough_blacklist_conf" > /dev/null
        fi
        
        if [ -s "$passthrough_blacklist_conf" ]; then # Check if file is not empty
            printf "Content of %s:\\n" "$passthrough_blacklist_conf"
            sudo cat "$passthrough_blacklist_conf"
        else
            printf "No specific drivers were blacklisted in %s during this run (e.g., no NVIDIA/AMD GPUs for VFIO or detected generally). File is empty or only contains comments.\\n" "$passthrough_blacklist_conf"
        fi


        printf "Ensuring vfio modules are loaded at boot via %s...\\n" "$modules_file"
        for module in vfio vfio_iommu_type1 vfio_pci; do
            if ! sudo grep -qP "^\s*${module}\s*(#.*)?$" "$modules_file"; then # Check if module exists, possibly commented
                echo "$module" | sudo tee -a "$modules_file" > /dev/null
                printf "Added %s to %s.\\n" "$module" "$modules_file"
            else
                # Ensure not commented out
                if sudo grep -qP "^\s*#\s*${module}" "$modules_file"; then
                    sudo sed -i "s/^\s*#\s*${module}/${module}/" "$modules_file"
                    printf "Uncommented %s in %s.\\n" "$module" "$modules_file"
                else
                    printf "%s already correctly listed in %s.\\n" "$module" "$modules_file"
                fi
            fi
        done

    elif [ "$action" == "disable" ]; then
        printf "Reverting GPU passthrough configuration...\\n"
        printf "Removing %s...\\n" "$vfio_conf"
        sudo rm -f "$vfio_conf"
        printf "Removing passthrough-specific blacklist file %s...\\n" "$passthrough_blacklist_conf"
        sudo rm -f "$passthrough_blacklist_conf"
        # Note: We do NOT touch /etc/modprobe.d/blacklist-nouveau.conf if created by gpu-nds

        printf "Commenting out vfio modules in %s (they can be auto-loaded if needed)...\\n" "$modules_file"
        for module in vfio vfio_iommu_type1 vfio_pci; do
            if sudo grep -qP "^\s*${module}" "$modules_file"; then # If module is present and not commented
                sudo sed -i "s/^\s*${module}/# ${module}/" "$modules_file"
                printf "Commented out %s in %s.\\n" "$module" "$modules_file"
            else
                printf "%s not found or already commented in %s.\\n" "$module" "$modules_file"
            fi
        done
    else
        printf "Invalid action: %s. Use 'enable' or 'disable'.\\n" "$action"
        return 1
    fi

    printf "Updating initramfs...\\n"
    sudo update-initramfs -u -k all # Use -k all to update all kernels, or just -u for current
    printf "Configuration applied. A reboot is required for changes to take full effect.\\n"
}

# Detaches the GPU from the host system, making it available for VM passthrough
# gpu passthrough detach
#
gpu-ptd() {
    local function_name="${FUNCNAME[0]}"
    echo "--- Debug: Entering ${function_name} ---"

    # Technical Description:
    #   Detaches GPU(s) from host drivers for VM passthrough.
    #   Unloads standard GPU drivers (nouveau, nvidia, amdgpu, radeon).
    #   Loads \'vfio-pci\' driver. Identifies GPUs via \'lspci -nn\'.
    #   For each GPU: unbinds, sets \'driver_override\' to \'vfio-pci\', binds to \'vfio-pci\'.
    #   Uses temp files for \'lspci\' outputs. Notifies via \'printf\'.
    # Dependencies:
    #   - `lspci`, `grep`, `find`, `ls`, `modprobe`, `awk`, `echo`.
    #   - `printf` function.
    #   - Access to /sys/kernel/iommu_groups/, /sys/bus/pci/devices/ & root privileges.

    printf "INFO: Starting GPU detachment process\\\\n"
    echo "--- Debug: Starting GPU detachment process ---"

    printf "INFO: Current GPU driver and IOMMU group (before detachment):\\\\n"
    echo "--- Debug: Current GPU driver and IOMMU group (before detachment) ---"
    LSPCI_NNK_TEMP_OUTPUT_1="/tmp/lspci_nnk_output_ptd1.txt"
    if lspci -nnk > "$LSPCI_NNK_TEMP_OUTPUT_1"; then
        grep -A3 "VGA compatible controller" "$LSPCI_NNK_TEMP_OUTPUT_1" | while read -r line; do
            printf "INFO: %s\\\\n" "$line"
            echo "--- Debug: Initial GPU info: $line ---"
        done
    else
        printf "ERROR: lspci -nnk command failed (first call).\\\\n"
        echo "Error: lspci -nnk command failed (first call)."
    fi
    [ -f "$LSPCI_NNK_TEMP_OUTPUT_1" ] && rm -f "$LSPPCI_NNK_TEMP_OUTPUT_1"

    echo "--- Debug: Unloading NVIDIA or AMD drivers ---"
    for driver in nouveau nvidia amdgpu radeon; do
        if lsmod | grep -q $driver; then
            printf "INFO: Unloading $driver driver\\\\n"
            echo "--- Debug: Unloading $driver driver ---"
            if ! modprobe -r $driver; then
                printf "WARNING: Failed to unload $driver driver. Continuing anyway.\\\\n"
                printf "%s: Warning: Failed to unload $driver driver. Continuing anyway.\\\\n" "$function_name"
            else
                printf "INFO: Successfully unloaded $driver driver\\\\n"
            fi
        else
            printf "INFO: $driver driver not loaded.\\\\n"
            echo "--- Debug: $driver driver not loaded. ---"
        fi
    done

    printf "INFO: Loading VFIO-PCI driver\\\\n"
    echo "--- Debug: Loading VFIO driver ---"
    if ! modprobe vfio-pci; then
        printf "ERROR: Failed to load VFIO-PCI driver. GPU detachment may fail.\\\\n"
        printf "%s: Error: Failed to load VFIO-PCI driver. GPU detachment may fail.\\\\n" "$function_name"
        echo "--- Debug: Exiting ${function_name} due to modprobe vfio-pci failure ---"
        return 1
    fi
    printf "INFO: VFIO-PCI driver loaded successfully.\\\\n"
    echo "--- Debug: VFIO-PCI driver loaded. ---"

    echo "--- Debug: Getting GPU PCI IDs ---"
    local gpu_ids="" # Initialize to empty
    local lspci_output_file="" # Ensure it's defined for rm -f robustness
    lspci_output_file=$(mktemp /tmp/lspci_nn_gpu_ids_ptd.XXXXXX)

    # Check if mktemp succeeded and created a file
    if [ -z "$lspci_output_file" ] || ! [ -e "$lspci_output_file" ]; then
        printf "ERROR: Failed to create temporary file for lspci output (mktemp failed).\\\\n"
        printf "%s: Error: Failed to create temporary file for lspci output.\\\\n" "$function_name"
        echo "--- Debug: mktemp failed for lspci_output_file. ---"
        # gpu_ids remains empty, will be caught by the check after this block
    elif ! lspci -nn > "$lspci_output_file"; then
        printf "ERROR: lspci -nn command failed (for gpu_ids extraction).\\\\n"
        printf "%s: Error: lspci -nn command failed.\\\\n" "$function_name"
        echo "Error: lspci -nn command failed (for gpu_ids extraction in gpu-ptd)."
        # gpu_ids remains empty
    else
        # lspci succeeded, now try to extract IDs
        local gpu_ids_output
        gpu_ids_output=$(grep -iE "VGA compatible controller|3D controller" "$lspci_output_file" | awk '{print $1}')
        local awk_status=$? # Status of awk (last command in pipe)

        if [ $awk_status -ne 0 ]; then
            # This case implies awk itself had an error or was passed no input from a failed grep with pipefail
            printf "ERROR: awk command failed while processing lspci output (status: %s).\\nPotentially empty output from grep: %s\\\\n" "$awk_status" "$gpu_ids_output"
            printf "%s: Error: awk command failed during GPU ID extraction.\\\\n" "$function_name"
            # gpu_ids remains empty
        elif [ -z "$gpu_ids_output" ]; then
            # awk succeeded (status 0), but grep found no matching lines, so output is empty.
            printf "INFO: No GPU devices (VGA compatible controller or 3D controller) found in lspci output.\\\\n"
            echo "--- Debug: grep found no relevant GPU devices in lspci output. ---"
            # gpu_ids remains empty
        else
            # Both lspci, grep, and awk seem to have worked, and we have output
            gpu_ids="$gpu_ids_output"
            printf "INFO: Found GPU(s) with PCI ID(s): %s\\\\n" "$gpu_ids"
            echo "--- Debug: Found GPU IDs: $gpu_ids ---"
        fi
    fi

    # Clean up the temporary file if it was created
    if [ -n "$lspci_output_file" ] && [ -f "$lspci_output_file" ]; then
        rm -f "$lspci_output_file"
    fi

    if [ -z "$gpu_ids" ]; then
        printf "ERROR: No GPU found after lspci processing.\\\\n"
        printf "%s: Error: No GPU found after lspci processing.\\\\n" "$function_name"
        echo "--- Debug: Exiting ${function_name} as no GPU IDs found ---"
        return 1
    fi

    printf "INFO: Processing GPU IDs for detachment: %s\\\\n" "$gpu_ids"
    echo "--- Debug: Processing GPU IDs for detachment: $gpu_ids ---"
    for id in $gpu_ids; do
        printf "INFO: Processing GPU with PCI ID: %s\\\\n" "$id"
        echo "--- Debug: Processing GPU ID: $id ---"
        if [ -e "/sys/bus/pci/devices/0000:$id/driver" ]; then
            printf "INFO: Unbinding GPU %s from current driver\\\\n" "$id"
            echo "--- Debug: Unbinding 0000:$id from current driver ---"
            if echo "0000:$id" > "/sys/bus/pci/devices/0000:$id/driver/unbind"; then
                printf "INFO: Successfully unbound GPU %s\\\\n" "$id"
                echo "--- Debug: Successfully unbound GPU $id ---"
            else
                printf "WARNING: Failed to unbind GPU %s\\\\n" "$id"
                echo "--- Debug: Failed to unbind GPU $id ---"
            fi
        else
            printf "INFO: GPU %s is not bound to any driver\\\\n" "$id"
            echo "--- Debug: No driver found for 0000:$id, skipping unbind ---"
        fi
        
        printf "INFO: Setting driver_override to vfio-pci for GPU %s\\\\n" "$id"
        echo "--- Debug: Setting driver_override to vfio-pci for 0000:$id ---"
        if ! echo "vfio-pci" > "/sys/bus/pci/devices/0000:$id/driver_override"; then
            printf "WARNING: Failed to set driver_override to vfio-pci for GPU %s\\\\n" "$id"
            echo "--- Debug: Failed to set driver_override for $id ---"
        fi

        printf "INFO: Binding GPU %s to vfio-pci driver\\\\n" "$id"
        echo "--- Debug: Binding 0000:$id to vfio-pci ---"
        if ! echo "0000:$id" > "/sys/bus/pci/drivers/vfio-pci/bind"; then
            printf "ERROR: Failed to bind GPU %s to VFIO-PCI\\\\n" "$id"
            echo "--- Debug: Failed to bind GPU $id to VFIO-PCI ---"
        else
            printf "INFO: Successfully bound GPU %s to VFIO-PCI\\\\n" "$id"
            echo "--- Debug: Successfully bound GPU $id to VFIO-PCI ---"
        fi
    done

    printf "INFO: GPU driver and IOMMU group after detachment:\\\\n"
    echo "--- Debug: GPU driver and IOMMU group (after detachment) ---"
    LSPCI_NNK_TEMP_OUTPUT_2="/tmp/lspci_nnk_output_ptd2.txt"
    if lspci -nnk > "$LSPCI_NNK_TEMP_OUTPUT_2"; then
        grep -A3 "VGA compatible controller" "$LSPCI_NNK_TEMP_OUTPUT_2" | while read -r line; do
            printf "INFO: %s\\\\n" "$line"
            echo "--- Debug: Final GPU info: $line ---"
        done
    else
        printf "ERROR: lspci -nnk command failed (second call).\\\\n"
        echo "Error: lspci -nnk command failed (second call)."
    fi
    [ -f "$LSPCI_NNK_TEMP_OUTPUT_2" ] && rm -f "$LSPCI_NNK_TEMP_OUTPUT_2"

    printf "INFO: GPU detachment process completed.\\\\n"
    printf "%s: GPU detachment process completed. Check logs for details.\\\\n" "$function_name"
    echo "--- Debug: Exiting ${function_name} ---"
}

# Attaches the GPU back to the host system
# gpu passthrough attach
#
gpu-pta() {
    local function_name="${FUNCNAME[0]}"
    echo "--- Debug: Entering ${function_name} ---"

    local current_hostname
    current_hostname=$(hostname -s) # Get short hostname like x1, x2
    local preferred_nvidia_driver="nouveau" # Default preference
    
    # Define colors using printf, similar to lo1 module
    local YELLOW=$(printf '\\033[0;33m')
    local RED=$(printf '\\033[0;31m')
    local NC=$(printf '\\033[0m') # No Color


    # Construct the preference variable name, e.g., x1_nvidia_driver_preference
    local preference_var_name="${current_hostname}_nvidia_driver_preference"
    local global_default_preference_var="default_nvidia_driver_preference" # Optional global default

    # Check if the node-specific preference is set in the config
    if [ -n "${!preference_var_name}" ]; then
        if [[ "${!preference_var_name}" == "nvidia" || "${!preference_var_name}" == "nouveau" ]]; then
            preferred_nvidia_driver="${!preference_var_name}"
            echo "--- Debug: Using node-specific NVIDIA driver preference for $current_hostname: $preferred_nvidia_driver ---"
        else
            echo "--- Debug: Invalid value for $preference_var_name: ${!preference_var_name}. Checking global default. ---"
            if [ -n "${!global_default_preference_var}" ] && ([[ "${!global_default_preference_var}" == "nvidia" || "${!global_default_preference_var}" == "nouveau" ]]); then
                preferred_nvidia_driver="${!global_default_preference_var}"
                echo "--- Debug: Using global default NVIDIA driver preference: $preferred_nvidia_driver ---"
            else
                echo "--- Debug: No valid global default. Using hardcoded default: $preferred_nvidia_driver. ---"
            fi
        fi
    elif [ -n "${!global_default_preference_var}" ] && ([[ "${!global_default_preference_var}" == "nvidia" || "${!global_default_preference_var}" == "nouveau" ]]); then
        preferred_nvidia_driver="${!global_default_preference_var}"
        echo "--- Debug: No specific preference for $current_hostname. Using global default NVIDIA driver preference: $preferred_nvidia_driver ---"
    else
        echo "--- Debug: No specific NVIDIA driver preference found for $current_hostname ($preference_var_name) and no global default. Using hardcoded default: $preferred_nvidia_driver. ---"
    fi

    # Technical Description:
    #   Attaches GPU(s) back to the host system.
    #   Identifies GPU PCI slot IDs.
    #   For each GPU:
    #     - Unbinds from vfio-pci (if bound).
    #     - Determines appropriate host driver (amdgpu, or nvidia/nouveau based on site config).
    #     - Binds the host driver.
    #     - Rescans PCI devices.
    # Dependencies:
    #   - lspci, /sys/bus/pci/drivers/
    #   - Sudo privileges for driver binding/unbinding.
    # Interactions:
    #   - Uses `*_nvidia_driver_preference` from site config for NVIDIA GPUs.
    #   - If `nouveau` is preferred but blacklisted by `gpu-nds`
    #     (via /etc/modprobe.d/blacklist-nouveau.conf), `nouveau` may fail to load.
    #     A warning is issued in this case.

    local gpu_pci_slots=()
    local pci_slot_id
    local driver
    local vendor_id

    printf "Identifying GPU PCI slots...\\n"
    while IFS= read -r line; do
        if echo "$line" | grep -qE "VGA compatible controller|3D controller"; then
            pci_slot_id=$(echo "$line" | awk '{print $1}')
            gpu_pci_slots+=("$pci_slot_id")
            printf "Found GPU at PCI slot: %s\\n" "$pci_slot_id"
        fi
    done < <(lspci -nn)

    if [ ${#gpu_pci_slots[@]} -eq 0 ]; then
        printf "No GPU devices found to attach.\\n"
        return 1
    fi

    for pci_slot_id in "${gpu_pci_slots[@]}"; do
        printf "Processing GPU at %s...\\n" "$pci_slot_id"

        # Unbind from vfio-pci if it's bound
        if [ -e "/sys/bus/pci/drivers/vfio-pci/$pci_slot_id" ]; then
            printf "Unbinding %s from vfio-pci...\\n" "$pci_slot_id"
            echo "$pci_slot_id" | sudo tee /sys/bus/pci/drivers/vfio-pci/unbind > /dev/null
        else
            printf "GPU %s not currently bound to vfio-pci.\\n" "$pci_slot_id"
        fi

        # Determine the correct driver
        vendor_id=$(lspci -n -s "$pci_slot_id" | awk '{print $3}' | cut -d':' -f1)
        case "$vendor_id" in
            1002) # AMD
                driver="amdgpu" # Or radeon, but amdgpu is more common for modern cards
                ;;
            10de) # NVIDIA
                driver="$preferred_nvidia_driver"
                echo "--- Debug: NVIDIA card detected. Using preferred driver from config: $driver ---"
                if [ "$driver" == "nouveau" ] && [ -f "/etc/modprobe.d/blacklist-nouveau.conf" ]; then
                    printf "${YELLOW}WARNING: Preferred driver for %s is 'nouveau', but /etc/modprobe.d/blacklist-nouveau.conf exists (likely from gpu-nds).${NC}\\n" "$current_hostname"
                    printf "${YELLOW}         'nouveau' may fail to load. Consider setting %s_nvidia_driver_preference to 'nvidia' in your site config,${NC}\\n" "$current_hostname"
                    printf "${YELLOW}         or ensure /etc/modprobe.d/blacklist-nouveau.conf is removed if 'nouveau' is truly intended for host use.${NC}\\n"
                fi
                ;;
            *)
                printf "${RED}ERROR: Unknown GPU vendor ID: %s for PCI slot %s. Cannot determine driver.${NC}\\n" "$vendor_id" "$pci_slot_id"
                continue
                ;;
        esac

        printf "Attempting to bind %s to %s driver...\\n" "$pci_slot_id" "$driver"
        echo "$pci_slot_id" | sudo tee "/sys/bus/pci/drivers/${driver}/bind" > /dev/null 2>&1
        if [ $? -eq 0 ]; then
            printf "Successfully bound %s to %s.\\n" "$pci_slot_id" "$driver"
        else
            printf "${RED}ERROR: Failed to bind %s to %s. Driver may not be loaded or GPU not supported by it.${NC}\\n" "$pci_slot_id" "$driver"
            printf "${RED}       Check dmesg for errors related to the driver or PCI ID %s.${NC}\\n" "$pci_slot_id"
        fi
    done

    printf "Rescanning PCI devices (may not be necessary for all driver changes)...\\n"
    echo 1 | sudo tee /sys/bus/pci/rescan > /dev/null

    echo "--- Debug: Exiting ${function_name} ---"
    printf "GPU attachment process complete. Check 'gpu-pts' or 'lspci -nnk' for status.\\n"
}

# Checks the current status of the GPU
# gpu passthrough status
#
gpu-pts() {
    local function_name="${FUNCNAME[0]}"
    # Corrected flags:
    local is_any_gpu_on_vfio=false      # True if lspci -nnk shows a VGA/3D controller using vfio-pci
    local host_drivers_for_gpu=""     # Name(s) of host driver(s) if a VGA/3D controller is using one
    local vga_devices_processed=false # Flag to check if any VGA/3D device block was processed

    # Define colors using printf, similar to lo1 module
    local CYAN=$(printf '\\033[0;36m')
    local MAGENTA=$(printf '\\033[0;35m') # Magenta is often perceived as pinkish-red
    local YELLOW=$(printf '\\033[0;33m')
    local INDIGO_BLUE=$(printf '\\033[38;2;75;0;130m') # True color for Indigo Blue
    local RED=$(printf '\\033[0;31m')
    local NC=$(printf '\\033[0m') # No Color

    echo "--- GPU Passthrough Status ---"
    echo -e "=============================="

    # Store VFIO-PCI module status for later display (informational only)
    local vfio_pci_loaded_info=""
    if lsmod | grep -q "vfio_pci"; then
        vfio_pci_loaded_info="[INFO] ${CYAN}VFIO-PCI module is loaded.${NC}"
    else
        vfio_pci_loaded_info="[INFO] ${MAGENTA}VFIO-PCI module is NOT loaded.${NC}"
    fi

    echo -e "\\n--- Kernel Drivers for GPU(s) ---"
    # Process lspci -nnk output to determine drivers for VGA/3D controllers
    local temp_lspci_gpu_blocks="/tmp/gpu_pts_blocks.$$"
    # Grep for VGA/3D controllers and a few lines of context.
    # Using -A4 as a heuristic for context; can be adjusted if needed.
    lspci -nnk | grep -A4 -iE "VGA compatible controller|3D controller" > "$temp_lspci_gpu_blocks"
    
    if [ -s "$temp_lspci_gpu_blocks" ]; then
        local current_gpu_block_content=""
        # Read the grep output line by line
        while IFS= read -r line_from_grep; do
            # Check if the line is a VGA/3D controller line, indicating start of a new block
            if echo "$line_from_grep" | grep -qE "VGA compatible controller|3D controller"; then
                # Process previous block if it exists and was for a GPU
                if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
                    echo "$current_gpu_block_content" # Print the full block for the user
                    vga_devices_processed=true
                    if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                        is_any_gpu_on_vfio=true
                    elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                        local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                        if [ -n "$driver_name" ]; then
                            if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                        fi
                    fi
                fi
                current_gpu_block_content="$line_from_grep" # Start new block
            elif [[ "$line_from_grep" == "--" ]]; then # grep -A separator, end of a block
                if [ -n "$current_gpu_block_content" ]; then
                    echo "$current_gpu_block_content" # Print the block
                    vga_devices_processed=true
                    if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                        is_any_gpu_on_vfio=true
                    elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                        local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                        if [ -n "$driver_name" ]; then
                           if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                        fi
                    fi
                    current_gpu_block_content="" # Reset block
                fi
                echo "$line_from_grep" # Print the "--" separator
            elif [ -n "$current_gpu_block_content" ]; then # Line is part of the current block
                current_gpu_block_content="$current_gpu_block_content"$'\\n'"$line_from_grep"
            fi
        done < "$temp_lspci_gpu_blocks"
        # Process the last accumulated block, if any
        if [ -n "$current_gpu_block_content" ]; then
            echo "$current_gpu_block_content"
            vga_devices_processed=true
            if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                is_any_gpu_on_vfio=true
            elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                if [ -n "$driver_name" ]; then
                    if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                fi
            fi
        fi
    else
        echo -e "${YELLOW}No VGA compatible or 3D controllers found by primary lspci grep.${NC}"
    fi
    rm -f "$temp_lspci_gpu_blocks"

    # Fallback check if the above parsing didn't set flags but devices exist
    if ! $vga_devices_processed && lspci -nnk | grep -qE "VGA compatible controller|3D controller"; then
        vga_devices_processed=true # Mark that devices are present
        echo -e "${YELLOW}[INFO] Primary parsing of GPU drivers was inconclusive; performing fallback check.${NC}"
        # A simpler check on the full output if primary parsing failed to set flags
        local full_lspci_check_output=$(lspci -nnk)
        if echo "$full_lspci_check_output" | grep -A4 "VGA compatible controller" | grep -q "Kernel driver in use: vfio-pci"; then
            is_any_gpu_on_vfio=true
        fi
        # Check for host drivers only if not on vfio
        if ! $is_any_gpu_on_vfio; then
            # This could be more sophisticated to find all host drivers
            local found_host_driver_names=$(echo "$full_lspci_check_output" | grep -A4 "VGA compatible controller" | grep "Kernel driver in use:" | awk -F': ' '{print $2}' | sort -u | tr '\\n' ',' | sed 's/,$//')
            if [ -n "$found_host_driver_names" ] && [ "$found_host_driver_names" != "vfio-pci" ]; then # ensure it's not vfio-pci again
                 host_drivers_for_gpu="$found_host_driver_names"
            fi
        fi
    fi
    if ! $vga_devices_processed ; then
         echo -e "${YELLOW}No VGA compatible or 3D controllers were processed by this script.${NC}"
    fi

    echo -e "\\n--- IOMMU Groups (Filtered for GPU(s)) ---"
    if ! command -v find &>/dev/null || ! command -v ls &>/dev/null || ! command -v lspci &>/dev/null; then
        echo "Required commands (find, ls, lspci) not available to list IOMMU groups."
    else
        local gpu_pci_bus_ids=()
        # Get PCI bus IDs (e.g., 01:00.0) of VGA/3D controllers
        while IFS= read -r line; do
            gpu_pci_bus_ids+=("$(echo "$line" | awk '{print $1}')")
        done < <(lspci -nn | grep -iE "VGA compatible controller|3D controller")

        if [ ${#gpu_pci_bus_ids[@]} -eq 0 ]; then
            echo "No VGA/3D controllers found to filter IOMMU groups."
        else
            echo "Monitoring for GPU(s) at PCI Bus IDs: ${gpu_pci_bus_ids[*]}"
            local overall_iommu_groups_found=false
            local gpu_related_group_displayed=false

            for iommu_group_dir in $(find /sys/kernel/iommu_groups/ -mindepth 1 -maxdepth 1 -type d 2>/dev/null | sort -V); do
                overall_iommu_groups_found=true
                local group_id=$(basename "$iommu_group_dir")
                local devices_in_group_output=""
                local this_group_contains_gpu=false

                # Iterate over devices to build output and check for GPU presence
                for device_symlink_name in $(ls -1 "$iommu_group_dir"/devices/ 2>/dev/null | sort -V); do
                    # device_symlink_name is like 0000:01:00.0
                    local device_pci_bus_id_full="$device_symlink_name"
                    # Strip leading "0000:" to get short bus ID like "01:00.0"
                    local device_pci_bus_id_short="${device_symlink_name#0000:}" 
                    
                    local device_info
                    device_info=$(lspci -nns "$device_pci_bus_id_full" 2>/dev/null)
                    
                    if [ -n "$device_info" ]; then
                        devices_in_group_output+="\\t${device_info}\\n"
                        # Check if this device is one of the target GPUs
                        for gpu_id in "${gpu_pci_bus_ids[@]}"; do
                            if [[ "$device_pci_bus_id_short" == "$gpu_id" ]]; then
                                this_group_contains_gpu=true
                                break 
                            fi
                        done
                    else
                        devices_in_group_output+="\\tError reading device info for $device_pci_bus_id_full\\n"
                    fi
                done

                if $this_group_contains_gpu; then
                    echo -e "IOMMU Group ${group_id}:"
                    # Remove last newline character from devices_in_group_output before printing
                    echo -e "${devices_in_group_output%\\n}" 
                    gpu_related_group_displayed=true
                fi
            done

            if ! $overall_iommu_groups_found; then
                echo "No IOMMU groups found by 'find' command."
            elif ! $gpu_related_group_displayed; then
                echo "No IOMMU groups found containing the identified GPU(s): ${gpu_pci_bus_ids[*]}"
            fi
        fi
    fi

    echo -e "\\n--- Loaded Kernel Modules (GPU related) ---"
    if lsmod | grep -E "vfio|nvidia|nouveau|amdgpu|radeon" --color=never; then
        : # lsmod already printed
    else
        echo "No common GPU-related modules (vfio, nvidia, nouveau, amdgpu, radeon) loaded."
    fi

    echo -e "\\n--- Default Boot Setting (persisted by gpu-pt3) ---"
    local vfio_conf_file="/etc/modprobe.d/vfio.conf"
    local blacklist_conf_file="/etc/modprobe.d/blacklist.conf"
    local pt3_default_state_msg=""

    # Check for vfio.conf and vfio-pci ids
    if [ -f "$vfio_conf_file" ] && grep -q -E "^options vfio-pci ids=" "$vfio_conf_file"; then
        pt3_default_state_msg="${INDIGO_BLUE}Persistent passthrough (gpu-pt3): ENABLED${NC}\\n"
        pt3_default_state_msg+="${INDIGO_BLUE}  - GPUs are likely assigned to vfio-pci at boot via $vfio_conf_file.${NC}"
        
        # Check blacklist status
        if [ -f "$blacklist_conf_file" ] && (grep -q "blacklist radeon" "$blacklist_conf_file" || grep -q "blacklist amdgpu" "$blacklist_conf_file"); then
            pt3_default_state_msg+="\\n${INDIGO_BLUE}  - Native GPU drivers (radeon/amdgpu) are blacklisted in $blacklist_conf_file.${NC}"
        else
            pt3_default_state_msg+="\\n${INDIGO_BLUE}  - Native GPU driver blacklisting in $blacklist_conf_file is NOT DETECTED or file is missing.${NC}"
        fi
    else
        pt3_default_state_msg="${RED}Persistent passthrough (gpu-pt3): DISABLED or NOT CONFIGURED${NC}\\n"
        pt3_default_state_msg+="${RED}  - $vfio_conf_file is missing or does not assign GPUs to vfio-pci.${NC}"

        # Check blacklist status even if vfio.conf is not set for passthrough (could be an inconsistent state)
        if [ -f "$blacklist_conf_file" ] && (grep -q "blacklist radeon" "$blacklist_conf_file" || grep -q "blacklist amdgpu" "$blacklist_conf_file"); then
            pt3_default_state_msg+="\\n${RED}  - Note: Native GPU drivers might still be blacklisted in $blacklist_conf_file (check for inconsistency).${NC}"
        else
            pt3_default_state_msg+="\\n${RED}  - Native GPU driver blacklisting in $blacklist_conf_file also NOT DETECTED or file is missing.${NC}"
        fi
    fi
    echo -e "$pt3_default_state_msg"

    # Display the VFIO-PCI module status
    echo -e "\\n--- VFIO-PCI Module Status ---"
    echo -e "${vfio_pci_loaded_info}"

    # Now display the Overall GPU State Determination
    echo -e "\\n--- Overall GPU State Determination ---"
    if $is_any_gpu_on_vfio; then
        echo -e "${CYAN}Boolean DETACHED state: true (a GPU device is actively using vfio-pci).${NC}"
        echo -e "${CYAN}--- Summary: GPU appears to be DETACHED (for VM use) ---${NC}"
        echo -e "${CYAN}    - At least one GPU device (VGA/3D controller) is bound to vfio-pci.${NC}"
        if [ -n "$host_drivers_for_gpu" ]; then
             echo -e "${YELLOW}    - Note: Other GPU devices might be ATTACHED to host (driver(s): $host_drivers_for_gpu). State could be MIXED.${NC}"
        fi
    elif [ -n "$host_drivers_for_gpu" ]; then
        echo -e "${MAGENTA}Boolean DETACHED state: false (GPU is attached to host driver(s): $host_drivers_for_gpu).${NC}"
        echo -e "${MAGENTA}--- Summary: GPU appears to be ATTACHED to host (driver(s): $host_drivers_for_gpu) ---${NC}"
        echo -e "${MAGENTA}    - GPU device(s) (VGA/3D controller) are using host driver(s): $host_drivers_for_gpu.${NC}"
        echo -e "${MAGENTA}    - No VGA/3D controller found using vfio-pci.${NC}"
    else
        # No GPU on vfio, no GPU on host driver. Check if GPUs were found at all.
        if $vga_devices_processed || lspci -nnk | grep -qE "VGA compatible controller|3D controller"; then # Check if any VGA/3D controller was found
            echo -e "${YELLOW}Boolean DETACHED state: indeterminate (GPU found, but no driver identified as 'in use' by this script).${NC}"
            echo -e "${YELLOW}--- Summary: GPU state is UNCLEAR for VGA/3D controllers ---${NC}"
            echo -e "${YELLOW}    - A VGA/3D controller was detected, but it's not using 'vfio-pci' nor any other recognized host driver.${NC}"
            echo -e "${YELLOW}    - It might be unbound. Check 'Kernel Drivers for GPU(s)' section and IOMMU details.${NC}"
        else
            echo -e "${YELLOW}Boolean DETACHED state: N/A (No VGA/3D GPU controllers detected).${NC}"
            echo -e "${YELLOW}--- Summary: No VGA/3D GPU controllers detected by lspci -nnk ---${NC}"
        fi
        echo -e "${YELLOW}    - Review IOMMU groups and loaded modules above.${NC}"
    fi

    echo ""
    printf "%s: GPU status check completed.\\n" "$function_name"
    echo "---------------------------------"
}
