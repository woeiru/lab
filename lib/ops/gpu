#!/bin/bash

# ============================================================================
# gpu - Function Summary
#
#   gpu-fun : Shows a summary of selected functions in the script.
#   gpu-var : Displays an overview of specific variables defined in the configuration file.
#   gpu-nds : Downloads and installs NVIDIA drivers, blacklisting Nouveau for host use.
#   gpu-pt1 : Configures initial GRUB and EFI settings for GPU passthrough, installs packages, and reboots.
#   gpu-pt2 : Adds kernel modules for GPU passthrough, updates initramfs, and reboots.
#   gpu-pt3 : Finalizes or reverts GPU passthrough setup (VFIO-PCI IDs, blacklisting drivers for passthrough).
#   gpu-ptd : Detaches the GPU from the host system for VM passthrough (dynamic).
#   gpu-pta : Attaches the GPU back to the host system (dynamic).
#   gpu-pts : Checks the current status of the GPU passthrough setup.
#
# ============================================================================
#
# ============================================================================
# GPU Driver and Passthrough Function Interactions - Technical Overview
#
# This section outlines crucial interactions between different GPU management
# functions, particularly concerning driver blacklisting, preferences, and
# dynamic vs. persistent configurations.
#
# Persistent Configuration (Primarily via `gpu-pt1`, `gpu-pt2`, `gpu-pt3`):
#   - These functions establish the boot-time default state for GPU passthrough.
#   - `gpu-pt3 enable`: Configures the system to assign specified GPUs to `vfio-pci`
#     at boot, creating persistent blacklists for host drivers for those GPUs.
#     This is the most robust way to dedicate GPUs for passthrough.
#   - `gpu-pt3 disable`: Reverts the persistent passthrough configuration, allowing
#     host drivers to claim GPUs at boot (defaulting to host usage).
#
# Dynamic Switching (`gpu-ptd`, `gpu-pta`):
#   - These functions allow runtime switching of GPU control between the host and
#     `vfio-pci` (for VMs) *without requiring a reboot*.
#   - They are designed to be as self-sufficient as possible, meaning they can
#     often work even if the persistent state set by `gpu-pt3` is for host usage
#     (i.e., after `gpu-pt3 disable` or if `gpu-pt3 enable` was never run).
#   - `gpu-ptd` (Detach): Attempts to dynamically prepare the system for passthrough
#     by loading necessary VFIO modules, unbinding host drivers, and binding `vfio-pci`.
#   - `gpu-pta` (Attach): Attempts to dynamically return GPU control to the host by
#     unbinding `vfio-pci`, loading necessary host drivers, and binding them.
#
# Blacklist Files:
#   - `/etc/modprobe.d/blacklist-nouveau.conf`:
#       - Created/Managed by: `gpu-nds` (NVIDIA Driver Setup).
#       - Purpose: Persistently blacklists `nouveau` if NVIDIA proprietary drivers
#         are installed for host use.
#
#   - `/etc/modprobe.d/zz-vfio-gpu-blacklist.conf`:
#       - Created/Managed by: `gpu-pt3 enable`.
#       - Removed by: `gpu-pt3 disable`.
#       - Purpose: Persistently blacklists host drivers (`nouveau`, `nvidia`,
#         `amdgpu`, `radeon`) for GPUs intended for passthrough at boot.
#         `gpu-ptd` does *not* create this file but attempts to achieve a similar
#         effect for the current session by unloading host drivers.
#
# Key Interactions & Considerations:
#
# 1. `gpu-nds` (NVIDIA Driver Setup):
#    - Installs NVIDIA proprietary drivers and can blacklist `nouveau` for host use.
#    This is independent of passthrough but affects which driver (`nvidia` or `nouveau`)
#    `gpu-pta` might try to load for an NVIDIA GPU if it's the host preference.
#
# 2. `gpu-pt1`, `gpu-pt2` (Initial Passthrough Setup):
#    - These set up essential prerequisites like IOMMU kernel parameters and ensure
#      VFIO kernel modules are listed in `/etc/modules`.
#    - While `gpu-ptd` attempts to load VFIO modules dynamically, having these base
#      prerequisites met (especially IOMMU in kernel cmdline) is highly recommended
#      for `gpu-ptd` to succeed reliably.
#
# 3. `gpu-pt3 enable` (Persistent Passthrough):
#    - Provides the most stable environment for passthrough by ensuring GPUs are
#      bound to `vfio-pci` from boot and host drivers are blacklisted.
#    - If you primarily use GPUs for passthrough, this is the recommended default state.
#
# 4. `gpu-pt3 disable` (Persistent Host GPU Usage):
#    - Sets the system to use GPUs with host drivers by default after boot.
#    - This is the state from which `gpu-ptd` can be used to dynamically switch
#      a GPU to `vfio-pci` for a VM session.
#
# 5. `gpu-ptd` (Dynamic Detach for VM):
#    - Will attempt to load `vfio`, `vfio_iommu_type1`, and `vfio_pci` if not present.
#    - Will attempt to unbind host drivers (e.g., `nvidia`, `nouveau`, `amdgpu`).
#    - Uses `driver_override` to bind `vfio-pci` to the target GPU(s).
#    - Its changes are not persistent across reboots.
#    - If IOMMU is not enabled in the kernel, `gpu-ptd` will warn but cannot fix it.
#
# 6. `gpu-pta` (Dynamic Attach to Host):
#    - Unbinds `vfio-pci`.
#    - Clears `driver_override`.
#    - Attempts to load the preferred/appropriate host driver (e.g., `nvidia`, `nouveau`, `amdgpu`)
#      if not already loaded, and binds it to the GPU.
#    - Considers `*_nvidia_driver_preference` from site config for NVIDIA GPUs.
#      Warns if `nouveau` is preferred but blacklisted by `gpu-nds`.
#
# General Workflow Examples:
#
#   Initial One-Time Setup (Common for both scenarios below):
#     1. Run `gpu-pt1`: Configures GRUB/EFI for IOMMU, installs related packages.
#        Reboot as prompted.
#     2. Run `gpu-pt2`: Adds necessary kernel modules (vfio, vfio_iommu_type1, vfio_pci)
#        to /etc/modules for them to be loaded at boot, updates initramfs.
#        Reboot as prompted.
#     3. (Optional) Run `gpu-nds`: If using an NVIDIA GPU and proprietary drivers are
#        desired for host usage (when GPU is attached to host). This blacklists nouveau
#        and installs the NVIDIA driver. Reboot if prompted.
#
#   Scenario A: GPU Primarily for Passthrough (Persistent Setup)
#     (After completing the Initial One-Time Setup)
#     a. Run `gpu-pt3 enable`: Assigns specified GPU(s) to `vfio-pci` at boot.
#        This involves creating /etc/modprobe.d/vfio.conf with the GPU IDs and
#        blacklisting their host drivers (e.g., nvidia, amdgpu, nouveau) in
#        /etc/modprobe.d/zz-vfio-gpu-blacklist.conf. Reboot.
#        The GPU is now persistently configured for VM passthrough.
#     b. (Optional) If the GPU needs to be temporarily used by the host:
#        Run `gpu-pta` to attach it to host drivers.
#        Run `gpu-ptd` to detach it again for VM use. (No reboots needed for these).
#
#   Scenario B: GPU Primarily for Host, Dynamic Passthrough as Needed
#     (After completing the Initial One-Time Setup)
#     a. Ensure the system boots with the GPU attached to host drivers.
#        This is the default state if `gpu-pt3 enable` has not been run for the GPU,
#        or if `gpu-pt3 disable` was run to revert a persistent passthrough setup.
#        (If `gpu-nds` was run, the NVIDIA GPU will use proprietary drivers; otherwise,
#        it might use nouveau or amdgpu for AMD).
#     b. When passthrough is needed for a VM: Run `gpu-ptd` for the target GPU.
#        This dynamically attempts to:
#          - Ensure VFIO modules are loaded (they should be if `gpu-pt2` was run).
#          - Unbind the GPU from its host driver.
#          - Bind the GPU to `vfio-pci`.
#     c. After VM use: Run `gpu-pta` for the target GPU. This dynamically:
#          - Unbinds the GPU from `vfio-pci`.
#          - Rebinds the GPU to its appropriate host driver.
#     d. Note: If dynamic switching (`gpu-ptd`) faces issues (e.g., host drivers
#        re-asserting control, or instability), using `gpu-pt3 enable` (Scenario A)
#        for a persistent passthrough setup is generally more robust.
#
# ============================================================================

# Define directory and file variables
DIR_FUN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
FILE_FUN=$(basename "$BASH_SOURCE")
BASE_FUN="${FILE_FUN%.*}"
FILEPATH_FUN="${DIR_FUN}/${FILE_FUN}"
CONFIG_FUN="${SITE_CONFIG_FILE}"

# Dynamically create variables based on the base name
eval "FILEPATH_${BASE_FUN}=\$FILEPATH_FUN"
eval "FILE_${BASE_FUN}=\$FILE_FUN"
eval "BASE_${BASE_FUN}=\$BASE_FUN"
eval "CONFIG_${BASE_FUN}=\$CONFIG_FUN"

# Shows a summary of selected functions in the script, displaying their usage, shortname, and description
# overview functions
#
gpu-fun() {

    # Technical Description:
    #   Lists selected functions from this script.
    #   Displays usage, shortname, and description for each.
    #   Likely parses comments or uses a predefined list.
    # Dependencies:
    #   - Relies on conventions for function comments if parsing.

    # Pass all arguments directly to aux-laf
    aux-laf "$FILEPATH_gpu" "$@"
}

# Displays an overview of specific variables defined in the configuration file, showing their names, values, and usage across different files
# overview variables
#
gpu-var() {

    # Technical Description:
    #   Shows details of configuration variables.
    #   Information includes name, value, and usage context.
    #   Likely reads from SITE_CONFIG_FILE and parses it.
    # Dependencies:
    #   - SITE_CONFIG_FILE environment variable.
    #   - Text processing tools (e.g. `grep`, `awk`).

    aux-acu -o "$CONFIG_gpu" "$DIR_FUN/.."
}

# Downloads and installs NVIDIA drivers, blacklisting Nouveau
# nvidia driver setup
# <driver_version> (optional)
gpu-nds() {
  local drv_ver="${1:-550.142}"  # default driver version; override as needed
  local url="https://us.download.nvidia.com/XFree86/Linux-x86_64/${drv_ver}/NVIDIA-Linux-x86_64-${drv_ver}.run"
  local installer="NVIDIA-Linux-x86_64-${drv_ver}.run"

  # Technical Description:
  #   Downloads a specific NVIDIA driver version (default or specified).
  #   Blacklists the Nouveau driver by creating a modprobe.d configuration file.
  #   Updates the initramfs.
  #   Installs prerequisite packages (dkms, build-essential, pve-headers).
  #   Makes the downloaded driver installer executable.
  #   Runs the NVIDIA installer silently with DKMS support.
  #   Prompts the user to reboot.
  # Dependencies:
  #   - `tee`, `update-initramfs`, `apt`, `wget`, `chmod`, `reboot`.
  #   - Root privileges for most operations.
  #   - Internet connectivity for downloading the driver.
  #   - `uname -r` to get the current kernel version for PVE headers.

  echo "1) Blacklisting Nouveau..."
  cat <<EOF | tee /etc/modprobe.d/blacklist-nouveau.conf
blacklist nouveau
options nouveau modeset=0
EOF
  update-initramfs -u   # rebuild initramfs

  echo "2) Installing prerequisites..."
  apt update
  apt install -y dkms build-essential pve-headers-$(uname -r)   # DKMS & kernel headers

  echo "3) Downloading NVIDIA driver ${drv_ver}..."
  wget -q "${url}" -O "${installer}" || { echo "Download failed"; return 1; }

  chmod +x "${installer}"

  echo "4) Installing driver with DKMS support..."
  ./"${installer}" --dkms --silent  # silent install, auto DKMS

  echo "5) Finalizing—reboot recommended."
  read -p "Reboot now? [y/N] " yn
  if [[ "$yn" =~ ^[Yy]$ ]]; then
    reboot
  else
    echo "You can reboot later with: reboot"
  fi

  echo "6) After reboot, verify with: nvidia-smi"   # verify driver loaded
}


# Configures initial GRUB and EFI settings for GPU passthrough, installs necessary packages, and reboots the system
# gpu passthrough step 1
#
gpu-pt1() {
    local function_name="${FUNCNAME[0]}"
    echo "Executing section 1:"

    # Display EFI boot information
    efibootmgr -v

    # Edit GRUB configuration
    sed -i 's/GRUB_CMDLINE_LINUX_DEFAULT="quiet"/GRUB_CMDLINE_LINUX_DEFAULT="quiet iommu=pt"/' /etc/default/grub
    update-grub
    update-grub2 # Corrected typo from update-grug2

    # Technical Description:
    #   Installs 'grub-efi-amd64' package for EFI systems.
    #   Notifies completion using 'printf', using 'function_name'.
    #   Forces a system reboot. This is step 1 of GPU passthrough setup.
    # Dependencies:
    #   - `apt` command for package installation.
    #   - `printf` function (for notifications).
    #   - `reboot` command.
    #   - Root privileges for apt and reboot.

    # Install grub-efi-amd64
    apt install grub-efi-amd64 -y

    # Notify status
    printf "%s: Completed section 1, system will reboot now.\\n" "$function_name"

    # Perform system reboot without prompting
    reboot
}

# Adds necessary kernel modules for GPU passthrough to /etc/modules, updates initramfs, and reboots the system
# gpu passthrough step 2
#
gpu-pt2() {
    local function_name="${FUNCNAME[0]}"
    echo "Executing section 2:"

    # Technical Description:
    #   Appends 'vfio', 'vfio_iommu_type1', 'vfio_pci' to /etc/modules.
    #   Updates initramfs for all kernels. Notifies completion via 'printf'.
    #   Forces a system reboot. This is step 2 of GPU passthrough setup.
    # Dependencies:
    #   - `echo` (to modify /etc/modules), `update-initramfs`.
    #   - `printf` function.
    #   - `reboot` command.
    #   - Root privileges for file edits and reboot.
    #   - Assumes /etc/modules is the correct file for loading modules.

    # Add modules to /etc/modules
    echo "vfio" >> /etc/modules
    echo "vfio_iommu_type1" >> /etc/modules
    echo "vfio_pci" >> /etc/modules

    # Update initramfs
    update-initramfs -u -k all

    # Notify status
    printf "%s: Completed section 2, system will reboot now.\\n" "$function_name"

    # Perform system reboot without prompting
    reboot
}

# Finalizes or reverts GPU passthrough setup by configuring or removing VFIO-PCI IDs and blacklisting specific GPU drivers
# gpu passthrough step 3
# <enable|disable>
gpu-pt3() {
    local action="$1"
    local vfio_conf="/etc/modprobe.d/vfio.conf"
    # Use a distinct blacklist file for passthrough-specific blacklisting
    local passthrough_blacklist_conf="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"
    # local blacklist_conf_generic="/etc/modprobe.d/blacklist.conf" # No longer primarily used by this function for GPU blacklisting
    local modules_file="/etc/modules"
    # local pci_ids=() # Not directly used, vendor_device_ids are used
    local all_gpu_pci_ids_details=() # Store "vendor_id:device_id pci_slot_id"
    local nvidia_gpus_for_vfio=false
    local amd_gpus_for_vfio=false
    local function_name="${FUNCNAME[0]}"


    # Technical Description:
    #   Configures or reverts GPU passthrough.
    #   'enable':
    #     - Identifies NVIDIA and AMD GPU PCI IDs.
    #     - Writes these IDs to /etc/modprobe.d/vfio.conf for vfio-pci.
    #     - Blacklists nouveau, nvidia, radeon, amdgpu drivers in a dedicated
    #       passthrough blacklist file (/etc/modprobe.d/zz-vfio-gpu-blacklist.conf)
    #       if corresponding GPUs are assigned to vfio-pci.
    #     - Ensures vfio modules are in /etc/modules.
    #   'disable':
    #     - Removes /etc/modprobe.d/vfio.conf.
    #     - Removes the passthrough-specific blacklist file.
    #     - Comments out vfio modules in /etc/modules.
    #   Prompts for reboot after changes.
    # Dependencies:
    #   - lspci
    #   - /etc/modprobe.d/, /etc/modules
    #   - Sudo privileges for file modifications and initramfs update.
    # Interactions:
    #   - Works in conjunction with gpu-pt1 and gpu-pt2 for full passthrough setup.
    #   - `gpu-nds` creates a separate blacklist for nouveau for host NVIDIA preference,
    #     which is not touched by `gpu-pt3 disable`.
    #   - `gpu-pta` and `gpu-ptd` handle dynamic (non-persistent) driver binding.

    if [ -z "$action" ]; then
        printf "Usage: %s <enable|disable>\\n" "$function_name"
        return 1
    fi
    
    # Define colors using printf, similar to lo1 module
    local YELLOW=$(printf '\\033[0;33m')
    local NC=$(printf '\\033[0m') # No Color

    printf "Fetching GPU PCI IDs and Vendor IDs...\\n"
    local lspci_output
    lspci_output=$(lspci -nn)
    while IFS= read -r line; do
        if echo "$line" | grep -qE "VGA compatible controller|3D controller"; then
            local pci_slot_id=$(echo "$line" | awk '{print $1}')
            # Correctly extract vendor:device ID, e.g., 10de:1f82 from [10de:1f82]
            local vendor_device_id=$(echo "$line" | grep -oP '\[\K[0-9a-fA-F]{4}:[0-9a-fA-F]{4}(?=\])')
            if [ -n "$vendor_device_id" ]; then
                 all_gpu_pci_ids_details+=("$vendor_device_id $pci_slot_id")
            else
                printf "${YELLOW}Warning: Could not parse vendor/device ID for line: %s${NC}\\n" "$line"
            fi
        fi
    done <<< "$lspci_output"

    if [ ${#all_gpu_pci_ids_details[@]} -eq 0 ]; then
        printf "No GPU devices found. Nothing to do.\\n"
        return 0
    fi

    if [ "$action" == "enable" ]; then
        printf "Configuring GPU passthrough (vfio-pci)...\\n"
        local vfio_options_line="options vfio-pci ids="
        local ids_to_add=""

        for detail in "${all_gpu_pci_ids_details[@]}"; do
            local vendor_device_id=$(echo "$detail" | awk '{print $1}')
            local pci_slot_id=$(echo "$detail" | awk '{print $2}') 
            local vendor_id=$(echo "$vendor_device_id" | cut -d':' -f1)

            printf "Found GPU: %s with ID %s\\n" "$pci_slot_id" "$vendor_device_id"
            # For now, assume all detected GPUs are intended for passthrough.
            # A more advanced version could allow selecting specific GPUs.
            if [ -n "$ids_to_add" ]; then
                ids_to_add+=","
            fi
            ids_to_add+="$vendor_device_id"

            if [ "$vendor_id" == "10de" ]; then # NVIDIA
                nvidia_gpus_for_vfio=true
            elif [ "$vendor_id" == "1002" ]; then # AMD
                amd_gpus_for_vfio=true
            fi
        done

        if [ -z "$ids_to_add" ]; then
            printf "${YELLOW}No specific GPU IDs identified to add to %s. This might be an error or no supported GPUs found.${NC}\\n" "$vfio_conf"
        else
            vfio_options_line+="$ids_to_add"
            printf "Ensuring %s exists and adding/updating GPU IDs...\\n" "$vfio_conf"
            echo "$vfio_options_line" | tee "$vfio_conf" > /dev/null
            printf "Content of %s:\\n" "$vfio_conf"
            cat "$vfio_conf"
        fi

        printf "Ensuring GPU drivers are blacklisted for passthrough in %s...\\n" "$passthrough_blacklist_conf"
        # Create or clear the passthrough-specific blacklist file
        rm -f "$passthrough_blacklist_conf"
        touch "$passthrough_blacklist_conf" # Ensure it exists even if no drivers are blacklisted

        if $nvidia_gpus_for_vfio; then
            printf "NVIDIA GPU(s) targeted for passthrough. Blacklisting nouveau and nvidia in %s.\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist nouveau"
                echo "options nouveau modeset=0" # Important for nouveau
                echo "blacklist nvidia"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        elif lspci -nn | grep -qE "VGA compatible controller.*\[10de:" || lspci -nn | grep -qE "3D controller.*\[10de:"; then
             # This case handles if no NVIDIA GPUs were explicitly added to vfio.conf (e.g. ids_to_add was empty for NVIDIA)
             # but NVIDIA GPUs are present on the system. This is a fallback/general blacklisting.
             printf "${YELLOW}General NVIDIA GPU(s) detected on system (though not specifically added to %s by this run).${NC}\\n" "$vfio_conf"
             printf "${YELLOW}Adding precautionary blacklist for nouveau and nvidia in %s.${NC}\\n" "$passthrough_blacklist_conf"
             {
                echo "blacklist nouveau"
                echo "options nouveau modeset=0"
                echo "blacklist nvidia"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        fi

        if $amd_gpus_for_vfio; then
            printf "AMD GPU(s) targeted for passthrough. Blacklisting radeon and amdgpu in %s.\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist radeon"
                echo "blacklist amdgpu"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        elif lspci -nn | grep -qE "VGA compatible controller.*\[1002:" || lspci -nn | grep -qE "3D controller.*\[1002:"; then
            # Similar fallback for AMD
            printf "${YELLOW}General AMD GPU(s) detected on system (though not specifically added to %s by this run).${NC}\\n" "$vfio_conf"
            printf "${YELLOW}Adding precautionary blacklist for radeon and amdgpu in %s.${NC}\\n" "$passthrough_blacklist_conf"
            {
                echo "blacklist radeon"
                echo "blacklist amdgpu"
            } | tee -a "$passthrough_blacklist_conf" > /dev/null
        fi
        
        if [ -s "$passthrough_blacklist_conf" ]; then # Check if file is not empty
            printf "Content of %s:\\n" "$passthrough_blacklist_conf"
            cat "$passthrough_blacklist_conf"
        else
            printf "No specific drivers were blacklisted in %s during this run (e.g., no NVIDIA/AMD GPUs for VFIO or detected generally). File is empty or only contains comments.\\n" "$passthrough_blacklist_conf"
        fi


        printf "Ensuring vfio modules are loaded at boot via %s...\\n" "$modules_file"
        for module in vfio vfio_iommu_type1 vfio_pci; do
            if ! grep -qP "^\s*${module}\s*(#.*)?$" "$modules_file"; then # Check if module exists, possibly commented
                echo "$module" | tee -a "$modules_file" > /dev/null
                printf "Added %s to %s.\\n" "$module" "$modules_file"
            else
                # Ensure not commented out
                if grep -qP "^\s*#\s*${module}" "$modules_file"; then
                    sed -i "s/^\s*#\s*${module}/${module}/" "$modules_file"
                    printf "Uncommented %s in %s.\\n" "$module" "$modules_file"
                else
                    printf "%s already correctly listed in %s.\\n" "$module" "$modules_file"
                fi
            fi
        done

    elif [ "$action" == "disable" ]; then
        printf "Reverting GPU passthrough configuration...\\n"
        printf "Removing %s...\\n" "$vfio_conf"
        rm -f "$vfio_conf"
        printf "Removing the passthrough-specific blacklist file %s...\\n" "$passthrough_blacklist_conf"
        rm -f "$passthrough_blacklist_conf"
        # Note: We do NOT touch /etc/modprobe.d/blacklist-nouveau.conf if created by gpu-nds

        printf "Commenting out vfio modules in %s (they can be auto-loaded if needed)...\\n" "$modules_file"
        for module in vfio vfio_iommu_type1 vfio_pci; do
            if grep -qP "^\s*${module}" "$modules_file"; then # If module is present and not commented
                sed -i "s/^\s*${module}/# ${module}/" "$modules_file"
                printf "Commented out %s in %s.\\n" "$module" "$modules_file"
            else
                printf "%s not found or already commented in %s.\\n" "$module" "$modules_file"
            fi
        done
    else
        printf "Invalid action: %s. Use 'enable' or 'disable'.\\n" "$action"
        return 1
    fi

    printf "Updating initramfs...\\n"
    update-initramfs -u -k all # Use -k all to update all kernels, or just -u for current
    printf "Configuration applied. A reboot is required for changes to take full effect.\\n"
}

# Detaches the GPU from the host system, making it available for VM passthrough
# gpu passthrough detach
#
gpu-ptd() {
    local function_name="${FUNCNAME[0]}"
    echo "--- Debug: Entering ${function_name} ---"
    local -a gpus_to_process
    local gpu_id_arg="$1" # Optional: specific GPU PCI ID to detach

    # Define colors using printf
    local YELLOW=$(printf '\\033[0;33m')
    local RED=$(printf '\\033[0;31m')
    local NC=$(printf '\\033[0m') # No Color

    # Check for IOMMU (Intel VT-d or AMD-Vi)
    # A more robust check for IOMMU active in kernel cmdline
    if ! grep -q 'iommu=pt\|intel_iommu=on\|amd_iommu=on' /proc/cmdline; then
        printf "${YELLOW}WARNING: IOMMU (intel_iommu=on/amd_iommu=on or iommu=pt) does not appear to be enabled in kernel command line.${NC}\\n"
        printf "${YELLOW}         VFIO passthrough may not work. Please ensure it is configured in your bootloader (e.g., GRUB).${NC}\\n"
        # Allow to continue, but it's likely to fail later if IOMMU isn't truly on.
    fi

    # Technical Description:
    #   Dynamically detaches GPU(s) from host drivers for VM passthrough.
    #   Attempts to be self-sufficient by loading necessary VFIO modules.
    #   Unloads standard GPU drivers (nouveau, nvidia, amdgpu, radeon) if they are in use by the target GPU(s).
    #   Loads 'vfio-pci' driver if not already loaded.
    #   Identifies GPU(s) via 'lspci -nn'. If an argument is provided, only that GPU is targeted.
    #   For each target GPU: unbinds from current driver, sets 'driver_override' to 'vfio-pci', binds to 'vfio-pci'.
    #   Changes are NOT persistent across reboots.
    # Dependencies:
    #   - `lspci`, `grep`, `find`, `ls`, `modprobe`, `awk`, `echo`, `tee`.
    #   - Root privileges for driver/module operations and sysfs access.
    #   - IOMMU enabled in BIOS/UEFI and kernel.

    printf "INFO: Starting GPU detachment process\\n"

    printf "INFO: Ensuring VFIO modules are loaded...\\n"
    for module in vfio vfio_iommu_type1 vfio_pci; do
        if lsmod | grep -q "^${module}"; then
            printf "INFO: Module %s is already loaded.\\n" "$module"
        else
            printf "INFO: Attempting to load module %s...\\n" "$module"
            if modprobe "$module"; then
                printf "INFO: Module %s loaded successfully.\\n" "$module"
            else
                printf "${RED}ERROR: Failed to load module %s. GPU detachment may fail.${NC}\\n" "$module"
                # For vfio_pci, this is critical. For others, it might also be.
                if [ "$module" == "vfio_pci" ]; then
                    printf "${RED}Exiting due to failure to load critical module vfio_pci.${NC}\\n"
                    return 1
                fi
            fi
        fi
    done

    printf "INFO: Identifying target GPU(s)...\\n"
    if [ -n "$gpu_id_arg" ]; then
        # Validate the provided PCI ID format (basic check)
        if ! [[ "$gpu_id_arg" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then # e.g., 01:00.0
            printf "${RED}ERROR: Invalid PCI ID format provided: %s. Expected format like 01:00.0${NC}\\n" "$gpu_id_arg"
            return 1
        fi
        # Check if this GPU exists and is a VGA/3D controller
        if lspci -s "$gpu_id_arg" -nn | grep -qE "VGA compatible controller|3D controller"; then
            gpus_to_process+=("$gpu_id_arg")
            printf "INFO: Targeting specified GPU: %s\\n" "$gpu_id_arg"
        else
            printf "${RED}ERROR: Specified PCI ID %s is not a VGA/3D controller or does not exist.${NC}\\n" "$gpu_id_arg"
            return 1
        fi
    else
        printf "INFO: No specific GPU ID provided, will attempt to detach all detected VGA/3D controllers not already on vfio-pci.\\n"
        local lspci_output
        lspci_output=$(lspci -nnk) # Get kernel driver info as well
        while IFS= read -r line; do
            if echo "$line" | grep -qE "VGA compatible controller|3D controller"; then
                local pci_slot_id=$(echo "$line" | awk '{print $1}')
                # Check if already on vfio-pci
                if echo "$lspci_output" | grep -A3 "^$pci_slot_id" | grep -q "Kernel driver in use: vfio-pci"; then
                    printf "INFO: GPU %s is already using vfio-pci. Skipping.\\n" "$pci_slot_id"
                else
                    gpus_to_process+=("$pci_slot_id")
                    printf "INFO: Identified GPU %s for detachment.\\n" "$pci_slot_id"
                fi
            fi
        done <<< "$lspci_output"
    fi

    if [ ${#gpus_to_process[@]} -eq 0 ]; then
        printf "${YELLOW}No suitable GPU devices found or specified for detachment.${NC}\\n"
        return 0
    fi

    printf "INFO: Processing GPU IDs for detachment: %s\\n" "${gpus_to_process[*]}"
    for id in "${gpus_to_process[@]}"; do
        printf "--- Processing GPU %s ---\\n" "$id"
        local full_pci_id="0000:$id"
        local driver_path="/sys/bus/pci/devices/$full_pci_id/driver"
        local current_driver=""

        if [ -L "$driver_path" ]; then
            current_driver=$(basename $(readlink -f "$driver_path"))
            printf "INFO: GPU %s is currently bound to driver: %s\\n" "$id" "$current_driver"
            if [ "$current_driver" == "vfio-pci" ]; then
                printf "INFO: GPU %s is already on vfio-pci. Skipping unbind/bind sequence.\\n" "$id"
                continue
            fi
            printf "INFO: Unbinding GPU %s from %s...\\n" "$id" "$current_driver"
            if echo "$full_pci_id" > "$driver_path/unbind"; then
                printf "INFO: Successfully unbound GPU %s from %s.\\n" "$id" "$current_driver"
            else
                printf "${YELLOW}WARNING: Failed to unbind GPU %s from %s. This might be an issue if the driver is in use.${NC}\\n" "$id" "$current_driver"
                # Attempt to remove the module as a more forceful way, if it's a known GPU driver
                if [[ " $current_driver " =~ " nvidia " || " $current_driver " =~ " nouveau " || " $current_driver " =~ " amdgpu " || " $current_driver " =~ " radeon " ]]; then
                    printf "INFO: Attempting to remove module %s to release GPU %s...\\n" "$current_driver" "$id"
                    if modprobe -r "$current_driver"; then
                        printf "INFO: Module %s removed.\\n" "$current_driver"
                    else
                        printf "${RED}ERROR: Failed to remove module %s. GPU %s may remain bound.${NC}\\n" "$current_driver" "$id"
                        # continue # Decide if you want to skip to next GPU or try to proceed
                    fi
                fi 
            fi
        else
            printf "INFO: GPU %s is not currently bound to any driver.\\n" "$id"
        fi
        
        printf "INFO: Setting driver_override to vfio-pci for GPU %s...\\n" "$id"
        if echo "vfio-pci" > "/sys/bus/pci/devices/$full_pci_id/driver_override"; then
             printf "INFO: Successfully set driver_override for %s.\\n" "$id"
        else
            printf "${RED}ERROR: Failed to set driver_override to vfio-pci for GPU %s. This is critical.${NC}\\n" "$id"
            # Consider not continuing for this GPU if override fails
            continue
        fi

        printf "INFO: Probing PCI bus to allow vfio-pci to claim GPU %s...\\n" "$id"
        # This tells the PCI subsystem to re-evaluate drivers for this device
        if echo 1 > "/sys/bus/pci/devices/$full_pci_id/remove"; then 
            sleep 0.5 # Brief pause
            echo 1 > "/sys/bus/pci/rescan"
            printf "INFO: PCI rescan initiated for %s.\\n" "$id"
        else
             printf "${YELLOW}WARNING: Could not remove/rescan PCI device %s. Binding might rely on existing vfio-pci probe.${NC}\\n" "$id"
             # Fallback: try to bind directly if rescan path failed
             printf "INFO: Attempting direct bind of GPU %s to vfio-pci driver...\\n" "$id"
             if ! echo "$full_pci_id" > "/sys/bus/pci/drivers/vfio-pci/bind"; then
                 printf "${RED}ERROR: Failed to bind GPU %s to VFIO-PCI after fallback attempt. See 'dmesg -e'.${NC}\\n" "$id"
             else
                 printf "INFO: Successfully bound GPU %s to VFIO-PCI via direct bind.\\n" "$id"
             fi
        fi

        # Verify binding
        sleep 0.5 # Give a moment for driver to bind
        if [ -L "$driver_path" ] && [ "$(basename "$(readlink -f "$driver_path")")" == "vfio-pci" ]; then
            printf "INFO: Successfully verified GPU %s is bound to VFIO-PCI.\\n" "$id"
        else
            # GPU is NOT bound to vfio-pci after the first set of operations.
            printf "INFO: GPU %s not bound to VFIO-PCI after initial attempt. Trying further methods...\\n" "$id"

            # Attempt 2: Force remove/rescan again
            printf "INFO: Attempting another remove and rescan for %s...\\n" "$id"
            if echo 1 > "/sys/bus/pci/devices/$full_pci_id/remove"; then
                sleep 0.5
                if echo 1 > "/sys/bus/pci/rescan"; then
                    printf "INFO: Second remove/rescan sequence initiated for %s.\\n" "$id"
                    sleep 0.5 # Give a moment for driver to bind after rescan
                    # Verify after second remove/rescan
                    if [ -L "$driver_path" ] && [ "$(basename "$(readlink -f "$driver_path")")" == "vfio-pci" ]; then
                        printf "INFO: Successfully bound GPU %s to VFIO-PCI after second remove/rescan.\\n" "$id"
                    else
                        # Attempt 3: Direct bind (if second remove/rescan didn't lead to binding)
                        printf "INFO: GPU %s still not bound after second rescan. Attempting direct bind to vfio-pci (final attempt)...\\n" "$id"
                        if echo "$full_pci_id" > "/sys/bus/pci/drivers/vfio-pci/bind"; then
                            sleep 0.5 # Give a moment
                            if [ -L "$driver_path" ] && [ "$(basename "$(readlink -f "$driver_path")")" == "vfio-pci" ]; then
                                printf "INFO: Successfully bound GPU %s to VFIO-PCI via direct bind (final attempt).\\n" "$id"
                            else
                                printf "${RED}ERROR: GPU %s still not bound to VFIO-PCI after direct bind. See 'dmesg -e'.${NC}\\n" "$id"
                            fi
                        else
                            printf "${RED}ERROR: Failed to issue direct bind command for GPU %s to VFIO-PCI. See 'dmesg -e'.${NC}\\n" "$id"
                        fi
                    fi
                else
                    printf "${YELLOW}WARNING: Second rescan command failed for %s. Attempting direct bind...${NC}\\n" "$id"
                    # Attempt 3: Direct bind (if second rescan command failed)
                    if echo "$full_pci_id" > "/sys/bus/pci/drivers/vfio-pci/bind"; then
                        sleep 0.5
                        if [ -L "$driver_path" ] && [ "$(basename "$(readlink -f "$driver_path")")" == "vfio-pci" ]; then
                            printf "INFO: Successfully bound GPU %s to VFIO-PCI via direct bind (after rescan cmd failure).\\n" "$id"
                        else
                            printf "${RED}ERROR: GPU %s still not bound to VFIO-PCI after direct bind (rescan cmd failure). See 'dmesg -e'.${NC}\\n" "$id"
                        fi
                    else
                        printf "${RED}ERROR: Failed to issue direct bind command for GPU %s to VFIO-PCI (rescan cmd failure). See 'dmesg -e'.${NC}\\n" "$id"
                    fi
                fi
            else
                printf "${YELLOW}WARNING: Second remove command failed for %s. Attempting direct bind...${NC}\\n" "$id"
                # Attempt 3: Direct bind (if second remove command failed)
                if echo "$full_pci_id" > "/sys/bus/pci/drivers/vfio-pci/bind"; then
                    sleep 0.5
                    if [ -L "$driver_path" ] && [ "$(basename "$(readlink -f "$driver_path")")" == "vfio-pci" ]; then
                        printf "INFO: Successfully bound GPU %s to VFIO-PCI via direct bind (after remove cmd failure).\\n" "$id"
                    else
                        printf "${RED}ERROR: GPU %s still not bound to VFIO-PCI after direct bind (remove cmd failure). See 'dmesg -e'.${NC}\\n" "$id"
                    fi
                else
                    printf "${RED}ERROR: Failed to issue direct bind command for GPU %s to VFIO-PCI (remove cmd failure). See 'dmesg -e'.${NC}\\n" "$id"
                fi
            fi
        fi
    done

    printf "INFO: GPU detachment process completed.\\n"
    printf "INFO: Verify with 'lspci -nnk' or 'gpu-pts'.\\n"
    echo "--- Debug: Exiting ${function_name} ---"
}

# Attaches the GPU back to the host system
# gpu passthrough attach
# <pci_id> (optional, e.g., 01:00.0)
gpu-pta() {
    local function_name="${FUNCNAME[0]}"
    echo "--- Debug: Entering ${function_name} ---"
    local gpu_id_arg="$1" # Optional: specific GPU PCI ID to attach
    local -a gpus_to_process

    local current_hostname
    current_hostname=$(hostname -s) # Get short hostname like x1, x2
    local preferred_nvidia_driver="nouveau" # Default preference
    
    # Define colors using printf
    local YELLOW=$(printf '\\033[0;33m')
    local RED=$(printf '\\033[0;31m')
    local GREEN=$(printf '\\033[0;32m')
    local NC=$(printf '\\033[0m') # No Color

    # Construct the preference variable name, e.g., x1_nvidia_driver_preference
    local preference_var_name="${current_hostname}_nvidia_driver_preference"
    local global_default_preference_var="default_nvidia_driver_preference" # Optional global default

    # Determine preferred NVIDIA driver from config (if SITE_CONFIG_FILE is sourced and vars are set)
    if [ -n "${!preference_var_name}" ]; then
        if [[ "${!preference_var_name}" == "nvidia" || "${!preference_var_name}" == "nouveau" ]]; then
            preferred_nvidia_driver="${!preference_var_name}"
            echo "--- Debug: Using node-specific NVIDIA driver preference for $current_hostname: $preferred_nvidia_driver ---"
        else
            echo "--- Debug: Invalid value for $preference_var_name: ${!preference_var_name}. Checking global default. ---"
            if [ -n "${!global_default_preference_var}" ] && ([[ "${!global_default_preference_var}" == "nvidia" || "${!global_default_preference_var}" == "nouveau" ]]); then
                preferred_nvidia_driver="${!global_default_preference_var}"
                echo "--- Debug: Using global default NVIDIA driver preference: $preferred_nvidia_driver ---"
            else
                echo "--- Debug: No valid global default. Using hardcoded default: $preferred_nvidia_driver. ---"
            fi
        fi
    elif [ -n "${!global_default_preference_var}" ] && ([[ "${!global_default_preference_var}" == "nvidia" || "${!global_default_preference_var}" == "nouveau" ]]); then
        preferred_nvidia_driver="${!global_default_preference_var}"
        echo "--- Debug: No specific preference for $current_hostname. Using global default NVIDIA driver preference: $preferred_nvidia_driver ---"
    else
        echo "--- Debug: No specific NVIDIA driver preference found for $current_hostname ($preference_var_name) and no global default. Using hardcoded default: $preferred_nvidia_driver. ---"
    fi 

    # Technical Description:
    #   Dynamically attaches GPU(s) back to the host system from vfio-pci.
    #   Attempts to be self-sufficient.
    #   Identifies GPU PCI slot IDs. If an argument is provided, only that GPU is targeted.
    #   For each target GPU:
    #     - Unbinds from vfio-pci (if bound).
    #     - Clears 'driver_override'.
    #     - Determines appropriate host driver (amdgpu, or nvidia/nouveau based on site config/preference).
    #     - Attempts to load the host driver module if not already loaded.
    #     - Binds the host driver.
    #     - Rescans PCI devices (may not always be necessary but can help).
    # Dependencies:
    #   - `lspci`, `modprobe`, `awk`, `echo`, `tee`, `hostname`.
    #   - Root privileges for driver/module operations and sysfs access.
    # Interactions:
    #   - Uses `*_nvidia_driver_preference` from site config for NVIDIA GPUs.
    #   - Warns if `nouveau` is preferred but blacklisted by `gpu-nds`.

    printf "INFO: Starting GPU attachment process...\\n"

    printf "INFO: Identifying target GPU(s)...\\n"
    if [ -n "$gpu_id_arg" ]; then
        if ! [[ "$gpu_id_arg" =~ ^[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9a-fA-F]$ ]]; then
            printf "${RED}ERROR: Invalid PCI ID format provided: %s. Expected format like 01:00.0${NC}\\n" "$gpu_id_arg"
            return 1
        fi
        # Check if this GPU exists and is a VGA/3D controller currently on vfio-pci
        if lspci -s "$gpu_id_arg" -k -nn | grep -A3 "VGA compatible controller\|3D controller" | grep -q "Kernel driver in use: vfio-pci"; then
            gpus_to_process+=("$gpu_id_arg")
            printf "INFO: Targeting specified GPU %s for attachment (currently on vfio-pci).\\n" "$gpu_id_arg"
        elif lspci -s "$gpu_id_arg" -nn | grep -qE "VGA compatible controller|3D controller"; then
             printf "${YELLOW}WARNING: Specified GPU %s exists but is not currently using vfio-pci. Will attempt to ensure host driver is bound.${NC}\\n" "$gpu_id_arg"
             gpus_to_process+=("$gpu_id_arg") # Still process it to ensure host driver
        else
            printf "${RED}ERROR: Specified PCI ID %s is not a VGA/3D controller or does not exist.${NC}\\n" "$gpu_id_arg"
            return 1
        fi
    else
        printf "INFO: No specific GPU ID provided, will attempt to attach all VGA/3D controllers currently on vfio-pci.\\n"
        local lspci_output_vfio
        lspci_output_vfio=$(lspci -nnk | grep -A1 "Kernel driver in use: vfio-pci" | grep -E "VGA compatible controller|3D controller")
        if [ -z "$lspci_output_vfio" ]; then
            printf "${YELLOW}No GPUs found currently using vfio-pci according to lspci -nnk.${NC}"
            # Optionally, could scan all GPUs and try to ensure they are on host drivers if not on vfio.
            # For now, let's stick to only those explicitly on vfio if no arg is given.
        else
            while IFS= read -r line; do
                local pci_slot_id=$(echo "$line" | awk '{print $1}')
                gpus_to_process+=("$pci_slot_id")
                printf "INFO: Identified GPU %s (on vfio-pci) for attachment.\\n" "$pci_slot_id"
            done <<< "$lspci_output_vfio"
        fi
    fi

    if [ ${#gpus_to_process[@]} -eq 0 ]; then
        printf "${YELLOW}No suitable GPU devices found or specified for attachment.${NC}\\n"
        return 0
    fi

    for pci_slot_id in "${gpus_to_process[@]}"; do
        printf "--- Processing GPU at %s ---\\n" "$pci_slot_id"
        local full_pci_id="0000:$pci_slot_id"
        local driver_path="/sys/bus/pci/devices/$full_pci_id/driver"
        local driver_override_path="/sys/bus/pci/devices/$full_pci_id/driver_override"

        # Unbind from vfio-pci if it's bound
        if [ -L "$driver_path" ] && [ "$(basename $(readlink -f "$driver_path"))" == "vfio-pci" ]; then
            printf "INFO: Unbinding %s from vfio-pci...\\n" "$pci_slot_id"
            if ! echo "$full_pci_id" > "/sys/bus/pci/drivers/vfio-pci/unbind"; then
                printf "${RED}ERROR: Failed to unbind %s from vfio-pci. Attachment may fail.${NC}\\n" "$pci_slot_id"
                # continue # Or try to proceed
            else
                printf "INFO: Successfully unbound %s from vfio-pci.\\n" "$pci_slot_id"
            fi
        else
            printf "INFO: GPU %s not currently bound to vfio-pci. Will attempt to bind host driver directly.\\n" "$pci_slot_id"
        fi

        # Clear driver_override
        printf "INFO: Clearing driver_override for %s...\\n" "$pci_slot_id"
        if ! echo "" > "$driver_override_path"; then # Write empty string to clear
            printf "${YELLOW}WARNING: Failed to clear driver_override for %s. This might interfere with host driver binding.${NC}\\n" "$pci_slot_id"
        else
            printf "INFO: Successfully cleared driver_override for %s.\\n" "$pci_slot_id"
        fi

        # Determine the correct host driver
        local vendor_id=$(lspci -n -s "$pci_slot_id" | awk '{print $3}' | cut -d':' -f1)
        local host_driver_to_bind=""
        case "$vendor_id" in
            "1002") # AMD
                host_driver_to_bind="amdgpu" # Could also be "radeon" for older cards
                # A more sophisticated check could look at lspci -k to see what module it *would* use
                # or try amdgpu first, then radeon if that fails.
                ;;
            "10de") # NVIDIA
                host_driver_to_bind="$preferred_nvidia_driver"
                echo "--- Debug: NVIDIA card detected. Will attempt to use preferred driver: $host_driver_to_bind ---"
                if [ "$host_driver_to_bind" == "nouveau" ] && [ -f "/etc/modprobe.d/blacklist-nouveau.conf" ]; then
                    printf "${YELLOW}WARNING: Preferred driver for %s is 'nouveau', but /etc/modprobe.d/blacklist-nouveau.conf exists (likely from gpu-nds).${NC}\\n" "$current_hostname"
                    printf "${YELLOW}         'nouveau' may fail to load. Consider setting %s_nvidia_driver_preference to 'nvidia' or removing the blacklist.${NC}\\n" "$current_hostname"
                fi
                ;;
            *)
                printf "${RED}ERROR: Unknown GPU vendor ID: %s for PCI slot %s. Cannot determine host driver.${NC}\\n" "$vendor_id" "$pci_slot_id"
                continue # Skip to next GPU
                ;;
        esac

        if [ -z "$host_driver_to_bind" ]; then
            printf "${RED}ERROR: Could not determine host driver for %s. Skipping.${NC}\\n" "$pci_slot_id"
            continue
        fi 

        printf "INFO: Attempting to ensure module %s is loaded...\\n" "$host_driver_to_bind"
        if ! lsmod | grep -q "^${host_driver_to_bind}"; then
            if modprobe "$host_driver_to_bind"; then
                printf "INFO: Module %s loaded successfully.\\n" "$host_driver_to_bind"
            else
                printf "${RED}ERROR: Failed to load module %s. Cannot bind GPU %s.${NC}\\n" "$host_driver_to_bind" "$pci_slot_id"
                continue
            fi
        else
            printf "INFO: Module %s is already loaded.\\n" "$host_driver_to_bind"
        fi

        printf "INFO: Attempting to bind %s to %s driver...\\n" "$pci_slot_id" "$host_driver_to_bind"
        # Try to bind via the driver's bind file
        if echo "$full_pci_id" > "/sys/bus/pci/drivers/$host_driver_to_bind/bind"; then
            printf "${GREEN}Successfully initiated binding of %s to %s.${NC}\\n" "$pci_slot_id" "$host_driver_to_bind"
        else
            printf "${YELLOW}WARNING: Failed to directly bind %s to %s. Will try PCI rescan.${NC}\\n" "$pci_slot_id" "$host_driver_to_bind"
            # As a fallback, or primary method, try telling the device to re-probe
            printf "INFO: Triggering PCI rescan for device %s to allow %s to claim it...\\n" "$pci_slot_id" "$host_driver_to_bind"
            if echo 1 > "/sys/bus/pci/devices/$full_pci_id/remove"; then
                sleep 0.5 # Brief pause
                echo 1 > "/sys/bus/pci/rescan"
                printf "INFO: PCI rescan initiated for %s.\\n" "$pci_slot_id"
            else
                printf "${RED}ERROR: Could not remove/rescan PCI device %s. Binding %s may fail.${NC}\\n" "$pci_slot_id" "$host_driver_to_bind"
            fi
        fi

        # Verify binding
        sleep 1 # Give a moment for driver to bind
        local final_driver
        if [ -L "$driver_path" ]; then
            final_driver=$(basename $(readlink -f "$driver_path"))
            if [ "$final_driver" == "$host_driver_to_bind" ]; then
                printf "${GREEN}Successfully verified GPU %s is bound to %s.${NC}\\n" "$pci_slot_id" "$host_driver_to_bind"
            else
                printf "${RED}ERROR: GPU %s is bound to %s instead of expected %s. Check dmesg.${NC}\\n" "$pci_slot_id" "$final_driver" "$host_driver_to_bind"
            fi
        else
            printf "${RED}ERROR: GPU %s is not bound to any driver after attachment attempt. Check dmesg.${NC}\\n" "$pci_slot_id"
        fi
    done

    # A general rescan might sometimes help graphics systems re-initialize
    # printf "INFO: Performing a general PCI bus rescan...\\n"
    # echo 1 > /sys/bus/pci/rescan # This might be too broad or disruptive in some cases

    echo "--- Debug: Exiting ${function_name} ---"
    printf "INFO: GPU attachment process complete. Check 'lspci -nnk' or 'gpu-pts' for status.\\n"
}

# Checks the current status of the GPU
# gpu passthrough status
#
gpu-pts() {
    local function_name="${FUNCNAME[0]}"
    # Flags for overall summary, populated by lspci parsing
    local is_any_gpu_on_vfio=false
    local host_drivers_for_gpu=""
    local vga_devices_processed=false # True if any VGA/3D device was found by lspci and processed

    # Define colors using printf
    local GREEN=$(printf '\\033[0;32m')
    local CYAN=$(printf '\\033[0;36m')
    local MAGENTA=$(printf '\\033[0;35m')
    local YELLOW=$(printf '\\033[0;33m')
    local INDIGO_BLUE=$(printf '\\033[38;2;75;0;130m')
    local RED=$(printf '\\033[0;31m')
    local NC=$(printf '\\033[0m') # No Color

    # Workflow Configuration Checklist & Runtime Status Symbols
    local CHECK_MARK="${GREEN}✓${NC}"
    local CROSS_MARK="${RED}✗${NC}"
    local QUESTION_MARK="${YELLOW}?${NC}"

    # Define paths needed for the checklist
    local modules_file_path="/etc/modules"
    local blacklist_nouveau_conf_path="/etc/modprobe.d/blacklist-nouveau.conf"
    local vfio_conf_path="/etc/modprobe.d/vfio.conf"
    local passthrough_blacklist_conf_path="/etc/modprobe.d/zz-vfio-gpu-blacklist.conf"

    # Initialize variables for lspci -nnk processing results
    local all_device_blocks_output=""
    local gpu_details_for_checklist=""

    # --- Hoisted lspci -nnk processing ---
    # This section populates all_device_blocks_output, gpu_details_for_checklist,
    # is_any_gpu_on_vfio, host_drivers_for_gpu, and vga_devices_processed.
    local temp_lspci_gpu_blocks="/tmp/gpu_pts_blocks.$$"
    lspci -nnk | grep -A4 -iE "VGA compatible controller|3D controller" > "$temp_lspci_gpu_blocks"
    
    if [ -s "$temp_lspci_gpu_blocks" ]; then
        local current_gpu_block_content=""
        # vga_devices_processed is already false, will be set true if blocks are processed or temp file had content

        while IFS= read -r line_from_grep; do
            if echo "$line_from_grep" | grep -qE "VGA compatible controller|3D controller"; then
                # Process previous block if it exists and was a GPU block
                if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
                    all_device_blocks_output+="${YELLOW}Device Block:${NC}\\n$current_gpu_block_content\\n\\n"
                    vga_devices_processed=true # A device block was processed
                    local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')
                    
                    local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
                    local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
                    local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
                    local device_name_short=$(echo "$device_name_raw" | sed 's/ \\[.*\\]//g' | awk '{print $1, $2}')
                    local device_name_for_checklist="$device_vendor_name $device_name_short"
                    device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
                    if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

                    if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                        is_any_gpu_on_vfio=true
                        gpu_details_for_checklist+="     ${CHECK_MARK} GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\\n"
                    elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                        local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                        if [ -n "$driver_name" ]; then
                            if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                            gpu_details_for_checklist+="     ${INDIGO_BLUE}*${NC} GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\\n"
                        else
                            gpu_details_for_checklist+="     ${RED}✗${NC} GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\\n"
                        fi
                    else
                        gpu_details_for_checklist+="     ${RED}✗${NC} GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\\n"
                    fi
                fi
                current_gpu_block_content="$line_from_grep" # Start new block
            elif [[ "$line_from_grep" == "--" ]]; then # Separator line from grep -A
                if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
                    all_device_blocks_output+="${YELLOW}Device Block:${NC}\\n$current_gpu_block_content\\n\\n"
                    vga_devices_processed=true
                    local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')

                    local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
                    local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
                    local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
                    local device_name_short=$(echo "$device_name_raw" | sed 's/ \\[.*\\]//g' | awk '{print $1, $2}')
                    local device_name_for_checklist="$device_vendor_name $device_name_short"
                    device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
                    if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

                    if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                        is_any_gpu_on_vfio=true
                        gpu_details_for_checklist+="     ${CHECK_MARK} GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\\n"
                    elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                        local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                        if [ -n "$driver_name" ]; then
                           if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                           gpu_details_for_checklist+="     ${INDIGO_BLUE}*${NC} GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\\n"
                        else
                           gpu_details_for_checklist+="     ${RED}✗${NC} GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\\n"
                        fi
                    else
                        gpu_details_for_checklist+="     ${RED}✗${NC} GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\\n"
                    fi
                fi
                current_gpu_block_content="" # Reset for next block
            elif [ -n "$current_gpu_block_content" ]; then # Accumulate lines for the current block
                current_gpu_block_content="$current_gpu_block_content"$'\\n'"$line_from_grep"
            fi
        done < "$temp_lspci_gpu_blocks"
        
        # Process the last block after loop ends
        if [ -n "$current_gpu_block_content" ] && echo "$current_gpu_block_content" | head -n1 | grep -qE "VGA compatible controller|3D controller"; then
            all_device_blocks_output+="${YELLOW}Device Block:${NC}\\n$current_gpu_block_content\\n\\n"
            vga_devices_processed=true
            local pci_id_for_block=$(echo "$current_gpu_block_content" | head -n1 | awk '{print $1}')

            local device_vendor_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f6)
            local device_name_raw=$(lspci -mms "$pci_id_for_block" 2>/dev/null | cut -d'"' -f8)
            local device_vendor_name=$(echo "$device_vendor_name_raw" | sed -E 's/ Corporation| Incorporated| Technologies Inc.//g')
            local device_name_short=$(echo "$device_name_raw" | sed 's/ \\[.*\\]//g' | awk '{print $1, $2}')
            local device_name_for_checklist="$device_vendor_name $device_name_short"
            device_name_for_checklist=$(echo "$device_name_for_checklist" | sed 's/[[:space:]]*$//')
            if [ -z "$device_name_for_checklist" ]; then device_name_for_checklist="N/A"; fi

            if echo "$current_gpu_block_content" | grep -q "Kernel driver in use: vfio-pci"; then
                is_any_gpu_on_vfio=true
                gpu_details_for_checklist+="     ${CHECK_MARK} GPU $pci_id_for_block ($device_name_for_checklist) on vfio-pci\\n"
            elif driver_line=$(echo "$current_gpu_block_content" | grep "Kernel driver in use:"); then
                local driver_name=$(echo "$driver_line" | awk -F': ' '{print $2}')
                if [ -n "$driver_name" ]; then
                    if [ -z "$host_drivers_for_gpu" ]; then host_drivers_for_gpu="$driver_name"; elif [[ ! "$host_drivers_for_gpu" =~ "$driver_name" ]]; then host_drivers_for_gpu="$host_drivers_for_gpu, $driver_name"; fi
                    gpu_details_for_checklist+="     ${INDIGO_BLUE}*${NC} GPU $pci_id_for_block ($device_name_for_checklist) on $driver_name\\n"
                else
                    gpu_details_for_checklist+="     ${RED}✗${NC} GPU $pci_id_for_block ($device_name_for_checklist) on UNKNOWN driver\\n"
                fi
            else
                 gpu_details_for_checklist+="     ${RED}✗${NC} GPU $pci_id_for_block ($device_name_for_checklist) UNBOUND or no driver info\\n"
            fi
        fi
        
        # If lspci | grep found devices, but loop didn't process them (e.g. parsing issue),
        # still mark vga_devices_processed as true and try basic flag population.
        if ! $vga_devices_processed && [ -s "$temp_lspci_gpu_blocks" ]; then
            vga_devices_processed=true 
            local full_lspci_check_output_from_temp=$(cat "$temp_lspci_gpu_blocks")
            if echo "$full_lspci_check_output_from_temp" | grep -q "Kernel driver in use: vfio-pci"; then
                is_any_gpu_on_vfio=true
            fi
            if ! $is_any_gpu_on_vfio; then
                local found_host_driver_names=$(echo "$full_lspci_check_output_from_temp" | grep "Kernel driver in use:" | awk -F': ' '{print $2}' | sort -u | tr '\\n' ',' | sed 's/,$//')
                if [ -n "$found_host_driver_names" ] && [ "$found_host_driver_names" != "vfio-pci" ]; then
                     host_drivers_for_gpu="$found_host_driver_names"
                fi
            fi
        fi
    else
        # temp_lspci_gpu_blocks is empty, so lspci | grep found no VGA/3D controllers.
        # vga_devices_processed remains false.
        : # No action needed, flags remain at their defaults
    fi
    rm -f "$temp_lspci_gpu_blocks"
    # --- End of Hoisted lspci -nnk processing ---

    # --- IOMMU Groups (Details) ---
    echo -e "\\n${CYAN}--- IOMMU Groups (Details) ---${NC}"
    if ! command -v find &>/dev/null || ! command -v ls &>/dev/null || ! command -v lspci &>/dev/null; then
        echo -e "${RED}Required commands (find, ls, lspci) not available to list IOMMU groups.${NC}"
    else
        local gpu_pci_bus_ids=()
        while IFS= read -r line; do
            gpu_pci_bus_ids+=("$(echo "$line" | awk '{print $1}')")
        done < <(lspci -nn | grep -iE "VGA compatible controller|3D controller")

        if [ ${#gpu_pci_bus_ids[@]} -eq 0 ]; then
            echo -e "${YELLOW}No VGA/3D controllers found to filter IOMMU groups.${NC}"
        else
            local overall_iommu_groups_found=false
            local gpu_related_group_displayed=false

            for iommu_group_dir in $(find /sys/kernel/iommu_groups/ -mindepth 1 -maxdepth 1 -type d 2>/dev/null | sort -V); do
                overall_iommu_groups_found=true
                local group_id=$(basename "$iommu_group_dir")
                local devices_in_group_output=""
                local this_group_contains_gpu=false

                for device_symlink_name in $(ls -1 "$iommu_group_dir"/devices/ 2>/dev/null | sort -V); do
                    local device_pci_bus_id_full="$device_symlink_name"
                    local device_pci_bus_id_short="${device_symlink_name#0000:}"
                    local device_info
                    device_info=$(lspci -nns "$device_pci_bus_id_full" 2>/dev/null)
                    
                    if [ -n "$device_info" ]; then
                        devices_in_group_output+="  ${device_info}\\n"
                        for gpu_id in "${gpu_pci_bus_ids[@]}"; do
                            if [[ "$device_pci_bus_id_short" == "$gpu_id" ]]; then
                                this_group_contains_gpu=true
                                break 
                            fi
                        done
                    else
                        devices_in_group_output+="  ${RED}Error reading device info for $device_pci_bus_id_full${NC}\\n"
                    fi
                done

                if $this_group_contains_gpu; then
                    echo -e "\\n${YELLOW}IOMMU Group ${group_id}:${NC}"
                    echo -e "${devices_in_group_output%\\n}" 
                    gpu_related_group_displayed=true
                fi
            done

            if ! $overall_iommu_groups_found; then
                echo -e "${YELLOW}No IOMMU groups found by 'find' command.${NC}"
            elif ! $gpu_related_group_displayed; then
                echo -e "${YELLOW}No IOMMU groups found containing the identified GPU(s):${NC} ${gpu_pci_bus_ids[*]}"
            fi
        fi
    fi

    # --- GPU Device Details (lspci -nnk) --- (NEW SECTION)
    echo -e "\\n${CYAN}--- GPU Device Details (lspci -nnk) ---${NC}"
    if [ -n "$all_device_blocks_output" ]; then
        echo -e "${all_device_blocks_output%\\n\\n}" # Print accumulated blocks, remove last two newlines
    elif $vga_devices_processed; then 
         echo -e "${YELLOW}No detailed GPU device blocks were formatted (e.g. lspci output was unusual or parsing issue).${NC}"
    else 
         echo -e "${YELLOW}No VGA compatible or 3D controllers found by lspci to display details.${NC}"
    fi
    # --- End of GPU Device Details ---

    # --- Loaded Kernel Modules (GPU related) ---
    echo -e "\\n${CYAN}--- Loaded Kernel Modules (GPU related) ---${NC}"
    if lsmod | command grep -E "vfio|nvidia|nouveau|amdgpu|radeon" --color=never; then
        : 
    else
        echo -e "${YELLOW}No common GPU-related modules (vfio, nvidia, nouveau, amdgpu, radeon) loaded.${NC}"
    fi

    echo -e "\\n${CYAN}--- GPU Passthrough Workflow & Runtime Status ---${NC}"
    echo -e "${CYAN}==================================================${NC}"

    # 1. Initial System Preparation (Configuration)
    echo -e "${MAGENTA}1. Initial System Preparation (Configuration):${NC}"
    
    local iommu_enabled_cmdline_status="$CROSS_MARK"
    # Use full path to grep and -E for extended regex for robust pattern matching
    if /usr/bin/grep -E -q 'iommu=pt|intel_iommu=on|amd_iommu=on' /proc/cmdline; then
        iommu_enabled_cmdline_status="$CHECK_MARK"
    fi
    echo -e "   ${iommu_enabled_cmdline_status} IOMMU Enabled in Kernel Command Line (gpu-pt1)"

    local vfio_modules_in_etc_modules_status="$CROSS_MARK"
    local vfio_ok=0; local vfio_iommu_type1_ok=0; local vfio_pci_ok=0
    if [ -f "$modules_file_path" ]; then
        if grep -qP "^\\s*vfio(\\s|$|#)" "$modules_file_path"; then vfio_ok=1; fi
        if grep -qP "^\\s*vfio_iommu_type1(\\s|$|#)" "$modules_file_path"; then vfio_iommu_type1_ok=1; fi
        if grep -qP "^\\s*vfio_pci(\\s|$|#)" "$modules_file_path"; then vfio_pci_ok=1; fi
    fi
    if [ "$vfio_ok" -eq 1 ] && [ "$vfio_iommu_type1_ok" -eq 1 ] && [ "$vfio_pci_ok" -eq 1 ]; then
        vfio_modules_in_etc_modules_status="$CHECK_MARK"
    fi
    echo -e "   ${vfio_modules_in_etc_modules_status} Core VFIO modules in $modules_file_path (gpu-pt2)"

    # 2. NVIDIA Host Driver Setup (Optional Configuration)
    echo -e "${MAGENTA}2. NVIDIA Host Driver Setup (Optional Configuration):${NC}"
    local nouveau_blacklisted_status="$QUESTION_MARK (Not found)"
    if [ -f "$blacklist_nouveau_conf_path" ]; then
        if grep -q "blacklist nouveau" "$blacklist_nouveau_conf_path"; then
            nouveau_blacklisted_status="$CHECK_MARK"
        else
            nouveau_blacklisted_status="$CROSS_MARK (Found, but 'blacklist nouveau' missing)"
        fi
    fi
    echo -e "   ${nouveau_blacklisted_status} Nouveau blacklisted for NVIDIA driver ($blacklist_nouveau_conf_path) (gpu-nds)"

    # 3. Persistent GPU Passthrough Configuration (gpu-pt3 enable status)
    echo -e "${MAGENTA}3. Persistent GPU Passthrough Configuration (gpu-pt3 enable):${NC}"
    local persistent_vfio_ids_status="$CROSS_MARK"
    if [ -f "$vfio_conf_path" ] && grep -qE "^options vfio-pci ids=" "$vfio_conf_path"; then
        persistent_vfio_ids_status="$CHECK_MARK"
    fi
    echo -e "   ${persistent_vfio_ids_status} GPUs assigned to vfio-pci at boot ($vfio_conf_path)"

    local persistent_blacklist_status="$CROSS_MARK"
    if [ -f "$passthrough_blacklist_conf_path" ] && grep -qE "blacklist (nvidia|nouveau|amdgpu|radeon)" "$passthrough_blacklist_conf_path"; then
        persistent_blacklist_status="$CHECK_MARK"
    fi
    echo -e "   ${persistent_blacklist_status} Host drivers blacklisted for selected GPUs ($passthrough_blacklist_conf_path)"

    # 4. Runtime Kernel Module & Driver Status
    echo -e "${MAGENTA}4. Runtime Kernel Module & Driver Status:${NC}"
    local vfio_pci_loaded_status="$CROSS_MARK"
    if lsmod | grep -q "vfio_pci"; then
        vfio_pci_loaded_status="$CHECK_MARK"
    fi
    echo -e "   ${vfio_pci_loaded_status} VFIO-PCI kernel module currently loaded"

    local other_gpu_modules_loaded="-"
    if lsmod | command grep -qE "nvidia|nouveau|amdgpu|radeon"; then # Use command grep
        other_gpu_modules_loaded="${GREEN}Host drivers (nvidia/nouveau/amdgpu/radeon) loaded${NC}"
    else
        other_gpu_modules_loaded="${YELLOW}No common host GPU drivers loaded${NC}"
    fi
    if lsmod | command grep -qE "vfio_iommu_type1|vfio"; then # Use command grep
        if [[ "$other_gpu_modules_loaded" == "-" || "$other_gpu_modules_loaded" == *"${YELLOW}No common host GPU drivers loaded${NC}"* ]]; then
            other_gpu_modules_loaded="${GREEN}VFIO modules (vfio/vfio_iommu_type1) loaded${NC}"
        else
            other_gpu_modules_loaded+=", ${GREEN}VFIO modules (vfio/vfio_iommu_type1) loaded${NC}"
        fi 
    fi
    echo -e "   - Other relevant modules: $other_gpu_modules_loaded"
    
    # 4.3 Kernel Drivers for GPU(s) Summary (MODIFIED SECTION)
    echo -e "   ${CYAN}--- Kernel Drivers for GPU(s) Summary ---${NC}" # Renamed title
    if [ -n "$gpu_details_for_checklist" ]; then
        echo -e "${gpu_details_for_checklist%\\n}" # Print accumulated checklist lines
    elif $vga_devices_processed; then 
         echo -e "     ${YELLOW}?${NC} No summary for GPU driver bindings could be generated."
    else 
         echo -e "     ${YELLOW}?${NC} No VGA/3D controllers found to summarize driver bindings."
    fi

    # Overall GPU State Determination
    echo -e "\n--- Overall GPU State Summary ---"
    if $is_any_gpu_on_vfio; then
        echo -e "[SUMMARY] GPU State: DETACHED (for VM use)"
        echo -e "  - At least one GPU device (VGA/3D controller) is bound to vfio-pci."
        if [ -n "$host_drivers_for_gpu" ]; then
             echo -e "  - Note: Other GPU devices might be ATTACHED to host (driver(s): $host_drivers_for_gpu). State could be MIXED."
        fi
    elif [ -n "$host_drivers_for_gpu" ]; then
        echo -e "[SUMMARY] GPU State: ATTACHED to host (driver(s): $host_drivers_for_gpu)"
        echo -e "  - GPU device(s) (VGA/3D controller) are using host driver(s): $host_drivers_for_gpu."
        echo -e "  - No VGA/3D controller found using vfio-pci."
    else
        if $vga_devices_processed; then # Check if any GPU was processed
            echo -e "[SUMMARY] GPU State: UNCLEAR / INDETERMINATE"
            echo -e "  - A VGA/3D controller was detected, but it's not using 'vfio-pci' nor any other recognized host driver."
            echo -e "  - It might be unbound. Check 'GPU Device Details' and IOMMU details."
        else # No VGA/3D controllers were found by lspci at all
            echo -e "[SUMMARY] GPU State: N/A (No VGA/3D GPU controllers detected)"
        fi
        echo -e "  - Review IOMMU groups and loaded kernel modules above."
    fi

    echo -e "\n--- GPU status check completed. (${function_name}) ---"
    echo -e "===================================================="
}
