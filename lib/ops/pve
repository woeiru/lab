#!/bin/bash

# Define directory and file variables
DIR_FUN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
FILE_FUN=$(basename "$BASH_SOURCE")
BASE_FUN="${FILE_FUN%.*}"
FILEPATH_FUN="${DIR_FUN}/${FILE_FUN}"
CONFIG_FUN="${SITE_CONFIG_FILE}"

# Dynamically create variables based on the base name
eval "FILEPATH_${BASE_FUN}=\$FILEPATH_FUN"
eval "FILE_${BASE_FUN}=\$FILE_FUN"
eval "BASE_${BASE_FUN}=\$BASE_FUN"
eval "CONFIG_${BASE_FUN}=\$CONFIG_FUN"

# Displays an overview of specific Proxmox Virtual Environment (PVE) related functions in the script, showing their usage, shortname, and description
# overview functions
# <script_path>
pve_fun() {
    # Technical Description:
    #   Invokes the 'aux_laf' (list available functions) utility.
    #   Accepts script path as explicit parameter for analysis.
    #   All remaining arguments are forwarded to 'aux_laf'.
    #   Primarily used for introspection of script capabilities.
    # Dependencies:
    #   - 'aux_laf' function.
    # Arguments:
    #   $1: script_path - path to the script file to analyze

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -lt 1 ]; then
        aux_use
        return 1
    fi

    local script_path="$1"
    
    if [ -z "$script_path" ] || [ ! -f "$script_path" ]; then
        aux_use
        return 1
    fi
    
    shift
    aux_laf "$script_path" "$@"
}

# Displays an overview of PVE-specific variables defined in the configuration file, showing their names, values, and usage across different files
# overview variables
# <config_file> <analysis_dir>
pve_var() {
    # Technical Description:
    #   Utilizes 'aux_acu' (analyze configuration usage) with the '-o' (overview) flag.
    #   First argument specifies the configuration file to scan.
    #   Second argument is passed as the scope for usage analysis.
    #   Helps in understanding how PVE-related variables from the site config are used across the project.
    # Dependencies:
    #   - 'aux_acu' function.
    # Arguments:
    #   $1: config_file - path to the configuration file to analyze
    #   $2: analysis_dir - directory scope for usage analysis

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 2 ]; then
        aux_use
        return 1
    fi

    local config_file="$1"
    local analysis_dir="$2"
    
    if [ -z "$config_file" ] || [ ! -f "$config_file" ]; then
        aux_use
        return 1
    fi
    
    if [ -z "$analysis_dir" ] || [ ! -d "$analysis_dir" ]; then
        aux_use
        return 1
    fi

    aux_acu -o "$config_file" "$analysis_dir"
}

# Disables specified Proxmox repository files by commenting out 'deb' lines, typically used to manage repository sources 
# disable repository
# -x (execute)
pve_dsr() {
    # Technical Description:
    #   Modifies Proxmox VE APT repository configuration files to disable them.
    #   Iterates through a predefined list of files:
    #     - "/etc/apt/sources.list.d/pve-enterprise.list"
    #     - "/etc/apt/sources.list.d/ceph.list"
    #   Uses 'sed -i' to comment out lines starting with 'deb' (e.g., 'deb http://...' becomes '#deb http://...').
    #   Calls 'aux_nos' for notifications about actions taken or if files are not found.
    # Dependencies:
    #   - 'sed' utility for in-place file editing.
    #   - 'aux_nos' function for status messages.
    #   - Root privileges are required to modify files in /etc/apt/.
    # Arguments:
    #   $1: -x - explicit execution flag required for safety and consistency

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 1 ] || [ "$1" != "-x" ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    files=(
        "/etc/apt/sources.list.d/pve-enterprise.list"
        "/etc/apt/sources.list.d/ceph.list"
    )

    for file in "${files[@]}"; do
        if [ -f "$file" ]; then
            sed -i '/^deb/ s/^/#/' "$file"
            aux_nos "$function_name" "Changes applied to $file"
        else
            aux_nos "$function_name" "File $file not found."
        fi
    done
}

# Removes the Proxmox subscription notice by modifying the web interface JavaScript file, with an option to restart the pveproxy service 
# remove sub notice
# -x (execute)
pve_rsn() {
    # Technical Description:
    #   Removes the "No valid subscription" notice from the Proxmox VE web interface.
    #   Modifies the JavaScript file '/usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js'.
    #   Uses 'sed -Ezi.bak' to perform an in-place, multiline-aware regular expression substitution.
    #   The regex targets 'Ext.Msg.show(...title: gettext('No valid sub')...' and prepends 'void({ //',
    #   effectively commenting out the JavaScript call that displays the notice. A backup '.bak' file is created.
    #   Prompts the user to restart 'pveproxy.service' using 'systemctl'.
    # Dependencies:
    #   - 'sed' utility (GNU version for -z and -i).
    #   - 'systemctl' for service management.
    #   - 'aux_nos' for status messages.
    #   - Root privileges required for file modification and service restart.
    # Arguments:
    #   $1: -x - explicit execution flag required for safety and consistency

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 1 ] || [ "$1" != "-x" ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    sed -Ezi.bak "s/(Ext\.Msg\.show\(\{\s+title: gettext\('No valid sub)/void\(\{ \/\/\1/g" /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js

    # Prompt user whether to restart the service
    read -p "Do you want to restart the pveproxy.service now? (y/n): " choice
    case "$choice" in
        y|Y ) systemctl restart pveproxy.service && aux_nos "$function_name" "Service restarted successfully.";;
        n|N ) aux_nos "$function_name" "Service not restarted.";;
        * ) aux_nos "$function_name" "Invalid choice. Service not restarted.";;
    esac
}

# Updates the Proxmox VE Appliance Manager (pveam) container template list
# container list update
# -x (execute)
pve_clu() {
    # Technical Description:
    #   Updates the local cache of available LXC container templates for Proxmox VE.
    #   Executes the 'pveam update' command. 'pveam' is the Proxmox VE Appliance Manager.
    #   This command fetches the latest list of templates from the configured sources.
    #   Calls 'aux_nos' to confirm execution.
    # Dependencies:
    #   - 'pveam' command-line utility.
    #   - 'aux_nos' function.
    #   - Network access to Proxmox template repositories.
    # Arguments:
    #   $1: -x - explicit execution flag required for safety and consistency

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 1 ] || [ "$1" != "-x" ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"

    pveam update

    aux_nos "$function_name" "executed"
}

# Downloads a specified container template to a given storage location, with error handling and options to list available templates
# container downloads
# <storage_id> <template_name>
pve_cdo() {
    # Technical Description:
    #   Downloads a specified LXC container template to a designated Proxmox VE storage.
    #   Uses 'pveam download <storage_id> <template_name>'. Output is initially suppressed (2>/dev/null).
    #   Includes interactive error handling:
    #     - If download fails, it prompts the user to view available templates ('pveam available').
    #     - If the user agrees, it then prompts if they want to try downloading a different template.
    #     - If yes, it recursively calls 'pve_cdo' with the new template name.
    # Dependencies:
    #   - 'pveam' command-line utility.
    #   - User interaction for error handling.
    # Arguments:
    #   $1: storage_id - The ID of the Proxmox storage where the template will be downloaded
    #   $2: template_name - The name of the container template to download (e.g., 'alpine-3.18-default_20230615_amd64.tar.xz')

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 2 ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    local ct_dl_sto="$1"
    local ct_dl="$2"
    
    # Attempt to download the template
    if ! pveam download "$ct_dl_sto" "$ct_dl" 2>/dev/null; then
        echo "Error: Unable to download template '$ct_dl'."
        
        # Ask user if they want to see available templates
        read -p "Would you like to see a list of available templates? (y/n): " answer
        
        if [[ "$answer" =~ ^[Yy]$ ]]; then
            echo "Available templates:"
            pveam available
            
            # Ask user if they want to try downloading a different template
            read -p "Would you like to try downloading a different template? (y/n): " retry_answer
            
            if [[ "$retry_answer" =~ ^[Yy]$ ]]; then
                read -p "Enter the name of the template you'd like to download: " new_ct_dl
                pve_cdo "$ct_dl_sto" "$new_ct_dl"
                return
            fi
        fi
        
        echo "$function_name: Failed to download template ( $ct_dl )"
    else
        echo "$function_name: Successfully downloaded template ( $ct_dl )"
    fi
}

# Configures a bind mount for a specified Proxmox container, linking a host directory to a container directory
# container bindmount
# <vmid> <mphost> <mpcontainer>
pve_cbm() {
    # Technical Description:
    #   Configures a bind mount for a Proxmox LXC container.
    #   Uses 'pct set <vmid> -mp0 <host_path>,mp=<container_path>' to define the mount point.
    #   '-mp0' refers to mount point 0; additional mount points would use -mp1, -mp2, etc.
    #   Requires exactly 3 arguments; validates all parameters before execution.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - 'aux_use' for usage display on incorrect arguments.
    #   - 'aux_nos' for success notification.
    # Arguments:
    #   $1: vmid - The numeric ID of the LXC container
    #   $2: mphost - The absolute path on the Proxmox host to be mounted into the container
    #   $3: mpcontainer - The absolute path inside the container where the host path will be mounted

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 3 ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    local vmid="$1"
    local mphost="$2"
    local mpcontainer="$3"

    # Ensure all arguments are provided
    if [[ -z "$vmid" || -z "$mphost" || -z "$mpcontainer" ]]; then
        aux_use
        return 1
    fi

    # Properly quote the entire argument for -mp0
    pct set "$vmid" -mp0 "$mphost,mp=$mpcontainer"

    aux_nos "$function_name" "executed ( $vmid / $mphost / $mpcontainer )"
}

# Sets up different containers specified in cfg/env/site.
# container create
# <id> <template> <hostname> <storage> <rootfs_size> <memory> <swap> <nameserver> <searchdomain> <password> <cpus> <privileged> <ip_address> <cidr> <gateway> <ssh_key_file> <net_bridge> <net_nic>
pve_ctc() {

    # Technical Description:
    #   Creates a new Proxmox LXC container using 'pct create'.
    #   Accepts a large number of positional arguments mapping to 'pct create' options.
    #   Key parameters include ID, template, hostname, storage, rootfs size, memory, network settings (IP, CIDR, gateway, bridge, NIC name), SSH keys, etc.
    #   Checks for the existence of the specified SSH key file ('$ssh_key_file') before proceeding.
    #   Conditionally adds '--unprivileged' if the 'privileged' argument is "no".
    #   Sets features 'keyctl=1' and 'nesting=1' by default.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - Valid Proxmox VE environment and resources (template, storage, network).
    #   - An existing SSH public key file.
    # Arguments: 
    #   $1: id - Container ID
    #   $2: template - Container template path
    #   $3: hostname - Container hostname
    #   $4: storage - Storage location
    #   $5: rootfs_size - Root filesystem size
    #   $6: memory - Memory allocation
    #   $7: swap - Swap allocation
    #   $8: nameserver - DNS nameserver
    #   $9: searchdomain - DNS search domain
    #   $10: password - Root password
    #   $11: cpus - CPU count
    #   $12: privileged - Privileged mode (yes/no)
    #   $13: ip_address - IP address
    #   $14: cidr - Network CIDR
    #   $15: gateway - Network gateway
    #   $16: ssh_key_file - SSH public key file path
    #   $17: net_bridge - Network bridge
    #   $18: net_nic - Network interface name

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 18 ]; then
        aux_use
        return 1
    fi

    local id="$1"
    local template="$2"
    local hostname="$3"
    local storage="$4"
    local rootfs_size="$5"
    local memory="$6"
    local swap="$7"
    local nameserver="$8"
    local searchdomain="$9"
    local password="${10}"
    local cpus="${11}"
    local privileged="${12}"
    local ip_address="${13}"
    local cidr="${14}"
    local gateway="${15}"
    local ssh_key_file="${16}"
    local net_bridge="${17}"
    local net_nic="${18}"

    if [ ! -f "$ssh_key_file" ]; then
        echo "SSH key file $ssh_key_file does not exist. Aborting."
        return 1
    fi

    # Correcting the parameters passed to pct create
    pct create "$id" "$template" \
        --hostname "$hostname" \
        --storage "$storage" \
        --rootfs "$storage:$rootfs_size" \
        --memory "$memory" \
        --swap "$swap" \
        --net0 "name=$net_nic,bridge=$net_bridge,ip=$ip_address/$cidr,gw=$gateway" \
        --nameserver "$nameserver" \
        --searchdomain "$searchdomain" \
        --password "$password" \
        --cores "$cpus" \
        --features "keyctl=1,nesting=1" \
        $(if [ "$privileged" == "no" ]; then echo "--unprivileged"; fi) \
        --ssh-public-keys "$ssh_key_file"
}

# Manages multiple Proxmox containers by starting, stopping, enabling, or disabling them, supporting individual IDs, ranges, or all containers
# container toggle
# <start|stop|enable|disable> <containers|all>
pve_cto() {
    # Technical Description:
    #   Manages the state (start, stop) and on-boot behavior (enable, disable) of Proxmox LXC containers.
    #   First argument 'action': 'start', 'stop', 'enable', or 'disable'.
    #   Subsequent arguments specify target containers:
    #     - Individual container IDs (e.g., 101 102).
    #     - Ranges of IDs (e.g., 100-105).
    #     - 'all' to target all containers (retrieved via 'pct list | awk').
    #   'start'/'stop' actions use 'pct start <vmid>' and 'pct stop <vmid>'.
    #   'enable'/'disable' actions modify the 'onboot:' line in the container's config file
    #   ('/etc/pve/lxc/<vmid>.conf') using 'sed' to set it to 'onboot: 1' or 'onboot: 0'.
    #   Lists container status using 'pct list' after 'start' or 'stop' actions.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - 'sed' and 'awk' utilities.
    #   - Container configuration files at '/etc/pve/lxc/'.
    # Arguments:
    #   $1: action - The action to perform: start, stop, enable, or disable
    #   $2+: containers - Container IDs, ranges (e.g., 100-105), or 'all'

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -lt 2 ]; then
        aux_use
        return 1
    fi

    local action=$1
    shift
    
    if [[ $action != "start" && $action != "stop" && $action != "enable" && $action != "disable" ]]; then
        aux_use
        return 1
    fi
    
    handle_action() {
        local vmid=$1
        local config_file="/etc/pve/lxc/${vmid}.conf"
        
        if [[ ! -f "$config_file" ]]; then
            echo "ERROR: Config file for container $vmid does not exist"
            return 1
        fi
        
        case $action in
            start)
                echo "Starting container $vmid"
                pct start "$vmid"
                ;;
            stop)
                echo "Stopping container $vmid"
                pct stop "$vmid"
                ;;
            enable|disable)
                local onboot_value=$([[ $action == "enable" ]] && echo 1 || echo 0)
                echo "Setting onboot to $onboot_value for container $vmid"
                
                sed -i '/^onboot:/d' "$config_file"
                echo "onboot: $onboot_value" >> "$config_file"
                
                echo "Container $vmid configuration updated:"
                grep '^onboot:' "$config_file" || echo "ERROR: onboot entry not found after modification"
                ;;
        esac
    }
    
    if [[ $1 == "all" ]]; then
        echo "Processing all containers"
        container_ids=$(pct list | awk 'NR>1 {print $1}')
        for vmid in $container_ids; do
            handle_action "$vmid"
        done
    else
        for arg in "$@"; do
            if [[ $arg == *-* ]]; then
                IFS='-' read -r start end <<< "$arg"
                for (( vmid=start; vmid<=end; vmid++ )); do
                    handle_action "$vmid"
                done
            else
                handle_action "$arg"
            fi
        done
    fi
    
    if [[ $action == "start" || $action == "stop" ]]; then
        echo "Current container status:"
        pct list
    fi
}

# Deploys or modifies the VM shutdown hook for GPU reattachment
# vm shutdown hook
# <operation> <vm_id> <hook_script> <lib_ops_dir> [vm_id2...]
pve_vmd() {
    # Technical Description:
    #   Manages a VM shutdown hook script for GPU reattachment in Proxmox VE.
    #   Operations: 'add', 'remove', 'debug', specified as the first argument ($1). VM ID is $2.
    #   The hook script is located at '/var/lib/vz/snippets/gpu-reattach-hook.pl'.
    #   'create_or_update_hook_script' inner function generates/updates this Perl script.
    #   The Perl script logs its execution to '/var/log/gpu-reattach-hook.log'.
    #   On 'post-stop' phase, the Perl script executes:
    #     bash -c 'source <lib_ops_dir>/pve && gpu_pta'
    #     This implies 'gpu_pta' is another script/function responsible for the actual GPU reattachment logic.
    #   'add' uses 'qm set <vmid> -hookscript local:snippets/gpu-reattach-hook.pl'.
    #   'remove' uses 'qm set <vmid> -delete hookscript'.
    #   'debug' provides diagnostic info (script content, permissions, qm config, manual trigger).
    # Dependencies:
    #   - 'qm' command-line utility.
    #   - Perl interpreter (for the hook script).
    #   - 'gpu_pta' script/function (defined elsewhere, sourced via lib_ops_dir).
    #   - 'aux_nos' for notifications.
    #   - Directory '/var/lib/vz/snippets' must be writable.
    # Arguments:
    #   $1: operation - The operation to perform: add, remove, or debug
    #   $2: vm_id - The VM ID to configure
    #   $3: hook_script - Path to the hook script
    #   $4: lib_ops_dir - Directory containing the ops library
    #   $5+: additional_vm_ids (optional) - Additional VM IDs to configure

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -lt 4 ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    local operation="$1"
    local vm_id="$2"
    local hook_script="$3"
    local lib_ops_dir="$4"

    # Validate required parameters
    if [[ -z "$hook_script" || -z "$lib_ops_dir" ]]; then
        aux_use
        return 1
    fi

    echo "Debug: Operation: $operation, VM ID: $vm_id"

    # Validate the provided operation.
    if [[ ! "$operation" =~ ^(add|remove|debug)$ ]]; then
        echo "Error: Invalid operation. Use 'add', 'remove', or 'debug'."
        return 1
    fi

    # Ensure the directory for Proxmox snippets exists, creating it if necessary.
    # This directory is where the hook script will be stored.
    if [ ! -d "/var/lib/vz/snippets" ]; then
        echo "Creating /var/lib/vz/snippets directory..."
        if ! mkdir -p "/var/lib/vz/snippets"; then
            echo "Error: Failed to create /var/lib/vz/snippets directory. Aborting."
            return 1
        fi
    fi

    # Defines an inner function to create or update the GPU reattachment hook script.
    # The script is written in Perl and logs its activity.
    # On VM 'post-stop', it executes 'gpu_pta' to handle GPU reattachment.
    create_or_update_hook_script() {
        if ! cat > "$hook_script" << EOL
#!/usr/bin/perl

use strict;
use warnings;
use POSIX qw(strftime);

my \$vmid = shift;
my \$phase = shift;

my \$log_file = '/var/log/gpu-reattach-hook.log';

open(my \$log, '>>', \$log_file) or die "Could not open log file: \$!";
print \$log strftime("%Y-%m-%d %H:%M:%S", localtime) . " - VM \$vmid, Phase: \$phase\n";

if (\$phase eq 'post-stop') {
    print \$log "Attempting to reattach GPU for VM \$vmid\n";
    my \$result = system("bash -c 'source $lib_ops_dir/pve && gpu_pta'");
    print \$log "gpu_pta execution result: \$result\n";
}

close(\$log);
EOL
        then
            echo "Error: Failed to create or update hook script. Aborting."
            return 1
        fi

        if ! chmod 755 "$hook_script"; then
            echo "Error: Failed to set permissions on hook script. Aborting."
            return 1
        fi
        echo "Hook script created/updated and made executable."
    }

    # Create or update hook script
    if ! create_or_update_hook_script; then
        return 1
    fi

    if [ "$operation" = "debug" ]; then
        echo "Debugging hook setup:"
        echo "1. Checking hook script existence and permissions:"
        ls -l "$hook_script"
        echo "2. Checking hook script content:"
        cat "$hook_script"
        echo "3. Checking Proxmox VM configurations for hook references:"
        grep -r "hookscript" /etc/pve/qemu-server/
        echo "4. Manually triggering hook script:"
        perl "$hook_script" "$vm_id" "post-stop"
        echo "5. Checking log file:"
        cat /var/log/gpu-reattach-hook.log
        return 0
    fi

    # Perform operation
    case "$operation" in
        add)
            if qm set "$vm_id" -hookscript "local:snippets/$(basename "$hook_script")"; then
                echo "Hook applied to VM $vm_id"
            else
                echo "Failed to apply hook to VM $vm_id"
                return 1
            fi
            ;;
        remove)
            if qm set "$vm_id" -delete hookscript; then
                echo "Hook removed from VM $vm_id"
            else
                echo "Failed to remove hook from VM $vm_id"
                return 1
            fi
            ;;
    esac

    aux_nos "$function_name" "Hook $operation completed for VM $vm_id"
}

# Sets up different virtual machines specified in cfg/env/site.
# virtual machine create
# <id> <name> <ostype> <machine> <iso> <boot> <bios> <efidisk> <scsihw> <agent> <disk> <sockets> <cores> <cpu> <memory> <balloon> <net>
pve_vmc() {

    # Technical Description:
    #   Creates a new Proxmox QEMU/KVM virtual machine using 'qm create'.
    #   Accepts numerous positional arguments that map directly to 'qm create' options.
    #   Key parameters include VM ID, name, OS type, machine type, ISO image (as IDE2 CD-ROM),
    #   boot order, BIOS type, EFI disk, SCSI hardware controller, QEMU guest agent enablement,
    #   SCSI disk, CPU configuration (sockets, cores, type), memory (static and ballooning),
    #   and network interface configuration (net0).
    # Dependencies:
    #   - 'qm' command-line utility.
    #   - Valid Proxmox VE environment and resources (ISO, storage, network).
    # Arguments: 
    #   $1: id - VM ID
    #   $2: name - VM name
    #   $3: ostype - OS type
    #   $4: machine - Machine type
    #   $5: iso - ISO image path
    #   $6: boot - Boot order
    #   $7: bios - BIOS type
    #   $8: efidisk - EFI disk configuration
    #   $9: scsihw - SCSI hardware controller
    #   $10: agent - QEMU guest agent setting
    #   $11: disk - SCSI disk configuration
    #   $12: sockets - CPU sockets
    #   $13: cores - CPU cores
    #   $14: cpu - CPU type
    #   $15: memory - Memory allocation
    #   $16: balloon - Memory ballooning
    #   $17: net - Network configuration

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 17 ]; then
        aux_use
        return 1
    fi

    local id="$1"
    local name="$2"
    local ostype="$3"
    local machine="$4"
    local iso="$5"
    local boot="$6"
    local bios="$7"
    local efidisk="$8"
    local scsihw="$9"
    local agent="${10}"
    local disk="${11}"
    local sockets="${12}"
    local cores="${13}"
    local cpu="${14}"
    local memory="${15}"
    local balloon="${16}"
    local net="${17}"

    qm create "$id" \
        --name "$name" \
        --ostype "$ostype" \
        --machine "$machine" \
        --ide2 "$iso,media=cdrom" \
        --boot "$boot" \
        --bios "$bios" \
        --efidisk0 "$efidisk" \
        --scsihw "$scsihw" \
        --agent "$agent" \
        --scsi0 "$disk" \
        --sockets "$sockets" \
        --cores "$cores" \
        --cpu "$cpu" \
        --memory "$memory" \
        --balloon "$balloon" \
        --net0 "$net"
}

# Starts a VM on the current node or migrates it from another node, with an option to shut down the source node after migration
# vm start get shutdown
# <vm_id> <cluster_nodes_str> <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path> [s: optional, shutdown other node]
pve_vms() {

    # Technical Description:
    #   Starts a Proxmox VM, migrating it from another node if necessary.
    #   Arguments: 
    #     $1 (vm_id): VM ID
    #     $2 (cluster_nodes_str): Space-separated cluster node names
    #     $3 (pci0_id): PCI ID for hostpci0
    #     $4 (pci1_id): PCI ID for hostpci1
    #     $5 (core_count_on): VM core count when passthrough is on
    #     $6 (core_count_off): VM core count when passthrough is off
    #     $7 (usb_devices_str): Space-separated USB device definitions
    #     $8 (pve_conf_path): Path to PVE config directory
    #     $9 (optional 's' to shutdown the source/original node).
    #   1. Retrieves current hostname.
    #   2. Uses 'pve_vck <vm_id> <cluster_nodes_str>' to determine the node currently hosting the VM ('node_id').
    #   3. If current hostname matches 'node_id' (VM is local):
    #      - Starts VM using 'qm start <vm_id>'.
    #      - If $9 is 's', attempts to shut down 'node_id' (which is the current node) via SSH.
    #   4. If current hostname differs from 'node_id' (VM is remote):
    #      - Calls 'pve-vmg <vm_id> <cluster_nodes_str> <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>' to migrate the VM to the current node.
    #      - Starts VM using 'qm start <vm_id>'.
    #      - If $9 is 's', shuts down the original remote node ('node_id') via SSH.
    # Dependencies:
    #   - 'pve_vck' function (to find VM's current node).
    #   - 'pve-vmg' function (to migrate VM).
    #   - 'qm' command-line utility.
    #   - 'ssh' for remote shutdown.
    #   - 'hostname' utility.
    #   - 'aux_use' for usage display.

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -lt 8 ] || [ $# -gt 9 ]; then
        aux_use
        return 1
    fi

    local vm_id="$1"
    local cluster_nodes_str="$2"
    local pci0_id="$3"
    local pci1_id="$4"
    local core_count_on="$5"
    local core_count_off="$6"
    local usb_devices_str="$7"
    local pve_conf_path="$8"
    local shutdown_flag="$9"

    local hostname=$(hostname)

    # Check if vm_id argument is provided
    if [ -z "$vm_id" ]; then
        aux_use
        return 1
    fi

    # Call pve_vck function to get node_id
    local node_id=$(pve_vck "$vm_id" "$cluster_nodes_str")

    # Check if node_id is empty
    if [ -z "$node_id" ]; then
        echo "Node ID is empty. Cannot proceed."
        return 1
    fi

    # Main logic
    if [ "$hostname" = "$node_id" ]; then
        qm start "$vm_id"
        if [ "$shutdown_flag" = "s" ]; then
            # Shutdown the other node
            echo "Shutting down node $node_id"
            ssh "root@$node_id" "shutdown now"
        fi 
    else
        pve-vmg "$vm_id" "$cluster_nodes_str" "$pci0_id" "$pci1_id" "$core_count_on" "$core_count_off" "$usb_devices_str" "$pve_conf_path"
        qm start "$vm_id"
        if [ "$shutdown_flag" = "s" ]; then
            # Shutdown the other node
            echo "Shutting down node $node_id"
            ssh "root@$node_id" "shutdown now"
        fi
    fi
}

# Migrates a VM from a remote node to the current node, handling PCIe passthrough disable/enable during the process
# vm get start
# <vm_id> <cluster_nodes_str> <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>
pve_vmg() {

    # Technical Description:
    #   Migrates a Proxmox VM from a remote node to the current node, handling PCIe passthrough.
    #   Arguments: 
    #     $1 (vm_id): VM ID
    #     $2 (cluster_nodes_str): Space-separated cluster node names
    #     $3 (pci0_id): PCI ID for hostpci0
    #     $4 (pci1_id): PCI ID for hostpci1
    #     $5 (core_count_on): VM core count when passthrough is on
    #     $6 (core_count_off): VM core count when passthrough is off
    #     $7 (usb_devices_str): Space-separated USB device definitions
    #     $8 (pve_conf_path): Path to PVE config directory
    #   1. Uses 'pve_vck <vm_id> <cluster_nodes_str>' to find the VM's current remote node ('node').
    #   2. If VM is found on a remote node:
    #      a. Disables PCIe passthrough on the remote node: 'ssh <node> "pve-vpt <vm_id> off <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>"'.
    #      b. Migrates the VM to the current node: 'ssh <node> "qm migrate <vm_id> $(hostname)"'.
    #      c. Enables PCIe passthrough on the current (target) node: 'pve-vpt <vm_id> on <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>'.
    #   Requires 'pve-vpt' to be available and executable via SSH on remote nodes.
    # Dependencies:
    #   - 'pve_vck' function.
    #   - 'pve-vpt' function (both locally and on remote nodes).
    #   - 'qm' command-line utility (on remote node for migration).
    #   - 'ssh' for remote command execution.
    #   - 'hostname' utility.
    #   - 'aux_use' for usage display.

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 8 ]; then
        aux_use
        return 1
    fi

    local vm_id="$1"
    local cluster_nodes_str="$2"
    local pci0_id="$3"
    local pci1_id="$4"
    local core_count_on="$5"
    local core_count_off="$6"
    local usb_devices_str="$7"
    local pve_conf_path="$8"

    # Call pve_vck to check if VM exists and get the node
    local node=$(pve_vck "$vm_id" "$cluster_nodes_str")
    if [ -n "$node" ]; then
        echo "VM found on node: $node"

        # Disable PCIe passthrough for the VM on the remote node
        if ! ssh "$node" "pve-vpt $vm_id off $pci0_id $pci1_id $core_count_on $core_count_off \"$usb_devices_str\" $pve_conf_path"; then
            echo "Failed to disable PCIe passthrough for VM on $node." >&2
            return 1
        fi

        # Migrate the VM to the current node
        if ! ssh "$node" "qm migrate $vm_id $(hostname)"; then
            echo "Failed to migrate VM from $node to $(hostname)." >&2
            return 1
        fi

        # Enable PCIe passthrough for the VM on the current node
        if ! pve-vpt "$vm_id" on "$pci0_id" "$pci1_id" "$core_count_on" "$core_count_off" "$usb_devices_str" "$pve_conf_path"; then
            echo "Failed to enable PCIe passthrough for VM on $(hostname)." >&2
            return 1
        fi

        echo "VM migrated and PCIe passthrough enabled."
        return 0  # Return success once VM is found and migrated
    fi

    echo "VM not found on any other node."
    return 1  # Return failure if VM is not found
}

# Toggles PCIe passthrough configuration for a specified VM, modifying its configuration file to enable or disable passthrough devices
# vm passthrough toggle
# <vm_id> <on|off> <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>
pve_vpt() {

    # Technical Description:
    #   Toggles PCIe and USB device passthrough for a Proxmox VM by modifying its configuration file.
    #   Arguments: 
    #     $1 (vm_id): VM ID
    #     $2 (action: 'on' or 'off')
    #     $3 (pci0_id): PCI ID for hostpci0
    #     $4 (pci1_id): PCI ID for hostpci1  
    #     $5 (core_count_on): VM core count when passthrough is on
    #     $6 (core_count_off): VM core count when passthrough is off
    #     $7 (usb_devices_str): Space-separated USB device definitions
    #     $8 (pve_conf_path): Path to PVE config directory
    #   VM config file path: '<pve_conf_path>/<vm_id>.conf' (e.g., /etc/pve/qemu-server/<vm_id>.conf).
    #   'on' action:
    #     - Sets 'cores' to 'core_count_on'.
    #     - Adds 'hostpci0', 'hostpci1', and USB devices to the config file.
    #       These are inserted after the first '[section_header]' line found by awk.
    #   'off' action:
    #     - Sets 'cores' to 'core_count_off'.
    #     - Removes all lines starting with 'usb[0-9]*:' or 'hostpci[0-9]*:' from the config.
    # Dependencies:
    #   - 'sed', 'awk', 'hostname' utilities.
    #   - 'aux_use' for usage display.

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 8 ]; then
        aux_use
        return 1
    fi
    
    local vm_id="$1"
    local action="$2"
    local pci0_id="$3"
    local pci1_id="$4"
    local core_count_on="$5"
    local core_count_off="$6"
    local usb_devices_str="$7"
    local pve_conf_path="$8"
    
    local vm_conf="$pve_conf_path/$vm_id.conf"
    
    # Validate required parameters
    if [[ -z "$vm_id" || -z "$action" || -z "$pci0_id" || -z "$pci1_id" || -z "$core_count_on" || -z "$core_count_off" || -z "$pve_conf_path" ]]; then
        aux_use
        return 1
    fi
    
    # Parse USB devices string into array
    IFS=' ' read -ra usb_devices <<< "$usb_devices_str"

    # Find the starting line of the VM configuration section
    local section_start=$(awk '/^\[/{print NR-1; exit}' "$vm_conf")

    # Action based on the parameter
    case "$action" in
        on)
            # Set core count based on configuration when toggled on
            sed -i "s/cores:.*/cores: $core_count_on/" "$vm_conf"

            # Add passthrough lines
            if [ -z "$section_start" ]; then
                # If no section found, append passthrough lines at the end of the file
                for usb_device in "${usb_devices[@]}"; do
                    [ -n "$usb_device" ] && echo "$usb_device" >> "$vm_conf"
                done
                echo "hostpci0: $pci0_id,pcie=1,x-vga=1" >> "$vm_conf"
                echo "hostpci1: $pci1_id,pcie=1" >> "$vm_conf"
            else
                # If a section is found, insert passthrough lines at the appropriate position
                for usb_device in "${usb_devices[@]}"; do
                    if [ -n "$usb_device" ]; then
                        sed -i "${section_start}a\\${usb_device}" "$vm_conf"
                        ((section_start++))
                    fi
                done
                sed -i "${section_start}a\\hostpci0: $pci0_id,pcie=1,x-vga=1" "$vm_conf"
                ((section_start++))
                sed -i "${section_start}a\\hostpci1: $pci1_id,pcie=1" "$vm_conf"
            fi

            echo "Passthrough lines added to $vm_conf."
            ;;
        off)
            # Set default core count when toggled off
            sed -i "s/cores:.*/cores: $core_count_off/" "$vm_conf"

            # Remove passthrough lines
            sed -i '/^usb[0-9]*:/d; /^hostpci[0-9]*:/d' "$vm_conf"

            echo "Passthrough lines removed from $vm_conf."
            ;;
        *)
            echo "Invalid parameter. Usage: pve-vpt <VM_ID> <on|off>"
            exit 1
            ;;
    esac
}

# Checks and reports which node in the Proxmox cluster is currently hosting a specified VM
# vm check node
# <vm_id> <cluster_nodes_array>
pve_vck() {

    # Technical Description:
    #   Determines which node in a Proxmox VE cluster is currently hosting the specified Virtual Machine (VM).
    #   It iterates through a list of cluster nodes provided as the second parameter.
    #   For each node, it uses `ssh` to execute `qm list` on remote nodes or executes `qm list` directly
    #   on the local node.
    #   The output of `qm list` is then parsed using `grep` to check if the given `vm_id` is present.
    #   If the VM is found, the function outputs the hostname of the node and returns 0.
    #   If the VM is not found on any node after checking all, it returns 1.
    #   Debug messages are printed to stderr if `set -x` or similar debugging is enabled.
    # Dependencies:
    #   - `qm` command-line utility (Proxmox VE).
    #   - `ssh` client for remote node checks.
    #   - `grep` utility for parsing command output.
    #   - `hostname` utility to identify the current node.
    #   - `aux_use` function for displaying usage if `vm_id` is not provided.

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi

    echo "DEBUG: pve_vck called with arguments: $@" >&2
    local vm_id="$1"
    local cluster_nodes_str="$2"
    local found_node=""
    echo "DEBUG: pve_vck: vm_id: $vm_id" >&2

    if [ -z "$vm_id" ]; then
        echo "ERR: pve_vck: VM ID not provided." >&2
        echo "DEBUG: pve_vck: vm_id is empty, calling aux_use" >&2
        aux_use
        return 1
    fi

    if [ -z "$cluster_nodes_str" ]; then
        echo "ERR: pve_vck: cluster_nodes parameter not provided." >&2
        echo "DEBUG: pve_vck: cluster_nodes is empty" >&2
        aux_use
        return 1
    fi

    # Parse the cluster nodes string into an array
    IFS=' ' read -ra cluster_nodes <<< "$cluster_nodes_str"
    
    if [ ${#cluster_nodes[@]} -eq 0 ]; then
        echo "ERR: pve_vck: cluster_nodes array is empty." >&2
        echo "DEBUG: pve_vck: cluster_nodes array is empty" >&2
        return 1
    fi
    
    echo "DEBUG: pve_vck: cluster_nodes: (${cluster_nodes[*]})" >&2

    local current_hostname
    current_hostname=$(hostname)
    echo "DEBUG: pve_vck: current_hostname: $current_hostname" >&2

    for node in "${cluster_nodes[@]}"; do
        echo "DEBUG: pve_vck: checking node: $node" >&2
        local qm_list_output
        if [ "$node" != "$current_hostname" ]; then
            echo "DEBUG: pve_vck: checking remote node $node via ssh qm list for vm_id $vm_id" >&2
            # Capture output and errors from ssh command
            qm_list_output=$(ssh "$node" "qm list")
            local ssh_exit_code=$?
            echo "DEBUG: pve_vck: ssh $node \\"qm list\\" exit code: $ssh_exit_code" >&2
            echo "DEBUG: pve_vck: output from ssh $node \\"qm list\\":" >&2
            echo "$qm_list_output" >&2 # Print the raw output

            if [ $ssh_exit_code -eq 0 ] && echo "$qm_list_output" | grep -q -w -- "$vm_id"; then
                echo "DEBUG: pve_vck: VM $vm_id found on remote node $node" >&2
                found_node="$node"
                break
            else
                echo "DEBUG: pve_vck: VM $vm_id NOT found on remote node $node (ssh qm list or grep failed)" >&2
            fi
        else
            echo "DEBUG: pve_vck: checking local node $node via qm list for vm_id $vm_id" >&2
            qm_list_output=$(qm list)
            local qm_exit_code=$?
            echo "DEBUG: pve_vck: local qm list exit code: $qm_exit_code" >&2
            echo "DEBUG: pve_vck: output from local qm list:" >&2
            echo "$qm_list_output" >&2 # Print the raw output

            if [ $qm_exit_code -eq 0 ] && echo "$qm_list_output" | grep -q -w -- "$vm_id"; then
                echo "DEBUG: pve_vck: VM $vm_id found on local node $node" >&2
                found_node="$node"
                break
            else
                echo "DEBUG: pve_vck: VM $vm_id NOT found on local node $node (local qm list or grep failed)" >&2
            fi
        fi
    done

    if [ -n "$found_node" ]; then
        echo "DEBUG: pve_vck: final found_node: $found_node, returning 0" >&2
        echo "$found_node" # This is the actual output for command substitution
        return 0
    else
        echo "DEBUG: pve_vck: VM $vm_id not found on any cluster node, returning 1" >&2
        # No stdout echo here, so command substitution gets empty string
        return 1
    fi
}
