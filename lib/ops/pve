#!/bin/bash

# ============================================================================
# pve - Function Summary
#
#   pve-fun : Displays an overview of specific Proxmox Virtual Environment (PVE) related functions in the script, showing their usage, shortname, and description.
#   pve-var : Displays an overview of PVE-specific variables defined in the configuration file, showing their names, values, and usage across different files.
#   pve-dsr : Disables specified Proxmox repository files by commenting out 'deb' lines, typically used to manage repository sources.
#   pve-rsn : Removes the Proxmox subscription notice by modifying the web interface JavaScript file, with an option to restart the pveproxy service.
#   pve-clu : Updates the Proxmox VE Appliance Manager (pveam) container template list.
#   pve-cdo : Downloads a specified container template to a given storage location, with error handling and options to list available templates.
#   pve-cbm : Configures a bind mount for a specified Proxmox container, linking a host directory to a container directory.
#   pve-ctc : Sets up different containers specified in cfg/env/site.
#   pve-cto : Manages multiple Proxmox containers by starting, stopping, enabling, or disabling them, supporting individual IDs, ranges, or all containers.
#   pve-vmd : Deploys or modifies the VM shutdown hook for GPU reattachment.
#   pve-vmc : Sets up different virtual machines specified in cfg/env/site.
#   pve-vms : Starts a VM on the current node or migrates it from another node, with an option to shut down the source node after migration.
#   pve-vmg : Migrates a VM from a remote node to the current node, handling PCIe passthrough disable/enable during the process.
#   pve-vpt : Toggles PCIe passthrough configuration for a specified VM, modifying its configuration file to enable or disable passthrough devices.
#   pve-vck : Checks and reports which node in the Proxmox cluster is currently hosting a specified VM.
#
# ============================================================================

# Define directory and file variables
DIR_FUN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
FILE_FUN=$(basename "$BASH_SOURCE")
BASE_FUN="${FILE_FUN%.*}"
FILEPATH_FUN="${DIR_FUN}/${FILE_FUN}"
CONFIG_FUN="${SITE_CONFIG_FILE}"

# Dynamically create variables based on the base name
eval "FILEPATH_${BASE_FUN}=\$FILEPATH_FUN"
eval "FILE_${BASE_FUN}=\$FILE_FUN"
eval "BASE_${BASE_FUN}=\$BASE_FUN"
eval "CONFIG_${BASE_FUN}=\$CONFIG_FUN"

# Displays an overview of specific Proxmox Virtual Environment (PVE) related functions in the script, showing their usage, shortname, and description
# overview functions
#  
pve-fun() {

    # Technical Description:
    #   Invokes the 'aux-laf' (list available functions) utility.
    #   '$FILEPATH_pve' (dynamically set to the full path of this script) is passed as the file to analyze.
    #   All arguments passed to 'pve-fun' are forwarded to 'aux-laf'.
    #   Primarily used for introspection of this script's capabilities.
    # Dependencies:
    #   - 'aux-laf' function.
    #   - '$FILEPATH_pve' environment variable.

    aux-laf "$FILEPATH_pve" "$@"
}

# Displays an overview of PVE-specific variables defined in the configuration file, showing their names, values, and usage across different files
# overview variables
# 
pve-var() {

    # Technical Description:
    #   Utilizes 'aux-acu' (analyze configuration usage) with the '-o' (overview) flag.
    #   '$CONFIG_pve' (dynamically set from '$SITE_CONFIG_FILE') specifies the configuration file to scan.
    #   '$DIR_FUN/..' (parent directory of this script) is passed as the scope for usage analysis.
    #   Helps in understanding how PVE-related variables from the site config are used across the project.
    # Dependencies:
    #   - 'aux-acu' function.
    #   - '$CONFIG_pve' environment variable.
    #   - '$DIR_FUN' environment variable.

    aux-acu -o "$CONFIG_pve" "$DIR_FUN/.."
}

# Disables specified Proxmox repository files by commenting out 'deb' lines, typically used to manage repository sources 
# disable repository
#  
pve-dsr() {

    # Technical Description:
    #   Modifies Proxmox VE APT repository configuration files to disable them.
    #   Iterates through a predefined list of files:
    #     - "/etc/apt/sources.list.d/pve-enterprise.list"
    #     - "/etc/apt/sources.list.d/ceph.list"
    #   Uses 'sed -i' to comment out lines starting with 'deb' (e.g., 'deb http://...' becomes '#deb http://...').
    #   Calls 'aux-nos' for notifications about actions taken or if files are not found.
    # Dependencies:
    #   - 'sed' utility for in-place file editing.
    #   - 'aux-nos' function for status messages.
    #   - Root privileges are required to modify files in /etc/apt/.

    local function_name="${FUNCNAME[0]}"
    files=(
        "/etc/apt/sources.list.d/pve-enterprise.list"
        "/etc/apt/sources.list.d/ceph.list"
    )

    for file in "${files[@]}"; do
        if [ -f "$file" ]; then
            sed -i '/^deb/ s/^/#/' "$file"
            aux-nos "$function_name" "Changes applied to $file"
        else
            aux-nos "$function_name" "File $file not found."
        fi
    done
}

# Removes the Proxmox subscription notice by modifying the web interface JavaScript file, with an option to restart the pveproxy service 
# remove sub notice
#   
pve-rsn() {

    # Technical Description:
    #   Removes the "No valid subscription" notice from the Proxmox VE web interface.
    #   Modifies the JavaScript file '/usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js'.
    #   Uses 'sed -Ezi.bak' to perform an in-place, multiline-aware regular expression substitution.
    #   The regex targets 'Ext.Msg.show(...title: gettext('No valid sub')...' and prepends 'void({ //',
    #   effectively commenting out the JavaScript call that displays the notice. A backup '.bak' file is created.
    #   Prompts the user to restart 'pveproxy.service' using 'systemctl'.
    # Dependencies:
    #   - 'sed' utility (GNU version for -z and -i).
    #   - 'systemctl' for service management.
    #   - 'aux-nos' for status messages.
    #   - Root privileges required for file modification and service restart.

    local function_name="${FUNCNAME[0]}"
    sed -Ezi.bak "s/(Ext\.Msg\.show\(\{\s+title: gettext\('No valid sub)/void\(\{ \/\/\1/g" /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js

    # Prompt user whether to restart the service
    read -p "Do you want to restart the pveproxy.service now? (y/n): " choice
    case "$choice" in
        y|Y ) systemctl restart pveproxy.service && aux-nos "$function_name" "Service restarted successfully.";;
        n|N ) aux-nos "$function_name" "Service not restarted.";;
        * ) aux-nos "$function_name" "Invalid choice. Service not restarted.";;
    esac
}

# Updates the Proxmox VE Appliance Manager (pveam) container template list
# container list update
#  
pve-clu() {

    # Technical Description:
    #   Updates the local cache of available LXC container templates for Proxmox VE.
    #   Executes the 'pveam update' command. 'pveam' is the Proxmox VE Appliance Manager.
    #   This command fetches the latest list of templates from the configured sources.
    #   Calls 'aux-nos' to confirm execution.
    # Dependencies:
    #   - 'pveam' command-line utility.
    #   - 'aux-nos' function.
    #   - Network access to Proxmox template repositories.

    local function_name="${FUNCNAME[0]}"

    pveam update

    aux-nos "$function_name" "executed"
}

# Downloads a specified container template to a given storage location, with error handling and options to list available templates
# container downloads
#   
pve-cdo() {

    # Technical Description:
    #   Downloads a specified LXC container template to a designated Proxmox VE storage.
    #   Uses 'pveam download <storage_id> <template_name>'. Output is initially suppressed (2>/dev/null).
    #   Arguments:
    #     $1 (ct_dl_sto): The ID of the Proxmox storage where the template will be downloaded.
    #     $2 (ct_dl): The name of the container template to download (e.g., 'alpine-3.18-default_20230615_amd64.tar.xz').
    #   Includes interactive error handling:
    #     - If download fails, it prompts the user to view available templates ('pveam available').
    #     - If the user agrees, it then prompts if they want to try downloading a different template.
    #     - If yes, it recursively calls 'pve-cdo' with the new template name.
    # Dependencies:
    #   - 'pveam' command-line utility.
    #   - User interaction for error handling.

    local function_name="${FUNCNAME[0]}"
    local ct_dl_sto="$1"
    local ct_dl="$2"

    # Attempt to download the template
    if ! pveam download "$ct_dl_sto" "$ct_dl" 2>/dev/null; then
        echo "Error: Unable to download template '$ct_dl'."
        
        # Ask user if they want to see available templates
        read -p "Would you like to see a list of available templates? (y/n): " answer
        
        if [[ "$answer" =~ ^[Yy]$ ]]; then
            echo "Available templates:"
            pveam available
            
            # Ask user if they want to try downloading a different template
            read -p "Would you like to try downloading a different template? (y/n): " retry_answer
            
            if [[ "$retry_answer" =~ ^[Yy]$ ]]; then
                read -p "Enter the name of the template you'd like to download: " new_ct_dl
                pve-cdo "$ct_dl_sto" "$new_ct_dl"
                return
            fi
        fi
        
        echo "$function_name: Failed to download template ( $ct_dl )"
    else
        echo "$function_name: Successfully downloaded template ( $ct_dl )"
    fi
}

# Configures a bind mount for a specified Proxmox container, linking a host directory to a container directory
# container bindmount
# <vmid> <mphost> <mpcontainer>
pve-cbm() {

    # Technical Description:
    #   Configures a bind mount for a Proxmox LXC container.
    #   Uses 'pct set <vmid> -mp0 <host_path>,mp=<container_path>' to define the mount point.
    #   '-mp0' refers to mount point 0; additional mount points would use -mp1, -mp2, etc.
    #   Arguments:
    #     $1 (vmid): The numeric ID of the LXC container.
    #     $2 (mphost): The absolute path on the Proxmox host to be mounted into the container.
    #     $3 (mpcontainer): The absolute path inside the container where the host path will be mounted.
    #   Requires exactly 3 arguments; otherwise, 'aux-use' is called.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - 'aux-use' for usage display on incorrect arguments.
    #   - 'aux-nos' for success notification.

    local function_name="${FUNCNAME[0]}"
    local vmid="$1"
    local mphost="$2"
    local mpcontainer="$3"
    if [ $# -ne 3 ]; then
	aux-use
        return 1
    fi

    # Debugging output to check the parameters
    echo "Function: $function_name"
    echo "VMID: $vmid"
    echo "MPHOST: $mphost"
    echo "MPCONTAINER: $mpcontainer"

    # Ensure all arguments are provided
    if [[ -z "$vmid" || -z "$mphost" || -z "$mpcontainer" ]]; then
        echo "Error: Missing arguments."
        return 1
    fi

    # Properly quote the entire argument for -mp0
    pct set "$vmid" -mp0 "$mphost,mp=$mpcontainer"

    aux-nos "$function_name" "executed ( $vmid / $mphost / $mpcontainer )"
}

# Sets up different containers specified in cfg/env/site.
# container create
# <passed global variables>
pve-ctc() {

    # Technical Description:
    #   Creates a new Proxmox LXC container using 'pct create'.
    #   Accepts a large number of positional arguments mapping to 'pct create' options.
    #   Key parameters include ID, template, hostname, storage, rootfs size, memory, network settings (IP, CIDR, gateway, bridge, NIC name), SSH keys, etc.
    #   Checks for the existence of the specified SSH key file ('$ssh_key_file') before proceeding.
    #   Conditionally adds '--unprivileged' if the 'privileged' argument is "no".
    #   Sets features 'keyctl=1' and 'nesting=1' by default.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - Valid Proxmox VE environment and resources (template, storage, network).
    #   - An existing SSH public key file.
    # Arguments: $1-$18 define various container parameters. See inline comments for specifics.

    local id="$1"
    local template="$2"
    local hostname="$3"
    local storage="$4"
    local rootfs_size="$5"
    local memory="$6"
    local swap="$7"
    local nameserver="$8"
    local searchdomain="$9"
    local password="${10}"
    local cpus="${11}"
    local privileged="${12}"
    local ip_address="${13}"
    local cidr="${14}"
    local gateway="${15}"
    local ssh_key_file="${16}"
    local net_bridge="${17}"
    local net_nic="${18}"

    if [ ! -f "$ssh_key_file" ]; then
        echo "SSH key file $ssh_key_file does not exist. Aborting."
        return 1
    fi

    # Correcting the parameters passed to pct create
    pct create "$id" "$template" \
        --hostname "$hostname" \
        --storage "$storage" \
        --rootfs "$storage:$rootfs_size" \
        --memory "$memory" \
        --swap "$swap" \
        --net0 "name=$net_nic,bridge=$net_bridge,ip=$ip_address/$cidr,gw=$gateway" \
        --nameserver "$nameserver" \
        --searchdomain "$searchdomain" \
        --password "$password" \
        --cores "$cpus" \
        --features "keyctl=1,nesting=1" \
        $(if [ "$privileged" == "no" ]; then echo "--unprivileged"; fi) \
        --ssh-public-keys "$ssh_key_file"
}

# Manages multiple Proxmox containers by starting, stopping, enabling, or disabling them, supporting individual IDs, ranges, or all containers
# container toggle
# <start|stop|enable|disable> <containers|all>
pve-cto() {

    # Technical Description:
    #   Manages the state (start, stop) and on-boot behavior (enable, disable) of Proxmox LXC containers.
    #   First argument 'action': 'start', 'stop', 'enable', or 'disable'.
    #   Subsequent arguments specify target containers:
    #     - Individual container IDs (e.g., 101 102).
    #     - Ranges of IDs (e.g., 100-105).
    #     - 'all' to target all containers (retrieved via 'pct list | awk').
    #   'start'/'stop' actions use 'pct start <vmid>' and 'pct stop <vmid>'.
    #   'enable'/'disable' actions modify the 'onboot:' line in the container's config file
    #   ('/etc/pve/lxc/<vmid>.conf') using 'sed' to set it to 'onboot: 1' or 'onboot: 0'.
    #   Lists container status using 'pct list' after 'start' or 'stop' actions.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - 'sed' and 'awk' utilities.
    #   - Container configuration files at '/etc/pve/lxc/'.

    local action=$1
    shift
    
    if [[ $action != "start" && $action != "stop" && $action != "enable" && $action != "disable" ]]; then
        echo "Invalid action: $action"
        echo "Usage: pve-cto <start|stop|enable|disable> <containers|all>"
        return 1
    fi
    
    handle_action() {
        local vmid=$1
        local config_file="/etc/pve/lxc/${vmid}.conf"
        
        if [[ ! -f "$config_file" ]]; then
            echo "ERROR: Config file for container $vmid does not exist"
            return 1
        fi
        
        case $action in
            start)
                echo "Starting container $vmid"
                pct start "$vmid"
                ;;
            stop)
                echo "Stopping container $vmid"
                pct stop "$vmid"
                ;;
            enable|disable)
                local onboot_value=$([[ $action == "enable" ]] && echo 1 || echo 0)
                echo "Setting onboot to $onboot_value for container $vmid"
                
                sed -i '/^onboot:/d' "$config_file"
                echo "onboot: $onboot_value" >> "$config_file"
                
                echo "Container $vmid configuration updated:"
                grep '^onboot:' "$config_file" || echo "ERROR: onboot entry not found after modification"
                ;;
        esac
    }
    
    if [[ $1 == "all" ]]; then
        echo "Processing all containers"
        container_ids=$(pct list | awk 'NR>1 {print $1}')
        for vmid in $container_ids; do
            handle_action "$vmid"
        done
    else
        for arg in "$@"; do
            if [[ $arg == *-* ]]; then
                IFS='-' read -r start end <<< "$arg"
                for (( vmid=start; vmid<=end; vmid++ )); do
                    handle_action "$vmid"
                done
            else
                handle_action "$arg"
            fi
        done
    fi
    
    if [[ $action == "start" || $action == "stop" ]]; then
        echo "Current container status:"
        pct list
    fi
}

# Deploys or modifies the VM shutdown hook for GPU reattachment
# vm shutdown hook
# Usage: pve-vmd <operation> <vm_id> [<vm_id2> ...]
pve-vmd() {

    # Technical Description:
    #   Manages a VM shutdown hook script for GPU reattachment in Proxmox VE.
    #   Operations: 'add', 'remove', 'debug', specified as the first argument ($1). VM ID is $2.
    #   The hook script is located at '/var/lib/vz/snippets/gpu-reattach-hook.pl'.
    #   'create_or_update_hook_script' inner function generates/updates this Perl script.
    #   The Perl script logs its execution to '/var/log/gpu-reattach-hook.log'.
    #   On 'post-stop' phase, the Perl script executes:
    #     bash -c 'source ${LIB_OPS_DIR}/pve && gpu-pta'
    #     This implies 'gpu-pta' is another script/function responsible for the actual GPU reattachment logic.
    #   'add' uses 'qm set <vmid> -hookscript local:snippets/gpu-reattach-hook.pl'.
    #   'remove' uses 'qm set <vmid> -delete hookscript'.
    #   'debug' provides diagnostic info (script content, permissions, qm config, manual trigger).
    # Dependencies:
    #   - 'qm' command-line utility.
    #   - Perl interpreter (for the hook script).
    #   - 'gpu-pta' script/function (defined elsewhere, sourced via LIB_OPS_DIR).
    #   - '$LIB_OPS_DIR' environment variable.
    #   - 'aux-nos' for notifications.
    #   - Directory '/var/lib/vz/snippets' must be writable.

    local function_name="${FUNCNAME[0]}"
    local hook_script="/var/lib/vz/snippets/gpu-reattach-hook.pl"
    local operation="$1"
    local vm_id="$2"

    echo "Debug: Operation: $operation, VM ID: $vm_id"

    # Validate the provided operation.
    if [[ ! "$operation" =~ ^(add|remove|debug)$ ]]; then
        echo "Error: Invalid operation. Use 'add', 'remove', or 'debug'."
        return 1
    fi

    # Ensure the directory for Proxmox snippets exists, creating it if necessary.
    # This directory is where the hook script will be stored.
    if [ ! -d "/var/lib/vz/snippets" ]; then
        echo "Creating /var/lib/vz/snippets directory..."
        if ! mkdir -p "/var/lib/vz/snippets"; then
            echo "Error: Failed to create /var/lib/vz/snippets directory. Aborting."
            return 1
        fi
    fi

    # Defines an inner function to create or update the GPU reattachment hook script.
    # The script is written in Perl and logs its activity.
    # On VM 'post-stop', it executes 'gpu-pta' to handle GPU reattachment.
    create_or_update_hook_script() {
        if ! cat > "$hook_script" << EOL
#!/usr/bin/perl

use strict;
use warnings;
use POSIX qw(strftime);

my \$vmid = shift;
my \$phase = shift;

my \$log_file = '/var/log/gpu-reattach-hook.log';

open(my \$log, '>>', \$log_file) or die "Could not open log file: \$!";
print \$log strftime("%Y-%m-%d %H:%M:%S", localtime) . " - VM \$vmid, Phase: \$phase\n";

if (\$phase eq 'post-stop') {
    print \$log "Attempting to reattach GPU for VM \$vmid\n";
    my \$result = system("bash -c 'source ${LIB_OPS_DIR}/pve && gpu-pta'");
    print \$log "gpu-pta execution result: \$result\n";
}

close(\$log);
EOL
        then
            echo "Error: Failed to create or update hook script. Aborting."
            return 1
        fi

        if ! chmod 755 "$hook_script"; then
            echo "Error: Failed to set permissions on hook script. Aborting."
            return 1
        fi
        echo "Hook script created/updated and made executable."
    }

    # Create or update hook script
    if ! create_or_update_hook_script; then
        return 1
    fi

    if [ "$operation" = "debug" ]; then
        echo "Debugging hook setup:"
        echo "1. Checking hook script existence and permissions:"
        ls -l "$hook_script"
        echo "2. Checking hook script content:"
        cat "$hook_script"
        echo "3. Checking Proxmox VM configurations for hook references:"
        grep -r "hookscript" /etc/pve/qemu-server/
        echo "4. Manually triggering hook script:"
        perl "$hook_script" "$vm_id" "post-stop"
        echo "5. Checking log file:"
        cat /var/log/gpu-reattach-hook.log
        return 0
    fi

    # Perform operation
    case "$operation" in
        add)
            if qm set "$vm_id" -hookscript "local:snippets/$(basename "$hook_script")"; then
                echo "Hook applied to VM $vm_id"
            else
                echo "Failed to apply hook to VM $vm_id"
                return 1
            fi
            ;;
        remove)
            if qm set "$vm_id" -delete hookscript; then
                echo "Hook removed from VM $vm_id"
            else
                echo "Failed to remove hook from VM $vm_id"
                return 1
            fi
            ;;
    esac

    aux-nos "$function_name" "Hook $operation completed for VM $vm_id"
}

# Sets up different virtual machines specified in cfg/env/site.
# virtual machine create
# <passed global variables>
pve-vmc() {

    # Technical Description:
    #   Creates a new Proxmox QEMU/KVM virtual machine using 'qm create'.
    #   Accepts numerous positional arguments that map directly to 'qm create' options.
    #   Key parameters include VM ID, name, OS type, machine type, ISO image (as IDE2 CD-ROM),
    #   boot order, BIOS type, EFI disk, SCSI hardware controller, QEMU guest agent enablement,
    #   SCSI disk, CPU configuration (sockets, cores, type), memory (static and ballooning),
    #   and network interface configuration (net0).
    # Dependencies:
    #   - 'qm' command-line utility.
    #   - Valid Proxmox VE environment and resources (ISO, storage, network).
    # Arguments: $1-$17 define various VM parameters. See inline comments for specifics.

    local id="$1"
    local name="$2"
    local ostype="$3"
    local machine="$4"
    local iso="$5"
    local boot="$6"
    local bios="$7"
    local efidisk="$8"
    local scsihw="$9"
    local agent="${10}"
    local disk="${11}"
    local sockets="${12}"
    local cores="${13}"
    local cpu="${14}"
    local memory="${15}"
    local balloon="${16}"
    local net="${17}"

    qm create "$id" \
        --name "$name" \
        --ostype "$ostype" \
        --machine "$machine" \
        --ide2 "$iso,media=cdrom" \
        --boot "$boot" \
        --bios "$bios" \
        --efidisk0 "$efidisk" \
        --scsihw "$scsihw" \
        --agent "$agent" \
        --scsi0 "$disk" \
        --sockets "$sockets" \
        --cores "$cores" \
        --cpu "$cpu" \
        --memory "$memory" \
        --balloon "$balloon" \
        --net0 "$net"
}

# Starts a VM on the current node or migrates it from another node, with an option to shut down the source node after migration
# vm start get shutdown
# <vm_id> [s: optional, shutdown other node]
pve-vms() {

    # Technical Description:
    #   Starts a Proxmox VM, migrating it from another node if necessary.
    #   Arguments: $1 (vm_id), $2 (optional 's' to shutdown the source/original node).
    #   1. Retrieves current hostname.
    #   2. Uses 'pve-vck <vm_id>' to determine the node currently hosting the VM ('node_id').
    #   3. If current hostname matches 'node_id' (VM is local):
    #      - Starts VM using 'qm start <vm_id>'.
    #      - If $2 is 's', attempts to shut down 'node_id' (which is the current node) via SSH.
    #   4. If current hostname differs from 'node_id' (VM is remote):
    #      - Calls 'pve-vmg <vm_id>' to migrate the VM to the current node.
    #      - Starts VM using 'qm start <vm_id>'.
    #      - If $2 is 's', shuts down the original remote node ('node_id') via SSH.
    # Dependencies:
    #   - 'pve-vck' function (to find VM's current node).
    #   - 'pve-vmg' function (to migrate VM).
    #   - 'qm' command-line utility.
    #   - 'ssh' for remote shutdown.
    #   - 'hostname' utility.
    #   - 'aux-use' for usage display.

    local hostname=$(hostname)

    # Check if vm_id argument is provided
    if [ -z "$1" ]; then
        aux-use
        return 1
    fi

    # Assign vm_id to a variable
    local vm_id=$1

    # Call pve-vck function to get node_id
    local node_id=$(pve-vck "$vm_id")

    # Check if node_id is empty
    if [ -z "$node_id" ]; then
        echo "Node ID is empty. Cannot proceed."
        return 1
    fi

    # Main logic
    if [ "$hostname" = "$node_id" ]; then
        qm start "$vm_id"
        if [ "$2" = "s" ]; then
            # Shutdown the other node
            echo "Shutting down node $node_id"
            ssh "root@$node_id" "shutdown now"
        fi 
    else
        pve-vmg "$vm_id"
        qm start "$vm_id"
        if [ "$2" = "s" ]; then
            # Shutdown the other node
            echo "Shutting down node $node_id"
            ssh "root@$node_id" "shutdown now"
        fi
    fi
}

# Migrates a VM from a remote node to the current node, handling PCIe passthrough disable/enable during the process
# vm get start
# <vm_id>
pve-vmg() {

    # Technical Description:
    #   Migrates a Proxmox VM from a remote node to the current node, handling PCIe passthrough.
    #   Argument: $1 (vm_id).
    #   1. Uses 'pve-vck <vm_id>' to find the VM's current remote node ('node').
    #   2. If VM is found on a remote node:
    #      a. Disables PCIe passthrough on the remote node: 'ssh <node> "pve-vpt <vm_id> off"'.
    #      b. Migrates the VM to the current node: 'ssh <node> "qm migrate <vm_id> $(hostname)"'.
    #      c. Enables PCIe passthrough on the current (target) node: 'pve-vpt <vm_id> on'.
    #   Requires 'pve-vpt' to be available and executable via SSH on remote nodes.
    # Dependencies:
    #   - 'pve-vck' function.
    #   - 'pve-vpt' function (both locally and on remote nodes).
    #   - 'qm' command-line utility (on remote node for migration).
    #   - 'ssh' for remote command execution.
    #   - 'hostname' utility.
    #   - 'aux-use' for usage display.

    local vm_id="$1"
    if [ $# -ne 1 ]; then
        aux-use
        return 1
    fi

    # Call pve-vck to check if VM exists and get the node
    local node=$(pve-vck "$vm_id")
    if [ -n "$node" ]; then
        echo "VM found on node: $node"

        # Disable PCIe passthrough for the VM on the remote node
        if ! ssh "$node" "pve-vpt $vm_id off"; then
            echo "Failed to disable PCIe passthrough for VM on $node." >&2
            return 1
        fi

        # Migrate the VM to the current node
        if ! ssh "$node" "qm migrate $vm_id $(hostname)"; then
            echo "Failed to migrate VM from $node to $(hostname)." >&2
            return 1
        fi

        # Enable PCIe passthrough for the VM on the current node
        if ! pve-vpt "$vm_id" on; then
            echo "Failed to enable PCIe passthrough for VM on $(hostname)." >&2
            return 1
        fi

        echo "VM migrated and PCIe passthrough enabled."
        return 0  # Return success once VM is found and migrated
    fi

    echo "VM not found on any other node."
    return 1  # Return failure if VM is not found
}

# Toggles PCIe passthrough configuration for a specified VM, modifying its configuration file to enable or disable passthrough devices
# vm passthrough toggle
# <vm_id> <on|off>
pve-vpt() {

    # Technical Description:
    #   Toggles PCIe and USB device passthrough for a Proxmox VM by modifying its configuration file.
    #   Arguments: $1 (vm_id), $2 (action: 'on' or 'off').
    #   VM config file path: '$PVE_CONF_PATH_QEMU/<vm_id>.conf' (e.g., /etc/pve/qemu-server/<vm_id>.conf).
    #   Relies on several host-specific environment variables for PCI IDs, USB devices, and core counts:
    #     - '${hostname}_node_pci0', '${hostname}_node_pci1': PCI IDs for passthrough.
    #     - '${hostname}_core_count_on', '${hostname}_core_count_off': VM core counts for on/off states.
    #     - '${hostname}_usb_devices[@]': An array of USB passthrough definitions (e.g., "usb0: host=1234:5678").
    #   'on' action:
    #     - Sets 'cores' to '${!core_count_on}'.
    #     - Adds 'hostpci0', 'hostpci1', and all entries from '${!usb_devices_var}' to the config file.
    #       These are inserted after the first '[section_header]' line found by awk.
    #   'off' action:
    #     - Sets 'cores' to '${!core_count_off}'.
    #     - Removes all lines starting with 'usb[0-9]*:' or 'hostpci[0-9]*:' from the config.
    # Dependencies:
    #   - 'sed', 'awk', 'hostname' utilities.
    #   - '$PVE_CONF_PATH_QEMU' environment variable.
    #   - Predefined, hostname-specific environment variables for device IDs and core counts.
    #   - 'aux-use' for usage display.

    local vm_id="$1"
    local action="$2"
    local vm_conf="$PVE_CONF_PATH_QEMU/$vm_id.conf"
    if [ $# -ne 2 ]; then
        aux-use
        return 1
    fi

    # Get hostname for variable names
    local hostname=$(hostname)
    local node_pci0="${hostname}_node_pci0"
    local node_pci1="${hostname}_node_pci1"
    local core_count_on="${hostname}_core_count_on"
    local core_count_off="${hostname}_core_count_off"
    local usb_devices_var="${hostname}_usb_devices[@]"

    # Find the starting line of the VM configuration section
    local section_start=$(awk '/^\[/{print NR-1; exit}' "$vm_conf")

    # Action based on the parameter
    case "$action" in
        on)
            # Set core count based on configuration when toggled on
            sed -i "s/cores:.*/cores: ${!core_count_on}/" "$vm_conf"

            # Add passthrough lines
            if [ -z "$section_start" ]; then
                # If no section found, append passthrough lines at the end of the file
                for usb_device in "${!usb_devices_var}"; do
                    echo "$usb_device" >> "$vm_conf"
                done
                echo "hostpci0: ${!node_pci0},pcie=1,x-vga=1" >> "$vm_conf"
                echo "hostpci1: ${!node_pci1},pcie=1" >> "$vm_conf"
            else
                # If a section is found, insert passthrough lines at the appropriate position
                for usb_device in "${!usb_devices_var}"; do
                    sed -i "${section_start}a\\${usb_device}" "$vm_conf"
                    ((section_start++))
                done
                sed -i "${section_start}a\\hostpci0: ${!node_pci0},pcie=1,x-vga=1" "$vm_conf"
                ((section_start++))
                sed -i "${section_start}a\\hostpci1: ${!node_pci1},pcie=1" "$vm_conf"
            fi

            echo "Passthrough lines added to $vm_conf."
            ;;
        off)
            # Set default core count when toggled off
            sed -i "s/cores:.*/cores: ${!core_count_off}/" "$vm_conf"

            # Remove passthrough lines
            sed -i '/^usb[0-9]*:/d; /^hostpci[0-9]*:/d' "$vm_conf"

            echo "Passthrough lines removed from $vm_conf."
            ;;
        *)
            echo "Invalid parameter. Usage: pve-vpt <VM_ID> <on|off>"
            exit 1
            ;;
    esac
}

# Checks and reports which node in the Proxmox cluster is currently hosting a specified VM
# vm check node
# <vm_id>
pve-vck() {
    echo "DEBUG: pve-vck called with arguments: $@" >&2
    local vm_id="$1"
    local found_node=""
    echo "DEBUG: pve-vck: vm_id: $vm_id" >&2

    # Ensure cluster_nodes is defined (expected to be an array of node hostnames, e.g., ("x1" "x2"))
    # This array should be sourced from a configuration file, e.g., site1
    if [ -z "$vm_id" ]; then
        echo "ERR: pve-vck: VM ID not provided." >&2 # Keep existing user-facing message
        echo "DEBUG: pve-vck: vm_id is empty, calling aux-use" >&2
        aux-use
        return 1
    fi

    # Check if cluster_nodes array is populated
    if ! declare -p cluster_nodes &>/dev/null || [[ ! "$(declare -p cluster_nodes)" == "declare -a "* ]]; then
        echo "ERR: pve-vck: cluster_nodes array is not defined or not an array. Please define it in your site configuration." >&2
        echo "DEBUG: pve-vck: cluster_nodes is not a defined array" >&2
        return 1
    fi
    if [ ${#cluster_nodes[@]} -eq 0 ]; then
        echo "ERR: pve-vck: cluster_nodes array is empty. Please define it in your site configuration." >&2 # Keep existing user-facing message
        echo "DEBUG: pve-vck: cluster_nodes array is empty" >&2
        return 1
    fi
    echo "DEBUG: pve-vck: cluster_nodes: (${cluster_nodes[*]})" >&2 # More robust way to print array

    local current_hostname
    current_hostname=$(hostname)
    echo "DEBUG: pve-vck: current_hostname: $current_hostname" >&2

    for node in "${cluster_nodes[@]}"; do
        echo "DEBUG: pve-vck: checking node: $node" >&2
        local qm_list_output
        if [ "$node" != "$current_hostname" ]; then
            echo "DEBUG: pve-vck: checking remote node $node via ssh qm list for vm_id $vm_id" >&2
            # Capture output and errors from ssh command
            qm_list_output=$(ssh "$node" "qm list")
            local ssh_exit_code=$?
            echo "DEBUG: pve-vck: ssh $node \\"qm list\\" exit code: $ssh_exit_code" >&2
            echo "DEBUG: pve-vck: output from ssh $node \\"qm list\\":" >&2
            echo "$qm_list_output" >&2 # Print the raw output

            if [ $ssh_exit_code -eq 0 ] && echo "$qm_list_output" | grep -q -w -- "$vm_id"; then
                echo "DEBUG: pve-vck: VM $vm_id found on remote node $node" >&2
                found_node="$node"
                break
            else
                echo "DEBUG: pve-vck: VM $vm_id NOT found on remote node $node (ssh qm list or grep failed)" >&2
            fi
        else
            echo "DEBUG: pve-vck: checking local node $node via qm list for vm_id $vm_id" >&2
            qm_list_output=$(qm list)
            local qm_exit_code=$?
            echo "DEBUG: pve-vck: local qm list exit code: $qm_exit_code" >&2
            echo "DEBUG: pve-vck: output from local qm list:" >&2
            echo "$qm_list_output" >&2 # Print the raw output

            if [ $qm_exit_code -eq 0 ] && echo "$qm_list_output" | grep -q -w -- "$vm_id"; then
                echo "DEBUG: pve-vck: VM $vm_id found on local node $node" >&2
                found_node="$node"
                break
            else
                echo "DEBUG: pve-vck: VM $vm_id NOT found on local node $node (local qm list or grep failed)" >&2
            fi
        fi
    done

    if [ -n "$found_node" ]; then
        echo "DEBUG: pve-vck: final found_node: $found_node, returning 0" >&2
        echo "$found_node" # This is the actual output for command substitution
        return 0
    else
        echo "DEBUG: pve-vck: VM $vm_id not found on any cluster node, returning 1" >&2
        # No stdout echo here, so command substitution gets empty string
        return 1
    fi
}
