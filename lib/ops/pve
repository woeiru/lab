#!/bin/bash

# Define directory and file variables
DIR_FUN="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"

# Source auxiliary functions library
source "${DIR_FUN}/../gen/aux"
FILE_FUN=$(basename "$BASH_SOURCE")
BASE_FUN="${FILE_FUN%.*}"
FILEPATH_FUN="${DIR_FUN}/${FILE_FUN}"
CONFIG_FUN="${SITE_CONFIG_FILE}"

# Dynamically create variables based on the base name
eval "FILEPATH_${BASE_FUN}=\$FILEPATH_FUN"
eval "FILE_${BASE_FUN}=\$FILE_FUN"
eval "BASE_${BASE_FUN}=\$BASE_FUN"
eval "CONFIG_${BASE_FUN}=\$CONFIG_FUN"

# Displays an overview of specific Proxmox Virtual Environment (PVE) related functions in the script, showing their usage, shortname, and description
# overview functions
# <script_path>
pve_fun() {
    # Technical Description:
    #   Invokes the 'ana_laf' (list available functions) utility.
    #   Accepts script path as explicit parameter for analysis.
    #   All remaining arguments are forwarded to 'ana_laf'.
    #   Primarily used for introspection of script capabilities.
    # Dependencies:
    #   - 'ana_laf' function.
    # Arguments:
    #   $1: script_path - path to the script file to analyze

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi

    # Use the provided script_path parameter or fall back to FILEPATH_pve
    local script_path="${1:-$FILEPATH_pve}"
    shift # Remove script_path from arguments so remaining args can be forwarded
    
    ana_laf "$script_path" "$@"
}

# Displays an overview of PVE-specific variables defined in the configuration file, showing their names, values, and usage across different files
# overview variables
# <config_file> <analysis_dir>
pve_var() {
    # Technical Description:
    #   Utilizes 'ana_acu' (analyze configuration usage) with the '-o' (overview) flag.
    #   First argument specifies the configuration file to scan.
    #   Second argument is passed as the scope for usage analysis.
    #   Helps in understanding how PVE-related variables from the site config are used across the project.
    # Dependencies:
    #   - 'ana_acu' function.
    # Arguments:
    #   $1: config_file - path to the configuration file to analyze
    #   $2: analysis_dir - directory scope for usage analysis

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 2 ]; then
        aux_use
        return 1
    fi

    local config_file="$1"
    local analysis_dir="$2"
    
    if ! aux_val "$config_file" "not_empty"; then
        aux_err "Configuration file path cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_chk "file_exists" "$config_file"; then
        aux_err "Configuration file not found: $config_file"
        aux_use
        return 1
    fi
    
    if ! aux_val "$analysis_dir" "not_empty"; then
        aux_err "Analysis directory path cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_chk "dir_exists" "$analysis_dir"; then
        aux_err "Analysis directory not found: $analysis_dir"
        aux_use
        return 1
    fi

    ana_acu -o "$config_file" "$analysis_dir"
}

# Disables specified Proxmox repository files by commenting out 'deb' lines, typically used to manage repository sources 
# disable repository
# -x (execute)
pve_dsr() {
    # Technical Description:
    #   Modifies Proxmox VE APT repository configuration files to disable them.
    #   Iterates through a predefined list of files:
    #     - "/etc/apt/sources.list.d/pve-enterprise.list"
    #     - "/etc/apt/sources.list.d/ceph.list"
    #   Uses 'sed -i' to comment out lines starting with 'deb' (e.g., 'deb http://...' becomes '#deb http://...').
    #   Calls 'aux_log' for notifications about actions taken or if files are not found.
    # Dependencies:
    #   - 'sed' utility for in-place file editing.
    #   - 'aux_log' function for status messages.
    #   - Root privileges are required to modify files in /etc/apt/.
    # Arguments:
    #   $1: -x - explicit execution flag required for safety and consistency

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 1 ] || [ "$1" != "-x" ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    
    aux_dbg "Starting disable repository operation"
    files=(
        "/etc/apt/sources.list.d/pve-enterprise.list"
        "/etc/apt/sources.list.d/ceph.list"
    )

    for file in "${files[@]}"; do
        if aux_chk "file_exists" "$file"; then
            if aux_cmd "sed" "-i" "/^deb/ s/^/#/" "$file"; then
                aux_info "Repository disabled" "file=$file"
            else
                aux_err "Failed to disable repository" "file=$file"
                return 2
            fi
        else
            aux_warn "Repository file not found" "file=$file"
        fi
    done
}

# Removes the Proxmox subscription notice by modifying the web interface JavaScript file, with an option to restart the pveproxy service 
# remove sub notice
# -x (execute)
pve_rsn() {
    # Technical Description:
    #   Removes the "No valid subscription" notice from the Proxmox VE web interface.
    #   Modifies the JavaScript file '/usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js'.
    #   Uses 'sed -Ezi.bak' to perform an in-place, multiline-aware regular expression substitution.
    #   The regex targets 'Ext.Msg.show(...title: gettext('No valid sub')...' and prepends 'void({ //',
    #   effectively commenting out the JavaScript call that displays the notice. A backup '.bak' file is created.
    #   Prompts the user to restart 'pveproxy.service' using 'systemctl'.
    # Dependencies:
    #   - 'sed' utility (GNU version for -z and -i).
    #   - 'systemctl' for service management.
    #   - 'aux_log' for status messages.
    #   - Root privileges required for file modification and service restart.
    # Arguments:
    #   $1: -x - explicit execution flag required for safety and consistency

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 1 ] || [ "$1" != "-x" ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    
    # Dependency checks
    if ! aux_chk "command" "sed"; then
        aux_err "sed command not found - required for file modification"
        return 127
    fi
    
    if ! aux_chk "command" "systemctl"; then
        aux_err "systemctl command not found - required for service management"
        return 127
    fi
    
    aux_dbg "Starting subscription notice removal"
    local js_file="/usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js"
    
    if ! aux_chk "file_exists" "$js_file"; then
        aux_err "Proxmox JavaScript file not found: $js_file"
        return 2
    fi
    
    aux_info "Modifying Proxmox subscription notice" "file=$js_file"
    
    if aux_cmd "sed" "-Ezi.bak" "s/(Ext\.Msg\.show\(\{\s+title: gettext\('No valid sub)/void\(\{ \/\/\1/g" "$js_file"; then
        aux_info "Subscription notice modified successfully"
    else
        aux_err "Failed to modify subscription notice"
        return 2
    fi

    # Enhanced user interaction for service restart
    aux_info "Service restart is recommended for changes to take effect"
    local restart_choice=$(aux_ask "Restart pveproxy.service now? (recommended)" "y" "y_or_n")
    
    if [ "$restart_choice" = "y" ]; then
        aux_info "Restarting pveproxy service as requested"
        if aux_cmd "systemctl" "restart" "pveproxy.service"; then
            aux_info "Service restarted successfully" "service=pveproxy"
            
            # Check service status after restart
            if aux_cmd "systemctl" "is-active" "pveproxy.service" >/dev/null 2>&1; then
                aux_info "Service is running properly after restart" "service=pveproxy"
            else
                aux_warn "Service may not be running properly after restart" "service=pveproxy"
                local check_status=$(aux_ask "Check service status?" "y" "y_or_n")
                if [ "$check_status" = "y" ]; then
                    aux_cmd "systemctl" "status" "pveproxy.service"
                fi
            fi
        else
            aux_err "Failed to restart service" "service=pveproxy"
            local check_logs=$(aux_ask "Check service logs for troubleshooting?" "y" "y_or_n")
            if [ "$check_logs" = "y" ]; then
                aux_cmd "journalctl" "-u" "pveproxy.service" "--lines=20"
            fi
            return 2
        fi
    else
        aux_info "Service restart skipped by user"
        aux_warn "Manual restart required later: systemctl restart pveproxy.service" "service=pveproxy"
    fi
}

# Updates the Proxmox VE Appliance Manager (pveam) container template list
# container list update
# -x (execute)
pve_clu() {
    # Technical Description:
    #   Updates the local cache of available LXC container templates for Proxmox VE.
    #   Executes the 'pveam update' command. 'pveam' is the Proxmox VE Appliance Manager.
    #   This command fetches the latest list of templates from the configured sources.
    #   Calls 'aux_log' to confirm execution.
    # Dependencies:
    #   - 'pveam' command-line utility.
    #   - 'aux_log' function.
    #   - Network access to Proxmox template repositories.
    # Arguments:
    #   $1: -x - explicit execution flag required for safety and consistency

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 1 ] || [ "$1" != "-x" ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    
    # Dependency checks
    if ! aux_chk "command" "pveam"; then
        aux_err "pveam command not found - Proxmox VE Appliance Manager required"
        return 127
    fi
    
    aux_info "Updating container template list"
    aux_dbg "Executing pveam update"

    if aux_cmd "pveam" "update"; then
        aux_info "Container template list updated successfully"
    else
        aux_err "Failed to update container template list"
        return 2
    fi
}

# Downloads a specified container template to a given storage location, with error handling and options to list available templates
# container downloads
# <storage_id> <template_name>
pve_cdo() {
    # Technical Description:
    #   Downloads a specified LXC container template to a designated Proxmox VE storage.
    #   Uses 'pveam download <storage_id> <template_name>'. Output is initially suppressed (2>/dev/null).
    #   Includes interactive error handling:
    #     - If download fails, it prompts the user to view available templates ('pveam available').
    #     - If the user agrees, it then prompts if they want to try downloading a different template.
    #     - If yes, it recursively calls 'pve_cdo' with the new template name.
    # Dependencies:
    #   - 'pveam' command-line utility.
    #   - User interaction for error handling.
    # Arguments:
    #   $1: storage_id - The ID of the Proxmox storage where the template will be downloaded
    #   $2: template_name - The name of the container template to download (e.g., 'alpine-3.18-default_20230615_amd64.tar.xz')

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 2 ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    local ct_dl_sto="$1"
    local ct_dl="$2"
    
    # Parameter validation
    if ! aux_val "$ct_dl_sto" "not_empty"; then
        aux_err "Storage ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$ct_dl" "not_empty"; then
        aux_err "Template name cannot be empty"
        aux_use
        return 1
    fi
    
    # Dependency checks
    if ! aux_chk "command" "pveam"; then
        aux_err "pveam command not found - Proxmox VE Appliance Manager required"
        return 127
    fi
    
    aux_info "Starting template download" "storage=$ct_dl_sto,template=$ct_dl"
    aux_dbg "Attempting to download template: $ct_dl to storage: $ct_dl_sto"
    
    # Attempt to download the template
    if aux_cmd "pveam" "download" "$ct_dl_sto" "$ct_dl"; then
        aux_info "Template downloaded successfully" "storage=$ct_dl_sto,template=$ct_dl"
        return 0
    else
        aux_err "Failed to download template" "storage=$ct_dl_sto,template=$ct_dl"
        
        # Enhanced user interaction for template selection
        local show_templates=$(aux_ask "Show available templates for selection?" "y" "y_or_n")
        
        if [ "$show_templates" = "y" ]; then
            aux_info "Displaying available templates"
            if aux_cmd "pveam" "available"; then
                aux_info "Templates listed successfully"
                
                # Enhanced template selection with validation
                local retry_download=$(aux_ask "Try downloading a different template?" "y" "y_or_n")
                
                if [ "$retry_download" = "y" ]; then
                    while true; do
                        local new_template=$(aux_ask "Enter template name (or 'cancel' to abort)" "" "not_empty")
                        
                        if [ "$new_template" = "cancel" ]; then
                            aux_info "Template download cancelled by user"
                            return 1
                        fi
                        
                        # Validate template name format
                        if aux_val "$new_template" "regex" ".*\.(tar\.(xz|gz)|tgz)$"; then
                            aux_dbg "Retrying download with new template: $new_template"
                            pve_cdo "$ct_dl_sto" "$new_template"
                            return $?
                        else
                            aux_warn "Invalid template format" "template=$new_template,expected_format=*.tar.xz,*.tar.gz,*.tgz"
                            local continue_choice=$(aux_ask "Try again with different template name?" "y" "y_or_n")
                            if [ "$continue_choice" != "y" ]; then
                                aux_info "Template selection cancelled by user"
                                return 1
                            fi
                        fi
                    done
                fi
            else
                aux_err "Failed to list available templates"
                local manual_entry=$(aux_ask "Enter template name manually?" "n" "y_or_n")
                if [ "$manual_entry" = "y" ]; then
                    local manual_template=$(aux_ask "Enter template name" "" "not_empty")
                    aux_info "Attempting manual template download" "template=$manual_template"
                    pve_cdo "$ct_dl_sto" "$manual_template"
                    return $?
                fi
            fi
        fi
        
        return 2
    fi
}

# Configures a bind mount for a specified Proxmox container, linking a host directory to a container directory
# container bindmount
# <vmid> <mphost> <mpcontainer>
pve_cbm() {
    # Technical Description:
    #   Configures a bind mount for a Proxmox LXC container.
    #   Uses 'pct set <vmid> -mp0 <host_path>,mp=<container_path>' to define the mount point.
    #   '-mp0' refers to mount point 0; additional mount points would use -mp1, -mp2, etc.
    #   Requires exactly 3 arguments; validates all parameters before execution.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - 'aux_use' for usage display on incorrect arguments.
    #   - 'aux_log' for success notification.
    # Arguments:
    #   $1: vmid - The numeric ID of the LXC container
    #   $2: mphost - The absolute path on the Proxmox host to be mounted into the container
    #   $3: mpcontainer - The absolute path inside the container where the host path will be mounted

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 3 ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    local vmid="$1"
    local mphost="$2"
    local mpcontainer="$3"

    # Parameter validation
    if ! aux_val "$vmid" "not_empty"; then
        aux_err "Container ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$vmid" "numeric"; then
        aux_err "Container ID must be numeric: $vmid"
        aux_use
        return 1
    fi
    
    if ! aux_val "$mphost" "not_empty"; then
        aux_err "Host mount path cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$mpcontainer" "not_empty"; then
        aux_err "Container mount path cannot be empty"
        aux_use
        return 1
    fi
    
    # Dependency checks
    if ! aux_chk "command" "pct"; then
        aux_err "pct command not found - Proxmox Container Toolkit required"
        return 127
    fi
    
    if ! aux_chk "dir_exists" "$mphost"; then
        aux_err "Host mount path does not exist: $mphost"
        return 2
    fi
    
    aux_info "Configuring bind mount" "vmid=$vmid,host_path=$mphost,container_path=$mpcontainer"
    aux_dbg "Setting up mount point 0 for container $vmid"

    # Configure the bind mount
    if aux_cmd "pct" "set" "$vmid" "-mp0" "$mphost,mp=$mpcontainer"; then
        aux_info "Bind mount configured successfully" "vmid=$vmid,host_path=$mphost,container_path=$mpcontainer"
    else
        aux_err "Failed to configure bind mount" "vmid=$vmid,host_path=$mphost,container_path=$mpcontainer"
        return 2
    fi
}

# Sets up different containers specified in cfg/env/site.
# container create
# <id> <template> <hostname> <storage> <rootfs_size> <memory> <swap> <nameserver> <searchdomain> <password> <cpus> <privileged> <ip_address> <cidr> <gateway> <ssh_key_file> <net_bridge> <net_nic>
pve_ctc() {

    # Technical Description:
    #   Creates a new Proxmox LXC container using 'pct create'.
    #   Accepts a large number of positional arguments mapping to 'pct create' options.
    #   Key parameters include ID, template, hostname, storage, rootfs size, memory, network settings (IP, CIDR, gateway, bridge, NIC name), SSH keys, etc.
    #   Checks for the existence of the specified SSH key file ('$ssh_key_file') before proceeding.
    #   Conditionally adds '--unprivileged' if the 'privileged' argument is "no".
    #   Sets features 'keyctl=1' and 'nesting=1' by default.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - Valid Proxmox VE environment and resources (template, storage, network).
    #   - An existing SSH public key file.
    # Arguments: 
    #   $1: id - Container ID
    #   $2: template - Container template path
    #   $3: hostname - Container hostname
    #   $4: storage - Storage location
    #   $5: rootfs_size - Root filesystem size
    #   $6: memory - Memory allocation
    #   $7: swap - Swap allocation
    #   $8: nameserver - DNS nameserver
    #   $9: searchdomain - DNS search domain
    #   $10: password - Root password
    #   $11: cpus - CPU count
    #   $12: privileged - Privileged mode (yes/no)
    #   $13: ip_address - IP address
    #   $14: cidr - Network CIDR
    #   $15: gateway - Network gateway
    #   $16: ssh_key_file - SSH public key file path
    #   $17: net_bridge - Network bridge
    #   $18: net_nic - Network interface name

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 18 ]; then
        aux_use
        return 1
    fi

    local id="$1"
    local template="$2"
    local hostname="$3"
    local storage="$4"
    local rootfs_size="$5"
    local memory="$6"
    local swap="$7"
    local nameserver="$8"
    local searchdomain="$9"
    local password="${10}"
    local cpus="${11}"
    local privileged="${12}"
    local ip_address="${13}"
    local cidr="${14}"
    local gateway="${15}"
    local ssh_key_file="${16}"
    local net_bridge="${17}"
    local net_nic="${18}"

    # Critical parameter validation
    if ! aux_val "$id" "not_empty"; then
        aux_err "Container ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$id" "numeric"; then
        aux_err "Container ID must be numeric: $id"
        aux_use
        return 1
    fi
    
    if ! aux_val "$template" "not_empty"; then
        aux_err "Template path cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$hostname" "not_empty"; then
        aux_err "Hostname cannot be empty"
        aux_use
        return 1
    fi

    # Dependency checks
    if ! aux_chk "command" "pct"; then
        aux_err "pct command not found - Proxmox Container Toolkit required"
        return 127
    fi

    if ! aux_chk "file_exists" "$ssh_key_file"; then
        aux_err "SSH key file does not exist: $ssh_key_file"
        return 2
    fi
    
    aux_info "Creating LXC container" "id=$id,hostname=$hostname,template=$template"
    aux_dbg "Container configuration: memory=$memory,cpus=$cpus,storage=$storage"

    # Create the container
    local create_args=(
        "$id" "$template"
        "--hostname" "$hostname"
        "--storage" "$storage"
        "--rootfs" "$storage:$rootfs_size"
        "--memory" "$memory"
        "--swap" "$swap"
        "--net0" "name=$net_nic,bridge=$net_bridge,ip=$ip_address/$cidr,gw=$gateway"
        "--nameserver" "$nameserver"
        "--searchdomain" "$searchdomain"
        "--password" "$password"
        "--cores" "$cpus"
        "--features" "keyctl=1,nesting=1"
        "--ssh-public-keys" "$ssh_key_file"
    )
    
    if [ "$privileged" = "no" ]; then
        create_args+=("--unprivileged")
    fi
    
    if aux_cmd "pct" "create" "${create_args[@]}"; then
        aux_info "Container created successfully" "id=$id,hostname=$hostname"
    else
        aux_err "Failed to create container" "id=$id,hostname=$hostname"
        return 2
    fi
}

# Manages multiple Proxmox containers by starting, stopping, enabling, or disabling them, supporting individual IDs, ranges, or all containers
# container toggle
# <start|stop|enable|disable> <containers|all>
pve_cto() {
    # Technical Description:
    #   Manages the state (start, stop) and on-boot behavior (enable, disable) of Proxmox LXC containers.
    #   First argument 'action': 'start', 'stop', 'enable', or 'disable'.
    #   Subsequent arguments specify target containers:
    #     - Individual container IDs (e.g., 101 102).
    #     - Ranges of IDs (e.g., 100-105).
    #     - 'all' to target all containers (retrieved via 'pct list | awk').
    #   'start'/'stop' actions use 'pct start <vmid>' and 'pct stop <vmid>'.
    #   'enable'/'disable' actions modify the 'onboot:' line in the container's config file
    #   ('/etc/pve/lxc/<vmid>.conf') using 'sed' to set it to 'onboot: 1' or 'onboot: 0'.
    #   Lists container status using 'pct list' after 'start' or 'stop' actions.
    # Dependencies:
    #   - 'pct' command-line utility.
    #   - 'sed' and 'awk' utilities.
    #   - Container configuration files at '/etc/pve/lxc/'.
    # Arguments:
    #   $1: action - The action to perform: start, stop, enable, or disable
    #   $2+: containers - Container IDs, ranges (e.g., 100-105), or 'all'

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -lt 2 ]; then
        aux_use
        return 1
    fi

    local action=$1
    shift
    
    if [[ $action != "start" && $action != "stop" && $action != "enable" && $action != "disable" ]]; then
        aux_use
        return 1
    fi
    
    handle_action() {
        local vmid=$1
        local config_file="/etc/pve/lxc/${vmid}.conf"
        
        if ! aux_chk "file_exists" "$config_file"; then
            aux_err "Config file for container does not exist" "vmid=$vmid,config_file=$config_file"
            return 1
        fi
        
        case $action in
            start)
                aux_info "Starting container" "vmid=$vmid"
                if aux_cmd "pct" "start" "$vmid"; then
                    aux_info "Container started successfully" "vmid=$vmid"
                else
                    aux_err "Failed to start container" "vmid=$vmid"
                    return 2
                fi
                ;;
            stop)
                aux_info "Stopping container" "vmid=$vmid"
                if aux_cmd "pct" "stop" "$vmid"; then
                    aux_info "Container stopped successfully" "vmid=$vmid"
                else
                    aux_err "Failed to stop container" "vmid=$vmid"
                    return 2
                fi
                ;;
            enable|disable)
                local onboot_value=$([[ $action == "enable" ]] && echo 1 || echo 0)
                aux_info "Setting onboot configuration" "vmid=$vmid,onboot=$onboot_value"
                
                if aux_cmd "sed" "-i" "/^onboot:/d" "$config_file" && 
                   aux_cmd "sh" "-c" "echo 'onboot: $onboot_value' >> '$config_file'"; then
                    
                    if aux_chk "command" "grep" && grep -q '^onboot:' "$config_file"; then
                        aux_info "Container configuration updated successfully" "vmid=$vmid,onboot=$onboot_value"
                    else
                        aux_err "onboot entry not found after modification" "vmid=$vmid"
                        return 2
                    fi
                else
                    aux_err "Failed to update container configuration" "vmid=$vmid"
                    return 2
                fi
                ;;
        esac
    }
    
    # Dependency checks
    if ! aux_chk "command" "pct"; then
        aux_err "pct command not found - Proxmox Container Toolkit required"
        return 127
    fi
    
    aux_info "Starting container management operation" "action=$action,targets=$*"
    
    if [[ $1 == "all" ]]; then
        aux_info "Processing all containers"
        aux_dbg "Retrieving list of all containers"
        
        if ! container_ids=$(aux_cmd "pct" "list" | awk 'NR>1 {print $1}'); then
            aux_err "Failed to retrieve container list"
            return 2
        fi
        
        local container_count=0
        for vmid in $container_ids; do
            aux_dbg "Processing container in batch" "vmid=$vmid,action=$action"
            handle_action "$vmid"
            ((container_count++))
        done
        aux_info "Processed containers in batch" "action=$action,count=$container_count"
    else
        for arg in "$@"; do
            if [[ $arg == *-* ]]; then
                IFS='-' read -r start end <<< "$arg"
                aux_info "Processing container range" "start=$start,end=$end,action=$action"
                for (( vmid=start; vmid<=end; vmid++ )); do
                    aux_dbg "Processing container in range" "vmid=$vmid,action=$action"
                    handle_action "$vmid"
                done
            else
                aux_dbg "Processing individual container" "vmid=$arg,action=$action"
                handle_action "$arg"
            fi
        done
    fi
    
    if [[ $action == "start" || $action == "stop" ]]; then
        aux_info "Displaying current container status"
        if aux_cmd "pct" "list"; then
            aux_info "Container status displayed successfully"
        else
            aux_warn "Failed to display container status"
        fi
    fi
}

# Deploys or modifies the VM shutdown hook for GPU reattachment
# vm shutdown hook
# <operation> <vm_id> <hook_script> <lib_ops_dir> [vm_id2...]
pve_vmd() {
    # Technical Description:
    #   Manages a VM shutdown hook script for GPU reattachment in Proxmox VE.
    #   Operations: 'add', 'remove', 'debug', specified as the first argument ($1). VM ID is $2.
    #   The hook script is located at '/var/lib/vz/snippets/gpu-reattach-hook.pl'.
    #   'create_or_update_hook_script' inner function generates/updates this Perl script.
    #   The Perl script logs its execution to '/var/log/gpu-reattach-hook.log'.
    #   On 'post-stop' phase, the Perl script executes:
    #     bash -c 'source <lib_ops_dir>/pve && gpu_pta'
    #     This implies 'gpu_pta' is another script/function responsible for the actual GPU reattachment logic.
    #   'add' uses 'qm set <vmid> -hookscript local:snippets/gpu-reattach-hook.pl'.
    #   'remove' uses 'qm set <vmid> -delete hookscript'.
    #   'debug' provides diagnostic info (script content, permissions, qm config, manual trigger).
    # Dependencies:
    #   - 'qm' command-line utility.
    #   - Perl interpreter (for the hook script).
    #   - 'gpu_pta' script/function (defined elsewhere, sourced via lib_ops_dir).
    #   - 'aux_log' for notifications.
    #   - Directory '/var/lib/vz/snippets' must be writable.
    # Arguments:
    #   $1: operation - The operation to perform: add, remove, or debug
    #   $2: vm_id - The VM ID to configure
    #   $3: hook_script - Path to the hook script
    #   $4: lib_ops_dir - Directory containing the ops library
    #   $5+: additional_vm_ids (optional) - Additional VM IDs to configure

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -lt 4 ]; then
        aux_use
        return 1
    fi

    local function_name="${FUNCNAME[0]}"
    local operation="$1"
    local vm_id="$2"
    local hook_script="$3"
    local lib_ops_dir="$4"

    # Validate required parameters
    if [[ -z "$hook_script" || -z "$lib_ops_dir" ]]; then
        aux_use
        return 1
    fi

    aux_dbg "Function parameters" "operation=$operation,vm_id=$vm_id,hook_script=$hook_script,lib_ops_dir=$lib_ops_dir"

    # Parameter validation
    if ! aux_val "$operation" "not_empty"; then
        aux_err "Operation parameter cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$vm_id" "numeric"; then
        aux_err "VM ID must be numeric: $vm_id"
        aux_use
        return 1
    fi

    # Validate the provided operation
    if [[ ! "$operation" =~ ^(add|remove|debug)$ ]]; then
        aux_err "Invalid operation specified" "operation=$operation,valid_operations=add,remove,debug"
        aux_use
        return 1
    fi
    
    # Dependency checks
    if ! aux_chk "command" "qm"; then
        aux_err "qm command not found - Proxmox VE required"
        return 127
    fi

    aux_info "Starting VM hook management" "operation=$operation,vm_id=$vm_id"
    aux_dbg "Hook script path: $hook_script, Lib ops dir: $lib_ops_dir"

    # Ensure the directory for Proxmox snippets exists, creating it if necessary
    local snippets_dir="/var/lib/vz/snippets"
    if ! aux_chk "dir_exists" "$snippets_dir"; then
        aux_info "Creating Proxmox snippets directory" "directory=$snippets_dir"
        if aux_cmd "mkdir" "-p" "$snippets_dir"; then
            aux_info "Snippets directory created successfully" "directory=$snippets_dir"
        else
            aux_err "Failed to create snippets directory" "directory=$snippets_dir"
            return 2
        fi
    fi

    # Defines an inner function to create or update the GPU reattachment hook script.
    # The script is written in Perl and logs its activity.
    # On VM 'post-stop', it executes 'gpu_pta' to handle GPU reattachment.
    create_or_update_hook_script() {
        aux_dbg "Creating hook script" "script_path=$hook_script"
        if ! cat > "$hook_script" << EOL
#!/usr/bin/perl

use strict;
use warnings;
use POSIX qw(strftime);

my \$vmid = shift;
my \$phase = shift;

my \$log_file = '/var/log/gpu-reattach-hook.log';

open(my \$log, '>>', \$log_file) or die "Could not open log file: \$!";
print \$log strftime("%Y-%m-%d %H:%M:%S", localtime) . " - VM \$vmid, Phase: \$phase\n";

if (\$phase eq 'post-stop') {
    print \$log "Attempting to reattach GPU for VM \$vmid\n";
    my \$result = system("bash -c 'source $lib_ops_dir/pve && gpu_pta'");
    print \$log "gpu_pta execution result: \$result\n";
}

close(\$log);
EOL
        then
            aux_err "Failed to create or update hook script" "script_path=$hook_script"
            return 2
        fi

        if aux_cmd "chmod" "755" "$hook_script"; then
            aux_info "Hook script created/updated and made executable" "script_path=$hook_script"
        else
            aux_err "Failed to set permissions on hook script" "script_path=$hook_script"
            return 2
        fi
    }

    # Create or update hook script
    if ! create_or_update_hook_script; then
        return 1
    fi

    if [ "$operation" = "debug" ]; then
        aux_info "Starting debug mode for hook setup" "vm_id=$vm_id"
        
        aux_info "Checking hook script existence and permissions"
        if aux_cmd "ls" "-l" "$hook_script"; then
            aux_info "Hook script permissions checked"
        else
            aux_warn "Hook script not found or permission issue" "script=$hook_script"
        fi
        
        aux_info "Checking hook script content"
        if aux_cmd "cat" "$hook_script"; then
            aux_info "Hook script content displayed"
        else
            aux_warn "Could not display hook script content" "script=$hook_script"
        fi
        
        aux_info "Checking Proxmox VM configurations for hook references"
        if aux_cmd "grep" "-r" "hookscript" "/etc/pve/qemu-server/"; then
            aux_info "Hook references found in VM configurations"
        else
            aux_warn "No hook references found in VM configurations"
        fi
        
        aux_info "Manually triggering hook script for testing" "vm_id=$vm_id"
        if aux_cmd "perl" "$hook_script" "$vm_id" "post-stop"; then
            aux_info "Hook script test execution completed"
        else
            aux_warn "Hook script test execution failed"
        fi
        
        aux_info "Checking log file contents"
        if aux_cmd "cat" "/var/log/gpu-reattach-hook.log"; then
            aux_info "Log file contents displayed"
        else
            aux_warn "Could not display log file contents" "log_file=/var/log/gpu-reattach-hook.log"
        fi
        
        return 0
    fi

    # Perform operation
    case "$operation" in
        add)
            local hook_name="local:snippets/$(basename "$hook_script")"
            aux_info "Adding hook to VM" "vm_id=$vm_id,hook_script=$hook_name"
            if aux_cmd "qm" "set" "$vm_id" "-hookscript" "$hook_name"; then
                aux_info "Hook applied to VM successfully" "vm_id=$vm_id,hook_script=$hook_name"
            else
                aux_err "Failed to apply hook to VM" "vm_id=$vm_id,hook_script=$hook_name"
                return 2
            fi
            ;;
        remove)
            aux_info "Removing hook from VM" "vm_id=$vm_id"
            if aux_cmd "qm" "set" "$vm_id" "-delete" "hookscript"; then
                aux_info "Hook removed from VM successfully" "vm_id=$vm_id"
            else
                aux_err "Failed to remove hook from VM" "vm_id=$vm_id"
                return 2
            fi
            ;;
    esac

    aux_info "Hook operation completed successfully" "operation=$operation,vm_id=$vm_id"
}

# Sets up different virtual machines specified in cfg/env/site.
# virtual machine create
# <id> <name> <ostype> <machine> <iso> <boot> <bios> <efidisk> <scsihw> <agent> <disk> <sockets> <cores> <cpu> <memory> <balloon> <net>
pve_vmc() {

    # Technical Description:
    #   Creates a new Proxmox QEMU/KVM virtual machine using 'qm create'.
    #   Accepts numerous positional arguments that map directly to 'qm create' options.
    #   Key parameters include VM ID, name, OS type, machine type, ISO image (as IDE2 CD-ROM),
    #   boot order, BIOS type, EFI disk, SCSI hardware controller, QEMU guest agent enablement,
    #   SCSI disk, CPU configuration (sockets, cores, type), memory (static and ballooning),
    #   and network interface configuration (net0).
    # Dependencies:
    #   - 'qm' command-line utility.
    #   - Valid Proxmox VE environment and resources (ISO, storage, network).
    # Arguments: 
    #   $1: id - VM ID
    #   $2: name - VM name
    #   $3: ostype - OS type
    #   $4: machine - Machine type
    #   $5: iso - ISO image path
    #   $6: boot - Boot order
    #   $7: bios - BIOS type
    #   $8: efidisk - EFI disk configuration
    #   $9: scsihw - SCSI hardware controller
    #   $10: agent - QEMU guest agent setting
    #   $11: disk - SCSI disk configuration
    #   $12: sockets - CPU sockets
    #   $13: cores - CPU cores
    #   $14: cpu - CPU type
    #   $15: memory - Memory allocation
    #   $16: balloon - Memory ballooning
    #   $17: net - Network configuration

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 17 ]; then
        aux_use
        return 1
    fi

    local id="$1"
    local name="$2"
    local ostype="$3"
    local machine="$4"
    local iso="$5"
    local boot="$6"
    local bios="$7"
    local efidisk="$8"
    local scsihw="$9"
    local agent="${10}"
    local disk="${11}"
    local sockets="${12}"
    local cores="${13}"
    local cpu="${14}"
    local memory="${15}"
    local balloon="${16}"
    local net="${17}"

    # Critical parameter validation
    if ! aux_val "$id" "not_empty"; then
        aux_err "VM ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$id" "numeric"; then
        aux_err "VM ID must be numeric: $id"
        aux_use
        return 1
    fi
    
    if ! aux_val "$name" "not_empty"; then
        aux_err "VM name cannot be empty"
        aux_use
        return 1
    fi
    
    # Dependency checks
    if ! aux_chk "command" "qm"; then
        aux_err "qm command not found - Proxmox VE required"
        return 127
    fi
    
    aux_info "Creating QEMU/KVM virtual machine" "id=$id,name=$name,ostype=$ostype"
    aux_dbg "VM configuration" "machine=$machine,memory=$memory,cpu=$cpu,cores=$cores"
    
    if aux_cmd "qm" "create" "$id" \
        "--name" "$name" \
        "--ostype" "$ostype" \
        "--machine" "$machine" \
        "--ide2" "$iso,media=cdrom" \
        "--boot" "$boot" \
        "--bios" "$bios" \
        "--efidisk0" "$efidisk" \
        "--scsihw" "$scsihw" \
        "--agent" "$agent" \
        "--scsi0" "$disk" \
        "--sockets" "$sockets" \
        "--cores" "$cores" \
        "--cpu" "$cpu" \
        "--memory" "$memory" \
        "--balloon" "$balloon" \
        "--net0" "$net"; then
        aux_info "VM created successfully" "id=$id,name=$name"
    else
        aux_err "Failed to create VM" "id=$id,name=$name"
        return 2
    fi
}

# Starts a VM on the current node or migrates it from another node, with an option to shut down the source node after migration
# vm start get shutdown
# <vm_id> <cluster_nodes_str> <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path> [s: optional, shutdown other node]
pve_vms() {

    # Technical Description:
    #   Starts a Proxmox VM, migrating it from another node if necessary.
    #   Arguments: 
    #     $1 (vm_id): VM ID
    #     $2 (cluster_nodes_str): Space-separated cluster node names
    #     $3 (pci0_id): PCI ID for hostpci0
    #     $4 (pci1_id): PCI ID for hostpci1
    #     $5 (core_count_on): VM core count when passthrough is on
    #     $6 (core_count_off): VM core count when passthrough is off
    #     $7 (usb_devices_str): Space-separated USB device definitions
    #     $8 (pve_conf_path): Path to PVE config directory
    #     $9 (optional 's' to shutdown the source/original node).
    #   1. Retrieves current hostname.
    #   2. Uses 'pve_vck <vm_id> <cluster_nodes_str>' to determine the node currently hosting the VM ('node_id').
    #   3. If current hostname matches 'node_id' (VM is local):
    #      - Starts VM using 'qm start <vm_id>'.
    #      - If $9 is 's', attempts to shut down 'node_id' (which is the current node) via SSH.
    #   4. If current hostname differs from 'node_id' (VM is remote):
    #      - Calls 'pve_vmg <vm_id> <cluster_nodes_str> <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>' to migrate the VM to the current node.
    #      - Starts VM using 'qm start <vm_id>'.
    #      - If $9 is 's', shuts down the original remote node ('node_id') via SSH.
    # Dependencies:
    #   - 'pve_vck' function (to find VM's current node).
    #   - 'pve_vmg' function (to migrate VM).
    #   - 'qm' command-line utility.
    #   - 'ssh' for remote shutdown.
    #   - 'hostname' utility.
    #   - 'aux_use' for usage display.

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -lt 8 ] || [ $# -gt 9 ]; then
        aux_use
        return 1
    fi

    local vm_id="$1"
    local cluster_nodes_str="$2"
    local pci0_id="$3"
    local pci1_id="$4"
    local core_count_on="$5"
    local core_count_off="$6"
    local usb_devices_str="$7"
    local pve_conf_path="$8"
    local shutdown_flag="$9"

    local hostname=$(hostname)

    # Parameter validation
    if ! aux_val "$vm_id" "not_empty"; then
        aux_err "VM ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$vm_id" "numeric"; then
        aux_err "VM ID must be numeric: $vm_id"
        aux_use
        return 1
    fi
    
    if ! aux_val "$cluster_nodes_str" "not_empty"; then
        aux_err "Cluster nodes parameter cannot be empty"
        aux_use
        return 1
    fi
    
    # Dependency checks
    if ! aux_chk "command" "qm"; then
        aux_err "qm command not found - Proxmox VE required"
        return 127
    fi
    
    if ! aux_chk "command" "hostname"; then
        aux_err "hostname command not found"
        return 127
    fi
    
    aux_info "Starting VM management operation" "vm_id=$vm_id,operation=start_or_migrate"
    aux_dbg "Cluster nodes: $cluster_nodes_str"

    # Call pve_vck function to get node_id
    aux_dbg "Checking VM location across cluster nodes"
    local node_id=$(pve_vck "$vm_id" "$cluster_nodes_str")

    # Check if node_id is empty
    if [ -z "$node_id" ]; then
        aux_err "VM not found on any cluster node" "vm_id=$vm_id"
        return 1
    fi
    
    aux_info "VM found on cluster node" "vm_id=$vm_id,node_id=$node_id"

    # Main logic
    if [ "$hostname" = "$node_id" ]; then
        aux_info "VM is on local node, starting directly" "vm_id=$vm_id,node=$hostname"
        if aux_cmd "qm" "start" "$vm_id"; then
            aux_info "VM started successfully" "vm_id=$vm_id"
        else
            aux_err "Failed to start VM" "vm_id=$vm_id"
            return 2
        fi
        
        if [ "$shutdown_flag" = "s" ]; then
            aux_info "Shutting down local node as requested" "node=$node_id"
            if aux_cmd "ssh" "root@$node_id" "shutdown now"; then
                aux_info "Node shutdown initiated" "node=$node_id"
            else
                aux_warn "Failed to initiate node shutdown" "node=$node_id"
            fi
        fi 
    else
        aux_info "VM is on remote node, migrating to local node" "vm_id=$vm_id,from_node=$node_id,to_node=$hostname"
        if pve_vmg "$vm_id" "$cluster_nodes_str" "$pci0_id" "$pci1_id" "$core_count_on" "$core_count_off" "$usb_devices_str" "$pve_conf_path"; then
            aux_info "VM migration completed successfully"
            
            if aux_cmd "qm" "start" "$vm_id"; then
                aux_info "VM started successfully after migration" "vm_id=$vm_id"
            else
                aux_err "Failed to start VM after migration" "vm_id=$vm_id"
                return 2
            fi
            
            if [ "$shutdown_flag" = "s" ]; then
                aux_info "Shutting down source node as requested" "node=$node_id"
                if aux_cmd "ssh" "root@$node_id" "shutdown now"; then
                    aux_info "Source node shutdown initiated" "node=$node_id"
                else
                    aux_warn "Failed to initiate source node shutdown" "node=$node_id"
                fi
            fi
        else
            aux_err "VM migration failed" "vm_id=$vm_id,from_node=$node_id"
            return 2
        fi
    fi
}

# Migrates a VM from a remote node to the current node, handling PCIe passthrough disable/enable during the process
# vm get start
# <vm_id> <cluster_nodes_str> <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>
pve_vmg() {

    # Technical Description:
    #   Migrates a Proxmox VM from a remote node to the current node, handling PCIe passthrough.
    #   Arguments: 
    #     $1 (vm_id): VM ID
    #     $2 (cluster_nodes_str): Space-separated cluster node names
    #     $3 (pci0_id): PCI ID for hostpci0
    #     $4 (pci1_id): PCI ID for hostpci1
    #     $5 (core_count_on): VM core count when passthrough is on
    #     $6 (core_count_off): VM core count when passthrough is off
    #     $7 (usb_devices_str): Space-separated USB device definitions
    #     $8 (pve_conf_path): Path to PVE config directory
    #   1. Uses 'pve_vck <vm_id> <cluster_nodes_str>' to find the VM's current remote node ('node').
    #   2. If VM is found on a remote node:
    #      a. Disables PCIe passthrough on the remote node: 'ssh <node> "source ${LIB_OPS_DIR}/pve && pve_vpt <vm_id> off <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>"'.
    #      b. Migrates the VM to the current node: 'ssh <node> "qm migrate <vm_id> $(hostname)"'.
    #      c. Enables PCIe passthrough on the current (target) node: 'pve_vpt <vm_id> on <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>'.
    #   Requires 'pve_vpt' to be available and executable via SSH on remote nodes.
    # Dependencies:
    #   - 'pve_vck' function.
    #   - 'pve_vpt' function (both locally and on remote nodes).
    #   - 'qm' command-line utility (on remote node for migration).
    #   - 'ssh' for remote command execution.
    #   - 'hostname' utility.
    #   - 'aux_use' for usage display.

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 8 ]; then
        aux_use
        return 1
    fi

    local vm_id="$1"
    local cluster_nodes_str="$2"
    local pci0_id="$3"
    local pci1_id="$4"
    local core_count_on="$5"
    local core_count_off="$6"
    local usb_devices_str="$7"
    local pve_conf_path="$8"

    # Parameter validation
    if ! aux_val "$vm_id" "numeric"; then
        aux_err "VM ID must be numeric: $vm_id"
        aux_use
        return 1
    fi
    
    if ! aux_val "$cluster_nodes_str" "not_empty"; then
        aux_err "Cluster nodes parameter cannot be empty"
        aux_use
        return 1
    fi
    
    # Dependency checks
    if ! aux_chk "command" "ssh"; then
        aux_err "ssh command not found - required for remote operations"
        return 127
    fi
    
    if ! aux_chk "command" "hostname"; then
        aux_err "hostname command not found"
        return 127
    fi
    
    local current_hostname=$(hostname)
    aux_info "Starting VM migration process" "vm_id=$vm_id,target_node=$current_hostname"
    aux_dbg "Migration parameters" "pci0_id=$pci0_id,pci1_id=$pci1_id,core_count_on=$core_count_on,core_count_off=$core_count_off"

    # Call pve_vck to check if VM exists and get the node
    aux_dbg "Locating VM on cluster nodes"
    local node=$(pve_vck "$vm_id" "$cluster_nodes_str")
    
    if [ -n "$node" ]; then
        aux_info "VM found on source node" "vm_id=$vm_id,source_node=$node"

        # Disable PCIe passthrough for the VM on the remote node
        aux_info "Disabling PCIe passthrough on source node" "vm_id=$vm_id,source_node=$node"
        if aux_cmd "ssh" "$node" "source \${LIB_OPS_DIR}/pve && pve_vpt $vm_id off $pci0_id $pci1_id $core_count_on $core_count_off \"$usb_devices_str\" $pve_conf_path"; then
            aux_info "PCIe passthrough disabled successfully on source node" "vm_id=$vm_id,source_node=$node"
        else
            aux_err "Failed to disable PCIe passthrough for VM on source node" "vm_id=$vm_id,source_node=$node"
            return 2
        fi

        # Migrate the VM to the current node
        aux_info "Migrating VM to current node" "vm_id=$vm_id,from=$node,to=$current_hostname"
        if aux_cmd "ssh" "$node" "qm migrate $vm_id $current_hostname"; then
            aux_info "VM migration completed successfully" "vm_id=$vm_id,from=$node,to=$current_hostname"
        else
            aux_err "Failed to migrate VM" "vm_id=$vm_id,from=$node,to=$current_hostname"
            return 2
        fi

        # Enable PCIe passthrough for the VM on the current node
        aux_info "Enabling PCIe passthrough on target node" "vm_id=$vm_id,target_node=$current_hostname"
        if pve_vpt "$vm_id" on "$pci0_id" "$pci1_id" "$core_count_on" "$core_count_off" "$usb_devices_str" "$pve_conf_path"; then
            aux_info "PCIe passthrough enabled successfully on target node" "vm_id=$vm_id,target_node=$current_hostname"
        else
            aux_err "Failed to enable PCIe passthrough for VM on target node" "vm_id=$vm_id,target_node=$current_hostname"
            return 2
        fi

        aux_info "VM migration and PCIe passthrough configuration completed successfully" "vm_id=$vm_id"
        return 0
    fi

    aux_warn "VM not found on any cluster node" "vm_id=$vm_id"
    return 1
}

# Toggles PCIe passthrough configuration for a specified VM, modifying its configuration file to enable or disable passthrough devices
# vm passthrough toggle
# <vm_id> <on|off> <pci0_id> <pci1_id> <core_count_on> <core_count_off> <usb_devices_str> <pve_conf_path>
pve_vpt() {

    # Technical Description:
    #   Toggles PCIe and USB device passthrough for a Proxmox VM by modifying its configuration file.
    #   Arguments: 
    #     $1 (vm_id): VM ID
    #     $2 (action: 'on' or 'off')
    #     $3 (pci0_id): PCI ID for hostpci0
    #     $4 (pci1_id): PCI ID for hostpci1  
    #     $5 (core_count_on): VM core count when passthrough is on
    #     $6 (core_count_off): VM core count when passthrough is off
    #     $7 (usb_devices_str): Space-separated USB device definitions
    #     $8 (pve_conf_path): Path to PVE config directory
    #   VM config file path: '<pve_conf_path>/<vm_id>.conf' (e.g., /etc/pve/qemu-server/<vm_id>.conf).
    #   'on' action:
    #     - Sets 'cores' to 'core_count_on'.
    #     - Adds 'hostpci0', 'hostpci1', and USB devices to the config file.
    #       These are inserted after the first '[section_header]' line found by awk.
    #   'off' action:
    #     - Sets 'cores' to 'core_count_off'.
    #     - Removes all lines starting with 'usb[0-9]*:' or 'hostpci[0-9]*:' from the config.
    # Dependencies:
    #   - 'sed', 'awk', 'hostname' utilities.
    #   - 'aux_use' for usage display.

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi
    
    if [ $# -ne 8 ]; then
        aux_use
        return 1
    fi
    
    local vm_id="$1"
    local action="$2"
    local pci0_id="$3"
    local pci1_id="$4"
    local core_count_on="$5"
    local core_count_off="$6"
    local usb_devices_str="$7"
    local pve_conf_path="$8"
    
    # Comprehensive parameter validation
    if ! aux_val "$vm_id" "not_empty"; then
        aux_err "VM ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$vm_id" "numeric"; then
        aux_err "VM ID must be numeric: $vm_id"
        aux_use
        return 1
    fi
    
    if ! aux_val "$action" "not_empty"; then
        aux_err "Action parameter cannot be empty"
        aux_use
        return 1
    fi
    
    if [[ "$action" != "on" && "$action" != "off" ]]; then
        aux_err "Action must be 'on' or 'off'" "action=$action"
        aux_use
        return 1
    fi
    
    if ! aux_val "$pci0_id" "not_empty"; then
        aux_err "PCI0 ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$pci1_id" "not_empty"; then
        aux_err "PCI1 ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$core_count_on" "numeric"; then
        aux_err "Core count (on) must be numeric: $core_count_on"
        aux_use
        return 1
    fi
    
    if ! aux_val "$core_count_off" "numeric"; then
        aux_err "Core count (off) must be numeric: $core_count_off"
        aux_use
        return 1
    fi
    
    if ! aux_val "$pve_conf_path" "not_empty"; then
        aux_err "PVE configuration path cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_chk "dir_exists" "$pve_conf_path"; then
        aux_err "PVE configuration directory does not exist" "path=$pve_conf_path"
        return 2
    fi
    
    aux_info "Starting PCIe passthrough toggle" "vm_id=$vm_id,action=$action"
    aux_dbg "Passthrough configuration" "pci0_id=$pci0_id,pci1_id=$pci1_id,core_on=$core_count_on,core_off=$core_count_off"
    
    local vm_conf="$pve_conf_path/$vm_id.conf"
    
    # Check if VM configuration file exists
    if ! aux_chk "file_exists" "$vm_conf"; then
        aux_err "VM configuration file does not exist" "vm_id=$vm_id,config_file=$vm_conf"
        return 2
    fi
    
    # Parse USB devices string into array using aux_arr utilities
    local usb_devices=()
    if [ -n "$usb_devices_str" ]; then
        aux_dbg "Parsing USB devices configuration" "usb_devices_input=$usb_devices_str"
        # Read the newline-separated string into an array
        while IFS= read -r line; do
            if [[ -n "$line" ]]; then
                aux_arr "add" "usb_devices" "$line"
                aux_dbg "Added USB device to configuration" "device=$line"
            fi
        done <<< "$usb_devices_str"
        
        local device_count=$(aux_arr "length" "usb_devices")
        aux_dbg "USB devices parsed" "count=$device_count"
    fi

    # Find the starting line of the VM configuration section
    aux_dbg "Analyzing VM configuration file structure" "config_file=$vm_conf"
    local section_start
    if ! section_start=$(aux_cmd "awk" "/^\\[/{print NR-1; exit}" "$vm_conf"); then
        aux_warn "Could not analyze configuration file structure" "config_file=$vm_conf"
        section_start=""
    fi
    aux_dbg "Configuration section analysis" "section_start=$section_start"

    # Action based on the parameter
    case "$action" in
        on)
            # Set core count based on configuration when toggled on
            aux_info "Setting VM cores for passthrough-on mode" "vm_id=$vm_id,cores=$core_count_on"
            if aux_cmd "sed" "-i" "s/cores:.*/cores: $core_count_on/" "$vm_conf"; then
                aux_dbg "Core count updated successfully" "cores=$core_count_on"
            else
                aux_err "Failed to update core count" "cores=$core_count_on"
                return 2
            fi

            # Add passthrough lines
            if [ -z "$section_start" ]; then
                # If no section found, append passthrough lines at the end of the file
                aux_dbg "Adding USB devices to end of configuration file" "vm_conf=$vm_conf"
                for usb_device in "${usb_devices[@]}"; do
                    if [[ -n "$usb_device" ]]; then
                        if ! aux_cmd "sh" "-c" "echo '$usb_device' >> '$vm_conf'"; then
                            aux_err "Failed to add USB device to configuration" "device=$usb_device"
                            return 2
                        fi
                        aux_dbg "Added USB device" "device=$usb_device"
                    fi
                done
                if ! aux_cmd "sh" "-c" "echo 'hostpci0: $pci0_id,pcie=1,x-vga=1' >> '$vm_conf'" ||
                   ! aux_cmd "sh" "-c" "echo 'hostpci1: $pci1_id,pcie=1' >> '$vm_conf'"; then
                    aux_err "Failed to add PCI passthrough configuration" "vm_id=$vm_id"
                    return 2
                fi
            else
                # If a section is found, insert passthrough lines at the appropriate position
                # Create a temporary file for safe insertion
                temp_file="${vm_conf}.tmp"
                
                # Copy everything up to the insertion point
                if ! aux_cmd "head" "-n" "$section_start" "$vm_conf"; then
                    aux_err "Failed to copy configuration file header" "vm_conf=$vm_conf,temp_file=$temp_file"
                    return 2
                fi > "$temp_file"
                
                # Add USB devices
                aux_dbg "Adding USB devices to temporary configuration file" "temp_file=$temp_file"
                for usb_device in "${usb_devices[@]}"; do
                    if [[ -n "$usb_device" ]]; then
                        if ! aux_cmd "sh" "-c" "echo '$usb_device' >> '$temp_file'"; then
                            aux_err "Failed to add USB device to temp file" "device=$usb_device,temp_file=$temp_file"
                            return 2
                        fi
                        aux_dbg "Added USB device to temp file" "device=$usb_device"
                    fi
                done
                
                # Add PCI devices
                if ! aux_cmd "sh" "-c" "echo 'hostpci0: $pci0_id,pcie=1,x-vga=1' >> '$temp_file'" ||
                   ! aux_cmd "sh" "-c" "echo 'hostpci1: $pci1_id,pcie=1' >> '$temp_file'"; then
                    aux_err "Failed to add PCI configuration to temp file" "temp_file=$temp_file"
                    return 2
                fi
                
                # Copy the rest of the file
                if ! aux_cmd "tail" "-n" "+$((section_start + 1))" "$vm_conf"; then
                    aux_err "Failed to copy remainder of configuration file" "vm_conf=$vm_conf,temp_file=$temp_file"
                    return 2
                fi >> "$temp_file"
                
                # Replace the original file
                if aux_cmd "mv" "$temp_file" "$vm_conf"; then
                    aux_dbg "Configuration file updated successfully" "temp_file=$temp_file,vm_conf=$vm_conf"
                else
                    aux_err "Failed to update configuration file" "temp_file=$temp_file,vm_conf=$vm_conf"
                    return 2
                fi
            fi

            aux_info "Passthrough configuration enabled successfully" "vm_id=$vm_id,config_file=$vm_conf"
            ;;
        off)
            # Set default core count when toggled off
            aux_info "Setting VM cores for passthrough-off mode" "vm_id=$vm_id,cores=$core_count_off"
            if aux_cmd "sed" "-i" "s/cores:.*/cores: $core_count_off/" "$vm_conf"; then
                aux_dbg "Core count updated successfully" "cores=$core_count_off"
            else
                aux_err "Failed to update core count" "cores=$core_count_off"
                return 2
            fi

            # Remove passthrough lines and orphaned host= lines
            aux_info "Removing passthrough configuration from VM" "vm_id=$vm_id"
            if aux_cmd "sed" "-i" "/^usb[0-9]*:/d; /^hostpci[0-9]*:/d; /^host=/d" "$vm_conf"; then
                aux_info "Passthrough configuration removed successfully" "vm_id=$vm_id,config_file=$vm_conf"
            else
                aux_err "Failed to remove passthrough configuration" "vm_id=$vm_id,config_file=$vm_conf"
                return 2
            fi
            ;;
        *)
            aux_err "Invalid action parameter" "action=$action,valid_actions=on,off"
            aux_use
            return 1
            ;;
    esac
}

# Checks and reports which node in the Proxmox cluster is currently hosting a specified VM
# vm check node
# <vm_id> <cluster_nodes_array>
pve_vck() {

    # Technical Description:
    #   Determines which node in a Proxmox VE cluster is currently hosting the specified Virtual Machine (VM).
    #   It iterates through a list of cluster nodes provided as the second parameter.
    #   For each node, it uses `ssh` to execute `qm list` on remote nodes or executes `qm list` directly
    #   on the local node.
    #   The output of `qm list` is then parsed using `grep` to check if the given `vm_id` is present.
    #   If the VM is found, the function outputs the hostname of the node and returns 0.
    #   If the VM is not found on any node after checking all, it returns 1.
    #   Debug messages are printed to stderr if `set -x` or similar debugging is enabled.
    # Dependencies:
    #   - `qm` command-line utility (Proxmox VE).
    #   - `ssh` client for remote node checks.
    #   - `grep` utility for parsing command output.
    #   - `hostname` utility to identify the current node.
    #   - `aux_use` function for displaying usage if `vm_id` is not provided.

    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        aux_tec
        return 0
    fi

    aux_dbg "called with arguments: $@"
    local vm_id="$1"
    local cluster_nodes_str="$2"
    local found_node=""
    aux_dbg "vm_id: $vm_id"

    if ! aux_val "$vm_id" "not_empty"; then
        aux_err "VM ID cannot be empty"
        aux_use
        return 1
    fi
    
    if ! aux_val "$vm_id" "numeric"; then
        aux_err "VM ID must be numeric: $vm_id"
        aux_use
        return 1
    fi

    if ! aux_val "$cluster_nodes_str" "not_empty"; then
        aux_err "Cluster nodes parameter cannot be empty"
        aux_use
        return 1
    fi

    # Parse the cluster nodes string into an array
    IFS=' ' read -ra cluster_nodes <<< "$cluster_nodes_str"
    
    if [ ${#cluster_nodes[@]} -eq 0 ]; then
        aux_err "Cluster nodes array is empty after parsing" "cluster_nodes_str=$cluster_nodes_str"
        return 1
    fi
    
    aux_dbg "Parsed cluster nodes" "cluster_nodes=(${cluster_nodes[*]}),count=${#cluster_nodes[@]}"

    local current_hostname
    if ! current_hostname=$(aux_cmd "hostname"); then
        aux_err "Failed to determine current hostname"
        return 2
    fi
    aux_dbg "Current hostname determined" "hostname=$current_hostname"

    # Dependency checks
    if ! aux_chk "command" "qm"; then
        aux_err "qm command not found - Proxmox VE required"
        return 127
    fi
    
    if ! aux_chk "command" "ssh"; then
        aux_err "ssh command not found - required for remote node checks"
        return 127
    fi
    
    aux_info "Starting VM location search across cluster" "vm_id=$vm_id,node_count=${#cluster_nodes[@]}"
    
    for node in "${cluster_nodes[@]}"; do
        aux_dbg "Checking node for VM" "node=$node,vm_id=$vm_id"
        local qm_list_output
        
        if [ "$node" != "$current_hostname" ]; then
            aux_dbg "Checking remote node via SSH" "node=$node,vm_id=$vm_id"
            # Capture output and errors from ssh command
            if qm_list_output=$(aux_cmd "ssh" "$node" "qm list"); then
                aux_dbg "SSH qm list successful" "node=$node"
                
                if echo "$qm_list_output" | aux_cmd "grep" "-q" "-w" "--" "$vm_id"; then
                    aux_info "VM found on remote node" "vm_id=$vm_id,node=$node"
                    found_node="$node"
                    break
                else
                    aux_dbg "VM not found on remote node" "vm_id=$vm_id,node=$node"
                fi
            else
                aux_warn "Failed to execute qm list on remote node" "node=$node"
            fi
        else
            aux_dbg "Checking local node" "node=$node,vm_id=$vm_id"
            if qm_list_output=$(aux_cmd "qm" "list"); then
                aux_dbg "Local qm list successful"
                
                if echo "$qm_list_output" | aux_cmd "grep" "-q" "-w" "--" "$vm_id"; then
                    aux_info "VM found on local node" "vm_id=$vm_id,node=$node"
                    found_node="$node"
                    break
                else
                    aux_dbg "VM not found on local node" "vm_id=$vm_id,node=$node"
                fi
            else
                aux_warn "Failed to execute qm list on local node" "node=$node"
            fi
        fi
    done

    if [ -n "$found_node" ]; then
        aux_info "VM location search completed successfully" "vm_id=$vm_id,found_node=$found_node"
        echo "$found_node" # This is the actual output for command substitution
        return 0
    else
        aux_warn "VM not found on any cluster node" "vm_id=$vm_id,nodes_checked=${#cluster_nodes[@]}"
        # No stdout echo here, so command substitution gets empty string
        return 1
    fi
}
